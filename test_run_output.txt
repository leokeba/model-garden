/home/leo/Dev/model-garden/.venv/lib/python3.13/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/leo/Dev/model-garden/.venv/lib/python3.13/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.14.0         Please see GitHub issue #2919 for more info
ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
INFO 10-20 16:29:32 [__init__.py:216] Automatically detected platform cuda.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
================================================================================
PRODUCTION-LIKE MEMORY LEAK TEST
Using: SFTTrainer + UnslothVisionDataCollator + Callbacks
================================================================================

1. Loading dataset...
   Memory: 1231.5 MB -> 1238.5 MB

2. Formatting dataset...
Loading vision-language model: Qwen/Qwen2.5-VL-3B-Instruct
Precision: 4-bit (memory efficient)
'(ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: 6b8b1842-7649-4525-856d-ccfc92822fd8)')' thrown while requesting HEAD https://huggingface.co/unslothai/repeat/resolve/main/config.json
WARNING:huggingface_hub.utils._http: '(ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: 6b8b1842-7649-4525-856d-ccfc92822fd8)')' thrown while requesting HEAD https://huggingface.co/unslothai/repeat/resolve/main/config.json
Retrying in 1s [Retry 1/5].
WARNING:huggingface_hub.utils._http: Retrying in 1s [Retry 1/5].
The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.
==((====))==  Unsloth 2025.10.3: Fast Qwen2_5_Vl patching. Transformers: 4.56.2. vLLM: 0.11.0.
   \\   /|    NVIDIA GeForce RTX 3090 Ti. Num GPUs = 1. Max memory: 23.544 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
âœ“ Model loaded with Unsloth optimizations
â ´ Loading model...
   Configuring LoRA adapters...
Configuring LoRA adapters for vision-language model...
âœ“ LoRA adapters configured (Unsloth)
Formatting vision-language dataset...
Converting OpenAI messages format to simple format for compatibility...
âœ“ Dataset formatted (300 examples)
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 300 | Num Epochs = 3 | Total steps = 100
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 37,152,768 of 3,791,775,744 (0.98% trained)
   Memory: 1884.6 MB (dataset is a list with 300 examples)

3. Setting up training with SFTTrainer...
Unsloth: Model does not have a default image size - using 512
   Memory before trainer creation: 1885.4 MB
   Memory after trainer creation: 1917.9 MB

4. Running training (50 steps)...
   Monitoring memory every 10 steps...
--------------------------------------------------------------------------------
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:20<34:23, 20.85s/it]  2%|â–         | 2/100 [00:27<20:31, 12.56s/it]  3%|â–Ž         | 3/100 [00:36<17:42, 10.96s/it]  4%|â–         | 4/100 [00:45<16:21, 10.23s/it]  5%|â–Œ         | 5/100 [00:54<15:04,  9.52s/it]  6%|â–Œ         | 6/100 [01:03<15:02,  9.61s/it]  7%|â–‹         | 7/100 [01:11<14:10,  9.14s/it]  8%|â–Š         | 8/100 [01:19<13:14,  8.64s/it]  9%|â–‰         | 9/100 [01:28<13:03,  8.61s/it]/home/leo/Dev/model-garden/test_production_like_leak.py:103: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead
  tensors = [obj for obj in gc.get_objects() if isinstance(obj, torch.Tensor)]
 10%|â–ˆ         | 10/100 [01:36<12:39,  8.44s/it]                                                 10%|â–ˆ         | 10/100 [01:36<12:39,  8.44s/it] 11%|â–ˆ         | 11/100 [01:45<12:50,  8.66s/it] 12%|â–ˆâ–        | 12/100 [01:52<11:56,  8.15s/it] 13%|â–ˆâ–Ž        | 13/100 [02:00<11:52,  8.19s/it] 14%|â–ˆâ–        | 14/100 [02:08<11:44,  8.19s/it] 15%|â–ˆâ–Œ        | 15/100 [02:18<12:22,  8.74s/it] 16%|â–ˆâ–Œ        | 16/100 [02:28<12:50,  9.17s/it] 17%|â–ˆâ–‹        | 17/100 [02:37<12:29,  9.03s/it] 18%|â–ˆâ–Š        | 18/100 [02:46<12:26,  9.11s/it] 19%|â–ˆâ–‰        | 19/100 [02:54<11:28,  8.50s/it]/home/leo/Dev/model-garden/test_production_like_leak.py:103: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead
  tensors = [obj for obj in gc.get_objects() if isinstance(obj, torch.Tensor)]
 20%|â–ˆâ–ˆ        | 20/100 [03:02<11:13,  8.42s/it]                                                 20%|â–ˆâ–ˆ        | 20/100 [03:02<11:13,  8.42s/it] 21%|â–ˆâ–ˆ        | 21/100 [03:11<11:35,  8.80s/it] 22%|â–ˆâ–ˆâ–       | 22/100 [03:21<11:39,  8.97s/it] 23%|â–ˆâ–ˆâ–Ž       | 23/100 [03:30<11:33,  9.00s/it] 24%|â–ˆâ–ˆâ–       | 24/100 [03:55<17:34, 13.88s/it] 25%|â–ˆâ–ˆâ–Œ       | 25/100 [04:04<15:31, 12.42s/it] 26%|â–ˆâ–ˆâ–Œ       | 26/100 [04:12<13:28, 10.93s/it] 27%|â–ˆâ–ˆâ–‹       | 27/100 [04:21<12:46, 10.51s/it] 28%|â–ˆâ–ˆâ–Š       | 28/100 [04:28<11:28,  9.56s/it] 29%|â–ˆâ–ˆâ–‰       | 29/100 [04:37<11:01,  9.31s/it] 30%|â–ˆâ–ˆâ–ˆ       | 30/100 [04:46<10:34,  9.07s/it]                                                 30%|â–ˆâ–ˆâ–ˆ       | 30/100 [04:46<10:34,  9.07s/it] 31%|â–ˆâ–ˆâ–ˆ       | 31/100 [04:54<10:19,  8.98s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [05:05<10:38,  9.38s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [05:15<10:45,  9.63s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [05:23<09:59,  9.09s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [05:33<10:17,  9.50s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [05:43<10:09,  9.53s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [05:52<09:46,  9.31s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [05:57<08:18,  8.05s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [06:05<08:13,  8.08s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [06:14<08:25,  8.43s/it]                                                 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [06:14<08:25,  8.43s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [06:23<08:30,  8.65s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [06:32<08:20,  8.64s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [06:40<07:59,  8.42s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [06:46<07:10,  7.68s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [06:55<07:32,  8.23s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [07:03<07:22,  8.19s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [07:12<07:15,  8.21s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [07:19<06:58,  8.05s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [07:27<06:45,  7.96s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [07:36<06:53,  8.26s/it]                                                 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [07:36<06:53,  8.26s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [07:46<07:01,  8.60s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [07:53<06:32,  8.17s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [08:02<06:42,  8.56s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [08:11<06:38,  8.66s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [08:20<06:39,  8.88s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [08:29<06:21,  8.67s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [08:36<06:01,  8.41s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [08:45<05:53,  8.43s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [08:53<05:42,  8.34s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [09:02<05:36,  8.42s/it]                                                 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [09:02<05:36,  8.42s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [09:11<05:40,  8.74s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [09:19<05:19,  8.41s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [09:28<05:24,  8.76s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [09:37<05:11,  8.64s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [09:45<04:53,  8.39s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [09:52<04:33,  8.03s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [09:59<04:21,  7.93s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [10:08<04:18,  8.09s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [10:17<04:17,  8.30s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [10:27<04:23,  8.77s/it]                                                 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [10:27<04:23,  8.77s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [10:36<04:21,  9.00s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [10:46<04:21,  9.35s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [10:55<04:10,  9.28s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [11:03<03:52,  8.94s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [11:11<03:35,  8.64s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [11:14<02:46,  6.92s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [11:23<02:54,  7.57s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [11:32<02:51,  7.82s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [11:40<02:47,  7.97s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [11:50<02:48,  8.44s/it]                                                 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [11:50<02:48,  8.44s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [11:57<02:33,  8.08s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [12:04<02:22,  7.92s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [12:12<02:10,  7.68s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [12:20<02:05,  7.85s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [12:27<01:56,  7.78s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [12:36<01:50,  7.90s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [12:43<01:40,  7.76s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [12:51<01:35,  7.96s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [13:01<01:32,  8.40s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [13:10<01:25,  8.54s/it]                                                 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [13:10<01:25,  8.54s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [13:20<01:21,  9.02s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [13:28<01:10,  8.78s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [13:37<01:01,  8.72s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [13:44<00:48,  8.15s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/100 [13:54<00:43,  8.74s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/100 [14:00<00:32,  8.13s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [14:09<00:24,  8.31s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 98/100 [14:18<00:16,  8.50s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [14:27<00:08,  8.51s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [14:35<00:00,  8.43s/it]                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [14:35<00:00,  8.43s/it]                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [14:36<00:00,  8.43s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [14:36<00:00,  8.76s/it]
Unsloth: Will smartly offload gradients to save VRAM!
   Step 10: 6484 tensors (CPU: 324, GPU: 6160), RAM: 4993.4 MB
{'loss': 0.2447, 'grad_norm': 0.29245343804359436, 'learning_rate': 0.000182, 'epoch': 0.27}
   Step 20: 6529 tensors (CPU: 330, GPU: 6199), RAM: 6388.6 MB
{'loss': 0.0972, 'grad_norm': 0.20599663257598877, 'learning_rate': 0.000162, 'epoch': 0.53}
   Step 30: 6529 tensors (CPU: 330, GPU: 6199), RAM: 8039.3 MB
{'loss': 0.0183, 'grad_norm': 0.14859752357006073, 'learning_rate': 0.000142, 'epoch': 0.8}
   Step 40: 6529 tensors (CPU: 330, GPU: 6199), RAM: 9432.4 MB
{'loss': 0.0023, 'grad_norm': 0.0390976183116436, 'learning_rate': 0.000122, 'epoch': 1.05}
   Step 50: 6529 tensors (CPU: 330, GPU: 6199), RAM: 9450.9 MB
{'loss': 0.001, 'grad_norm': 0.06221967190504074, 'learning_rate': 0.00010200000000000001, 'epoch': 1.32}
   Step 60: 6529 tensors (CPU: 330, GPU: 6199), RAM: 9461.5 MB
{'loss': 0.0006, 'grad_norm': 0.03618936985731125, 'learning_rate': 8.2e-05, 'epoch': 1.59}
   Step 70: 6529 tensors (CPU: 330, GPU: 6199), RAM: 9472.9 MB
{'loss': 0.0004, 'grad_norm': 0.030443761497735977, 'learning_rate': 6.2e-05, 'epoch': 1.85}
   Step 80: 6529 tensors (CPU: 330, GPU: 6199), RAM: 9503.2 MB
{'loss': 0.0004, 'grad_norm': 0.021928347647190094, 'learning_rate': 4.2e-05, 'epoch': 2.11}
   Step 90: 6529 tensors (CPU: 330, GPU: 6199), RAM: 9539.1 MB
{'loss': 0.0003, 'grad_norm': 0.013214468024671078, 'learning_rate': 2.2000000000000003e-05, 'epoch': 2.37}
   Step 100: 6529 tensors (CPU: 330, GPU: 6199), RAM: 9565.7 MB
{'loss': 0.0003, 'grad_norm': 0.020170385017991066, 'learning_rate': 2.0000000000000003e-06, 'epoch': 2.64}
{'train_runtime': 876.2756, 'train_samples_per_second': 0.913, 'train_steps_per_second': 0.114, 'train_loss': 0.03655168171040714, 'epoch': 2.64}
--------------------------------------------------------------------------------

5. Results:
   Memory before training: 1917.9 MB
   Memory after training:  9570.5 MB
   Total growth: +7652.6 MB

6. Step-by-step analysis:
   Steps 10->20: +1395.2 MB (139.5 MB/step)
   Steps 20->30: +1650.8 MB (165.1 MB/step)
   Steps 30->40: +1393.1 MB (139.3 MB/step)
   Steps 40->50: +18.6 MB (1.9 MB/step)
   Steps 50->60: +10.6 MB (1.1 MB/step)
   Steps 60->70: +11.4 MB (1.1 MB/step)
   Steps 70->80: +30.3 MB (3.0 MB/step)
   Steps 80->90: +35.9 MB (3.6 MB/step)
   Steps 90->100: +26.6 MB (2.7 MB/step)

================================================================================
TEST COMPLETE
================================================================================

Average leak rate: 50.8 MB/step
Extrapolated to 250 steps: 12700.9 MB

âš ï¸  LEAK CONFIRMED: Memory growing >10 MB/step
   This matches production behavior!
