import"../chunks/DsnmJJEf.js";import{q as Ta,p as Ea,a as Na,E as qa,f as u,h as Qa,b as l,c as Ia,$ as Ja,t as L,e as t,g as r,l as z,i as f,w as Wa,n as d,r as o,s as pa,m as B,j as C,k as S,d as A}from"../chunks/nG1nXqCg.js";import{i as c}from"../chunks/95pJECEZ.js";import{r as n,s as H,a as ma,e as va,i as ua}from"../chunks/0MiSZMzD.js";import{b as v,c as _a}from"../chunks/B_wnMFLY.js";import{b as xe}from"../chunks/D-D7rAU1.js";import{g as Da}from"../chunks/iuvDXYJF.js";import{B as he,C as Ga,a as Ua}from"../chunks/DYLRpWCU.js";function Ya(F,y){y.name&&!y.output_dir&&(y.output_dir=`./models/${y.name.toLowerCase().replace(/[^a-z0-9]/g,"-")}`)}var Ka=u('<div class="p-4 bg-red-50 border border-red-200 rounded-lg"><p class="text-red-700"> </p></div>'),Xa=(F,y)=>y.model_type="text",Za=u('<div class="w-full h-full rounded-full bg-white scale-50"></div>'),er=(F,y)=>y.model_type="vision",ar=u('<div class="w-full h-full rounded-full bg-white scale-50"></div>'),rr=u("<option> </option>"),tr=u("<option> </option>"),or=u('<p class="text-xs text-gray-500 mt-1">üé® Vision-language models can analyze images and text together</p>'),sr=u('Enter a HuggingFace dataset identifier (e.g., "username/dataset-name")<br/> For specific files, use: "username/repo::train.jsonl"',1),ir=u('<p class="text-sm text-blue-800 mb-2">HuggingFace datasets should use OpenAI messages format with base64 images:</p> <pre class="text-xs bg-blue-100 p-2 rounded overflow-x-auto"><code></code></pre> <p class="text-xs text-blue-700 mt-2"><strong>Example:</strong> <code>Barth371/train_pop_valet_no_wrong_doc</code></p>',1),lr=u('<p class="text-sm text-blue-800 mb-2">Your dataset should be in JSONL format with:</p> <pre class="text-xs bg-blue-100 p-2 rounded overflow-x-auto"><code></code></pre> <p class="text-xs text-blue-700 mt-2"><strong>Tip:</strong> Use <code>model-garden create-vision-dataset</code> CLI to generate sample data</p>',1),dr=u('<div class="p-4 bg-blue-50 border border-blue-200 rounded-lg"><h4 class="text-sm font-semibold text-blue-900 mb-2">üìã Vision Dataset Format</h4> <!></div>'),nr=u('<div class="mb-4 p-3 bg-yellow-50 border border-yellow-200 rounded-lg"><p class="text-sm text-yellow-800">‚ö†Ô∏è <strong>Vision models require:</strong> Lower batch size (1-2), higher gradient accumulation (8+), and lower learning rate (2e-5)</p></div>'),pr=u("<strong>‚úÖ Merged 16-bit (Recommended):</strong> Full model with LoRA weights merged using Unsloth. Creates split files for vLLM compatibility.",1),mr=u("<strong>üì¶ Merged 4-bit:</strong> Full model with LoRA weights merged in 4-bit quantized format. Smaller file size.",1),vr=u("<strong>üîß LoRA Adapters Only (Advanced):</strong> Saves only the adapter weights. Requires the base model to load.",1),ur=u('<form class="space-y-6"><!> <div><h3 class="text-lg font-semibold text-gray-900 mb-4">Basic Configuration</h3> <div class="grid grid-cols-1 gap-4"><div><div class="block text-sm font-medium text-gray-700 mb-2">Model Type *</div> <div class="grid grid-cols-2 gap-3"><button type="button"><div class="flex items-center gap-2"><div><!></div> <span class="font-medium">Text-Only (LLM)</span></div> <p class="text-sm text-gray-600 mt-2 ml-6">Fine-tune language models for text generation tasks</p></button> <button type="button"><div class="flex items-center gap-2"><div><!></div> <span class="font-medium">Vision-Language (VLM)</span></div> <p class="text-sm text-gray-600 mt-2 ml-6">Fine-tune multimodal models for image + text tasks</p></button></div></div> <div><label for="name" class="block text-sm font-medium text-gray-700 mb-1">Model Name *</label> <input type="text" id="name" placeholder="my-finance-model" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500" required/></div> <div><label for="base_model" class="block text-sm font-medium text-gray-700 mb-1">Base Model *</label> <select id="base_model" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500" required><!></select> <!></div> <div><label for="dataset_path" class="block text-sm font-medium text-gray-700 mb-1">Dataset Path *</label> <input type="text" id="dataset_path" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500" required/> <div class="mt-2 flex items-center"><input type="checkbox" id="from_hub" class="h-4 w-4 text-primary-600 focus:ring-primary-500 border-gray-300 rounded"/> <label for="from_hub" class="ml-2 block text-sm text-gray-700">Load from HuggingFace Hub</label></div> <p class="text-xs text-gray-500 mt-1"><!></p></div> <div><label for="validation_dataset_path" class="block text-sm font-medium text-gray-700 mb-1">Validation Dataset Path (Optional)</label> <input type="text" id="validation_dataset_path" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <div class="mt-2 flex items-center"><input type="checkbox" id="validation_from_hub" class="h-4 w-4 text-primary-600 focus:ring-primary-500 border-gray-300 rounded"/> <label for="validation_from_hub" class="ml-2 block text-sm text-gray-700">Load validation dataset from HuggingFace Hub</label></div> <p class="text-xs text-gray-500 mt-1">üìä Optional: Provide a validation dataset to track validation loss during training<br/> <!></p></div> <!> <div><label for="output_dir" class="block text-sm font-medium text-gray-700 mb-1">Output Directory</label> <input type="text" id="output_dir" placeholder="./models/my-model" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/></div></div></div> <div><h3 class="text-lg font-semibold text-gray-900 mb-4">Training Hyperparameters</h3> <!> <div class="grid grid-cols-2 gap-4"><div><label for="learning_rate" class="block text-sm font-medium text-gray-700 mb-1">Learning Rate</label> <input type="number" id="learning_rate" step="0.00001" min="0" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/></div> <div><label for="num_epochs" class="block text-sm font-medium text-gray-700 mb-1">Epochs</label> <input type="number" id="num_epochs" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/></div> <div><label for="batch_size" class="block text-sm font-medium text-gray-700 mb-1">Batch Size</label> <input type="number" id="batch_size" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/></div> <div><label for="gradient_accumulation" class="block text-sm font-medium text-gray-700 mb-1">Gradient Accumulation</label> <input type="number" id="gradient_accumulation" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Higher for vision models (8+)</p></div> <div><label for="max_steps" class="block text-sm font-medium text-gray-700 mb-1">Max Steps</label> <input type="number" id="max_steps" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">-1 for full epochs</p></div> <div><label for="warmup_steps" class="block text-sm font-medium text-gray-700 mb-1">Warmup Steps</label> <input type="number" id="warmup_steps" min="0" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Linear learning rate warmup</p></div> <div><label for="logging_steps" class="block text-sm font-medium text-gray-700 mb-1">Logging Steps</label> <input type="number" id="logging_steps" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Log metrics every N steps</p></div> <div><label for="save_steps" class="block text-sm font-medium text-gray-700 mb-1">Save Steps</label> <input type="number" id="save_steps" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Save checkpoint every N steps</p></div> <div><label for="eval_steps" class="block text-sm font-medium text-gray-700 mb-1">Evaluation Steps</label> <input type="number" id="eval_steps" placeholder="Auto (same as save_steps)" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Evaluate every N steps (only used if validation dataset provided)</p></div> <div class="col-span-2"><label for="optim" class="block text-sm font-medium text-gray-700 mb-1">Optimizer</label> <select id="optim" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"><option>AdamW 8-bit (Recommended - Memory Efficient)</option><option>AdamW (PyTorch)</option><option>AdamW Fused (Faster)</option><option>Adafactor (Very Memory Efficient)</option><option>SGD</option></select> <p class="text-xs text-gray-500 mt-1">8-bit AdamW is recommended for GPU memory efficiency</p></div></div></div> <div><h3 class="text-lg font-semibold text-gray-900 mb-4">LoRA Configuration</h3> <div class="grid grid-cols-3 gap-4"><div><label for="lora_r" class="block text-sm font-medium text-gray-700 mb-1">LoRA Rank (r)</label> <input type="number" id="lora_r" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Higher = more parameters</p></div> <div><label for="lora_alpha" class="block text-sm font-medium text-gray-700 mb-1">LoRA Alpha</label> <input type="number" id="lora_alpha" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Scaling factor</p></div> <div><label for="lora_dropout" class="block text-sm font-medium text-gray-700 mb-1">LoRA Dropout</label> <input type="number" id="lora_dropout" min="0" max="1" step="0.05" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">0 = no dropout (default)</p></div></div></div> <div><h3 class="text-lg font-semibold text-gray-900 mb-4">Model Save Options</h3> <div><label for="save_method" class="block text-sm font-medium text-gray-700 mb-2">Save Method</label> <select id="save_method" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"><option>Save Merged Model (16-bit) - Recommended</option><option>Save Merged Model (4-bit) - Smaller Size</option><option>Save LoRA Adapters Only - Advanced</option></select> <div class="mt-3 p-3 bg-blue-50 border border-blue-200 rounded-lg"><p class="text-sm text-blue-800"><!></p></div></div></div> <div class="flex gap-4 pt-4"><!> <!></div></form>'),_r=u('<div class="min-h-screen bg-gray-50"><div class="bg-white shadow"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex justify-between items-center py-6"><div class="flex items-center"><!> <h1 class="text-3xl font-bold text-gray-900 ml-4">New Training Job</h1></div></div></div></div> <div class="max-w-3xl mx-auto px-4 sm:px-6 lg:px-8 py-8"><!></div></div>');function kr(F,y){Ea(y,!0);let a=Na({name:"",model_type:"text",base_model:"unsloth/tinyllama-bnb-4bit",dataset_path:"./data/sample.jsonl",validation_dataset_path:"",output_dir:"",hyperparameters:{learning_rate:2e-4,num_epochs:3,batch_size:2,max_steps:-1,gradient_accumulation_steps:4,warmup_steps:10,logging_steps:10,save_steps:100,eval_steps:null,optim:"adamw_8bit"},lora_config:{r:16,lora_alpha:16,lora_dropout:0},from_hub:!1,validation_from_hub:!1,save_method:"merged_16bit"}),M=pa(!1),h=pa("");const we=["unsloth/tinyllama-bnb-4bit","unsloth/phi-2-bnb-4bit","unsloth/mistral-7b-bnb-4bit","unsloth/llama-2-7b-bnb-4bit","unsloth/llama-3-8b-bnb-4bit"],ke=["Qwen/Qwen2.5-VL-3B-Instruct","Qwen/Qwen2.5-VL-7B-Instruct","Qwen/Qwen2.5-VL-72B-Instruct","unsloth/Qwen2.5-VL-3B-Instruct-bnb-4bit","unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit"];qa(()=>{a.model_type==="vision"?(a.base_model=ke[0],a.dataset_path="./data/vision_dataset.jsonl",a.hyperparameters.batch_size=1,a.hyperparameters.gradient_accumulation_steps=8,a.hyperparameters.learning_rate=2e-5):(a.base_model=we[0],a.dataset_path="./data/sample.jsonl",a.hyperparameters.batch_size=2,a.hyperparameters.gradient_accumulation_steps=4,a.hyperparameters.learning_rate=2e-4)});async function ca(w){if(w.preventDefault(),!a.name||!a.base_model||!a.dataset_path){A(h,"Please fill in all required fields");return}A(M,!0),A(h,"");try{const x=await Ua.createTrainingJob({...a,is_vision:a.model_type==="vision"});x.success?Da(`/training/${x.data.job_id}`):A(h,"Failed to create training job")}catch(x){A(h,x instanceof Error?x.message:"Failed to create training job",!0)}finally{A(M,!1)}}var T=_r();Qa(w=>{Ja.title="New Training Job - Model Garden"});var E=t(T),Le=t(E),Se=t(Le),Ae=t(Se),ba=t(Ae);he(ba,{href:"/training",variant:"ghost",size:"sm",children:(w,x)=>{d();var k=L("‚Üê Training Jobs");l(w,k)},$$slots:{default:!0}}),d(2),o(Ae),o(Se),o(Le),o(E);var Me=r(E,2),ga=t(Me);Ga(ga,{children:(w,x)=>{var k=ur(),je=t(k);{var fa=e=>{var s=Ka(),p=t(s),b=t(p,!0);o(p),o(s),z(()=>B(b,f(h))),l(e,s)};c(je,e=>{f(h)&&e(fa)})}var N=r(je,2),ze=r(t(N),2),q=t(ze),Fe=r(t(q),2),j=t(Fe);j.__click=[Xa,a];var $e=t(j),Q=t($e),ya=t(Q);{var xa=e=>{var s=Za();l(e,s)};c(ya,e=>{a.model_type==="text"&&e(xa)})}o(Q),d(2),o($e),d(2),o(j);var $=r(j,2);$.__click=[er,a];var Re=t($),I=t(Re),ha=t(I);{var wa=e=>{var s=ar();l(e,s)};c(ha,e=>{a.model_type==="vision"&&e(wa)})}o(I),d(2),o(Re),d(2),o($),o(Fe),o(q);var J=r(q,2),W=r(t(J),2);n(W),W.__input=[Ya,a],o(J);var D=r(J,2),R=r(t(D),2),ka=t(R);{var La=e=>{var s=C(),p=S(s);va(p,17,()=>we,ua,(b,_)=>{var i=rr(),m=t(i,!0);o(i);var g={};z(()=>{B(m,f(_)),g!==(g=f(_))&&(i.value=(i.__value=f(_))??"")}),l(b,i)}),l(e,s)},Sa=e=>{var s=C(),p=S(s);va(p,17,()=>ke,ua,(b,_)=>{var i=tr(),m=t(i,!0);o(i);var g={};z(()=>{B(m,f(_)),g!==(g=f(_))&&(i.value=(i.__value=f(_))??"")}),l(b,i)}),l(e,s)};c(ka,e=>{a.model_type==="text"?e(La):e(Sa,!1)})}o(R);var Aa=r(R,2);{var Ma=e=>{var s=or();l(e,s)};c(Aa,e=>{a.model_type==="vision"&&e(Ma)})}o(D);var G=r(D,2),O=r(t(G),2);n(O);var U=r(O,2),Oe=t(U);n(Oe),d(2),o(U);var Pe=r(U,2),ja=t(Pe);{var za=e=>{var s=sr();d(2),l(e,s)},Fa=e=>{var s=C(),p=S(s);{var b=i=>{var m=L("Path to your JSONL dataset with image paths/base64 or local file");l(i,m)},_=i=>{var m=L("Path to your JSONL dataset file");l(i,m)};c(p,i=>{a.model_type==="vision"?i(b):i(_,!1)},!0)}l(e,s)};c(ja,e=>{a.from_hub?e(za):e(Fa,!1)})}o(Pe),o(G);var Y=r(G,2),P=r(t(Y),2);n(P);var K=r(P,2),Ve=t(K);n(Ve),d(2),o(K);var Be=r(K,2),$a=r(t(Be),3);{var Ra=e=>{var s=L('Use HuggingFace format: "username/repo" or "username/repo::validation.jsonl"');l(e,s)};c($a,e=>{a.validation_from_hub&&e(Ra)})}o(Be),o(Y);var Ce=r(Y,2);{var Oa=e=>{var s=dr(),p=r(t(s),2);{var b=i=>{var m=ir(),g=r(S(m),2),ye=t(g);ye.textContent='{"messages": [{"role": "user", "content": [{"type": "image", "image": "data:image/jpeg;base64,..."}, {"type": "text", "text": "What is shown?"}]}]}',o(g),d(2),l(i,m)},_=i=>{var m=lr(),g=r(S(m),2),ye=t(g);ye.textContent='{"text": "What is in this image?", "image": "/path/to/image.jpg", "response": "A cat sitting on a table"}',o(g),d(2),l(i,m)};c(p,i=>{a.from_hub?i(b):i(_,!1)})}o(s),l(e,s)};c(Ce,e=>{a.model_type==="vision"&&e(Oa)})}var He=r(Ce,2),Te=r(t(He),2);n(Te),o(He),o(ze),o(N);var X=r(N,2),Ee=r(t(X),2);{var Pa=e=>{var s=nr();l(e,s)};c(Ee,e=>{a.model_type==="vision"&&e(Pa)})}var Ne=r(Ee,2),Z=t(Ne),qe=r(t(Z),2);n(qe),o(Z);var ee=r(Z,2),Qe=r(t(ee),2);n(Qe),o(ee);var ae=r(ee,2),Ie=r(t(ae),2);n(Ie),o(ae);var re=r(ae,2),Je=r(t(re),2);n(Je),d(2),o(re);var te=r(re,2),We=r(t(te),2);n(We),d(2),o(te);var oe=r(te,2),De=r(t(oe),2);n(De),d(2),o(oe);var se=r(oe,2),Ge=r(t(se),2);n(Ge),d(2),o(se);var ie=r(se,2),Ue=r(t(ie),2);n(Ue),d(2),o(ie);var le=r(ie,2),Ye=r(t(le),2);n(Ye),d(2),o(le);var Ke=r(le,2),de=r(t(Ke),2),ne=t(de);ne.value=ne.__value="adamw_8bit";var pe=r(ne);pe.value=pe.__value="adamw_torch";var me=r(pe);me.value=me.__value="adamw_torch_fused";var ve=r(me);ve.value=ve.__value="adafactor";var Xe=r(ve);Xe.value=Xe.__value="sgd",o(de),d(2),o(Ke),o(Ne),o(X);var ue=r(X,2),Ze=r(t(ue),2),_e=t(Ze),ea=r(t(_e),2);n(ea),d(2),o(_e);var ce=r(_e,2),aa=r(t(ce),2);n(aa),d(2),o(ce);var ra=r(ce,2),ta=r(t(ra),2);n(ta),d(2),o(ra),o(Ze),o(ue);var be=r(ue,2),oa=r(t(be),2),V=r(t(oa),2),ge=t(V);ge.value=ge.__value="merged_16bit";var fe=r(ge);fe.value=fe.__value="merged_4bit";var sa=r(fe);sa.value=sa.__value="lora",o(V);var ia=r(V,2),la=t(ia),Va=t(la);{var Ba=e=>{var s=pr();d(),l(e,s)},Ca=e=>{var s=C(),p=S(s);{var b=i=>{var m=mr();d(),l(i,m)},_=i=>{var m=vr();d(),l(i,m)};c(p,i=>{a.save_method==="merged_4bit"?i(b):i(_,!1)},!0)}l(e,s)};c(Va,e=>{a.save_method==="merged_16bit"?e(Ba):e(Ca,!1)})}o(la),o(ia),o(oa),o(be);var da=r(be,2),na=t(da);he(na,{type:"submit",variant:"primary",get loading(){return f(M)},get disabled(){return f(M)},children:(e,s)=>{d();var p=L();z(()=>B(p,f(M)?"Creating...":"Start Training")),l(e,p)},$$slots:{default:!0}});var Ha=r(na,2);he(Ha,{href:"/training",variant:"secondary",children:(e,s)=>{d();var p=L("Cancel");l(e,p)},$$slots:{default:!0}}),o(da),o(k),z(()=>{H(j,1,`p-4 border-2 rounded-lg text-left transition-all ${a.model_type==="text"?"border-primary-500 bg-primary-50":"border-gray-300 hover:border-gray-400"}`),H(Q,1,`w-4 h-4 rounded-full border-2 ${a.model_type==="text"?"border-primary-500 bg-primary-500":"border-gray-400"}`),H($,1,`p-4 border-2 rounded-lg text-left transition-all ${a.model_type==="vision"?"border-primary-500 bg-primary-50":"border-gray-300 hover:border-gray-400"}`),H(I,1,`w-4 h-4 rounded-full border-2 ${a.model_type==="vision"?"border-primary-500 bg-primary-500":"border-gray-400"}`),ma(O,"placeholder",a.from_hub?"username/dataset-name":a.model_type==="vision"?"./data/vision_dataset.jsonl":"./data/my-dataset.jsonl"),ma(P,"placeholder",a.validation_from_hub?"username/val-dataset-name":a.model_type==="vision"?"./data/vision_val_dataset.jsonl":"./data/my-val-dataset.jsonl")}),Wa("submit",k,ca),v(W,()=>a.name,e=>a.name=e),xe(R,()=>a.base_model,e=>a.base_model=e),v(O,()=>a.dataset_path,e=>a.dataset_path=e),_a(Oe,()=>a.from_hub,e=>a.from_hub=e),v(P,()=>a.validation_dataset_path,e=>a.validation_dataset_path=e),_a(Ve,()=>a.validation_from_hub,e=>a.validation_from_hub=e),v(Te,()=>a.output_dir,e=>a.output_dir=e),v(qe,()=>a.hyperparameters.learning_rate,e=>a.hyperparameters.learning_rate=e),v(Qe,()=>a.hyperparameters.num_epochs,e=>a.hyperparameters.num_epochs=e),v(Ie,()=>a.hyperparameters.batch_size,e=>a.hyperparameters.batch_size=e),v(Je,()=>a.hyperparameters.gradient_accumulation_steps,e=>a.hyperparameters.gradient_accumulation_steps=e),v(We,()=>a.hyperparameters.max_steps,e=>a.hyperparameters.max_steps=e),v(De,()=>a.hyperparameters.warmup_steps,e=>a.hyperparameters.warmup_steps=e),v(Ge,()=>a.hyperparameters.logging_steps,e=>a.hyperparameters.logging_steps=e),v(Ue,()=>a.hyperparameters.save_steps,e=>a.hyperparameters.save_steps=e),v(Ye,()=>a.hyperparameters.eval_steps,e=>a.hyperparameters.eval_steps=e),xe(de,()=>a.hyperparameters.optim,e=>a.hyperparameters.optim=e),v(ea,()=>a.lora_config.r,e=>a.lora_config.r=e),v(aa,()=>a.lora_config.lora_alpha,e=>a.lora_config.lora_alpha=e),v(ta,()=>a.lora_config.lora_dropout,e=>a.lora_config.lora_dropout=e),xe(V,()=>a.save_method,e=>a.save_method=e),l(w,k)},$$slots:{default:!0}}),o(Me),o(T),l(F,T),Ia()}Ta(["click","input"]);export{kr as component};
