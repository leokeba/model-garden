import"../chunks/DsnmJJEf.js";import{w as er,G as rr,H as ar,I as tr,J as ir,K as sr,p as or,a as lr,M as dr,f as g,h as nr,b as d,c as ur,$ as pr,t as S,e as a,n as v,r as t,g as i,i as c,s as qe,l as q,j as Be,k as Ve,d as z}from"../chunks/BiS4-4H5.js";import{d as mr,s as J,e as vr}from"../chunks/a3GB5oE2.js";import{i as y}from"../chunks/Ds2ovkUt.js";import{B as ae,C as cr,s as P,e as Ce,i as Te,a as _r}from"../chunks/CiTacZsq.js";import{r as _,s as br}from"../chunks/BeoOJVqf.js";import{b}from"../chunks/7JXFuezT.js";import{g as gr}from"../chunks/C8064N31.js";function Ie(s,l,r=!1){if(s.multiple){if(l==null)return;if(!tr(l))return ir();for(var u of s.options)u.selected=l.includes(B(u));return}for(u of s.options){var n=B(u);if(sr(n,l)){u.selected=!0;return}}(!r||l!==void 0)&&(s.selectedIndex=-1)}function fr(s){var l=new MutationObserver(()=>{Ie(s,s.__value)});l.observe(s,{childList:!0,subtree:!0,attributes:!0,attributeFilter:["value"]}),ar(()=>{l.disconnect()})}function yr(s,l,r=l){var u=!0;er(s,"change",n=>{var f=n?"[selected]":":checked",k;if(s.multiple)k=[].map.call(s.querySelectorAll(f),B);else{var V=s.querySelector(f)??s.querySelector("option:not([disabled])");k=V&&B(V)}r(k)}),rr(()=>{var n=l();if(Ie(s,n,u),u&&n===void 0){var f=s.querySelector(":checked");f!==null&&(n=B(f),r(n))}s.__value=n,u=!1}),fr(s)}function B(s){return"__value"in s?s.__value:s.value}function xr(s,l){l.name&&!l.output_dir&&(l.output_dir=`./models/${l.name.toLowerCase().replace(/[^a-z0-9]/g,"-")}`)}var hr=g('<div class="p-4 bg-red-50 border border-red-200 rounded-lg"><p class="text-red-700"> </p></div>'),wr=(s,l)=>l.model_type="text",kr=g('<div class="w-full h-full rounded-full bg-white scale-50"></div>'),$r=(s,l)=>l.model_type="vision",Lr=g('<div class="w-full h-full rounded-full bg-white scale-50"></div>'),zr=g("<option> </option>"),jr=g("<option> </option>"),Mr=g('<p class="text-xs text-gray-500 mt-1">üé® Vision-language models can analyze images and text together</p>'),Sr=g('<div class="p-4 bg-blue-50 border border-blue-200 rounded-lg"><h4 class="text-sm font-semibold text-blue-900 mb-2">üìã Vision Dataset Format</h4> <p class="text-sm text-blue-800 mb-2">Your dataset should be in JSONL format with:</p> <pre class="text-xs bg-blue-100 p-2 rounded overflow-x-auto"><code></code></pre> <p class="text-xs text-blue-700 mt-2"><strong>Tip:</strong> Use <code>model-garden create-vision-dataset</code> CLI to generate sample data</p></div>'),qr=g('<div class="mb-4 p-3 bg-yellow-50 border border-yellow-200 rounded-lg"><p class="text-sm text-yellow-800">‚ö†Ô∏è <strong>Vision models require:</strong> Lower batch size (1-2), higher gradient accumulation (8+), and lower learning rate (2e-5)</p></div>'),Br=g('<form class="space-y-6"><!> <div><h3 class="text-lg font-semibold text-gray-900 mb-4">Basic Configuration</h3> <div class="grid grid-cols-1 gap-4"><div><div class="block text-sm font-medium text-gray-700 mb-2">Model Type *</div> <div class="grid grid-cols-2 gap-3"><button type="button"><div class="flex items-center gap-2"><div><!></div> <span class="font-medium">Text-Only (LLM)</span></div> <p class="text-sm text-gray-600 mt-2 ml-6">Fine-tune language models for text generation tasks</p></button> <button type="button"><div class="flex items-center gap-2"><div><!></div> <span class="font-medium">Vision-Language (VLM)</span></div> <p class="text-sm text-gray-600 mt-2 ml-6">Fine-tune multimodal models for image + text tasks</p></button></div></div> <div><label for="name" class="block text-sm font-medium text-gray-700 mb-1">Model Name *</label> <input type="text" id="name" placeholder="my-finance-model" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500" required/></div> <div><label for="base_model" class="block text-sm font-medium text-gray-700 mb-1">Base Model *</label> <select id="base_model" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500" required><!></select> <!></div> <div><label for="dataset_path" class="block text-sm font-medium text-gray-700 mb-1">Dataset Path *</label> <input type="text" id="dataset_path" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500" required/> <p class="text-xs text-gray-500 mt-1"><!></p></div> <!> <div><label for="output_dir" class="block text-sm font-medium text-gray-700 mb-1">Output Directory</label> <input type="text" id="output_dir" placeholder="./models/my-model" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/></div></div></div> <div><h3 class="text-lg font-semibold text-gray-900 mb-4">Training Hyperparameters</h3> <!> <div class="grid grid-cols-2 gap-4"><div><label for="learning_rate" class="block text-sm font-medium text-gray-700 mb-1">Learning Rate</label> <input type="number" id="learning_rate" step="0.00001" min="0" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/></div> <div><label for="num_epochs" class="block text-sm font-medium text-gray-700 mb-1">Epochs</label> <input type="number" id="num_epochs" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/></div> <div><label for="batch_size" class="block text-sm font-medium text-gray-700 mb-1">Batch Size</label> <input type="number" id="batch_size" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/></div> <div><label for="gradient_accumulation" class="block text-sm font-medium text-gray-700 mb-1">Gradient Accumulation</label> <input type="number" id="gradient_accumulation" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Higher for vision models (8+)</p></div> <div><label for="max_steps" class="block text-sm font-medium text-gray-700 mb-1">Max Steps</label> <input type="number" id="max_steps" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">-1 for full epochs</p></div></div></div> <div><h3 class="text-lg font-semibold text-gray-900 mb-4">LoRA Configuration</h3> <div class="grid grid-cols-2 gap-4"><div><label for="lora_r" class="block text-sm font-medium text-gray-700 mb-1">LoRA Rank (r)</label> <input type="number" id="lora_r" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/></div> <div><label for="lora_alpha" class="block text-sm font-medium text-gray-700 mb-1">LoRA Alpha</label> <input type="number" id="lora_alpha" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/></div></div></div> <div class="flex gap-4 pt-4"><!> <!></div></form>'),Vr=g('<div class="min-h-screen bg-gray-50"><div class="bg-white shadow"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex justify-between items-center py-6"><div class="flex items-center"><!> <h1 class="text-3xl font-bold text-gray-900 ml-4">New Training Job</h1></div></div></div></div> <div class="max-w-3xl mx-auto px-4 sm:px-6 lg:px-8 py-8"><!></div></div>');function Fr(s,l){or(l,!0);let r=lr({name:"",model_type:"text",base_model:"unsloth/tinyllama-bnb-4bit",dataset_path:"./data/sample.jsonl",output_dir:"",hyperparameters:{learning_rate:2e-4,num_epochs:3,batch_size:2,max_steps:-1,gradient_accumulation_steps:4},lora_config:{r:16,lora_alpha:16},from_hub:!1}),u=qe(!1),n=qe("");const f=["unsloth/tinyllama-bnb-4bit","unsloth/phi-2-bnb-4bit","unsloth/mistral-7b-bnb-4bit","unsloth/llama-2-7b-bnb-4bit","unsloth/llama-3-8b-bnb-4bit"],k=["Qwen/Qwen2.5-VL-3B-Instruct","Qwen/Qwen2.5-VL-7B-Instruct","Qwen/Qwen2.5-VL-72B-Instruct","unsloth/Qwen2.5-VL-3B-Instruct-bnb-4bit","unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit"];dr(()=>{r.model_type==="vision"?(r.base_model=k[0],r.dataset_path="./data/vision_dataset.jsonl",r.hyperparameters.batch_size=1,r.hyperparameters.gradient_accumulation_steps=8,r.hyperparameters.learning_rate=2e-5):(r.base_model=f[0],r.dataset_path="./data/sample.jsonl",r.hyperparameters.batch_size=2,r.hyperparameters.gradient_accumulation_steps=4,r.hyperparameters.learning_rate=2e-4)});async function V($){if($.preventDefault(),!r.name||!r.base_model||!r.dataset_path){z(n,"Please fill in all required fields");return}z(u,!0),z(n,"");try{const x=await _r.createTrainingJob({...r,is_vision:r.model_type==="vision"});x.success?gr(`/training/${x.data.job_id}`):z(n,"Failed to create training job")}catch(x){z(n,x instanceof Error?x.message:"Failed to create training job",!0)}finally{z(u,!1)}}var Q=Vr();nr($=>{pr.title="New Training Job - Model Garden"});var A=a(Q),te=a(A),ie=a(te),se=a(ie),Je=a(se);ae(Je,{href:"/training",variant:"ghost",size:"sm",children:($,x)=>{v();var L=S("‚Üê Training Jobs");d($,L)},$$slots:{default:!0}}),v(2),t(se),t(ie),t(te),t(A);var oe=i(A,2),Pe=a(oe);cr(Pe,{children:($,x)=>{var L=Br(),le=a(L);{var Qe=e=>{var o=hr(),p=a(o),h=a(p,!0);t(p),t(o),q(()=>J(h,c(n))),d(e,o)};y(le,e=>{c(n)&&e(Qe)})}var O=i(le,2),de=i(a(O),2),F=a(de),ne=i(a(F),2),j=a(ne);j.__click=[wr,r];var ue=a(j),N=a(ue),Ae=a(N);{var Oe=e=>{var o=kr();d(e,o)};y(Ae,e=>{r.model_type==="text"&&e(Oe)})}t(N),v(2),t(ue),v(2),t(j);var C=i(j,2);C.__click=[$r,r];var pe=a(C),R=a(pe),Fe=a(R);{var Ne=e=>{var o=Lr();d(e,o)};y(Fe,e=>{r.model_type==="vision"&&e(Ne)})}t(R),v(2),t(pe),v(2),t(C),t(ne),t(F);var D=i(F,2),G=i(a(D),2);_(G),G.__input=[xr,r],t(D);var H=i(D,2),T=i(a(H),2),Re=a(T);{var De=e=>{var o=Be(),p=Ve(o);Ce(p,17,()=>f,Te,(h,w)=>{var m=zr(),re=a(m,!0);t(m);var M={};q(()=>{J(re,c(w)),M!==(M=c(w))&&(m.value=(m.__value=c(w))??"")}),d(h,m)}),d(e,o)},Ge=e=>{var o=Be(),p=Ve(o);Ce(p,17,()=>k,Te,(h,w)=>{var m=jr(),re=a(m,!0);t(m);var M={};q(()=>{J(re,c(w)),M!==(M=c(w))&&(m.value=(m.__value=c(w))??"")}),d(h,m)}),d(e,o)};y(Re,e=>{r.model_type==="text"?e(De):e(Ge,!1)})}t(T);var He=i(T,2);{var Ee=e=>{var o=Mr();d(e,o)};y(He,e=>{r.model_type==="vision"&&e(Ee)})}t(H);var E=i(H,2),I=i(a(E),2);_(I);var me=i(I,2),Ke=a(me);{var Ue=e=>{var o=S("Path to your JSONL dataset with image paths and text");d(e,o)},We=e=>{var o=S("Path to your JSONL dataset file");d(e,o)};y(Ke,e=>{r.model_type==="vision"?e(Ue):e(We,!1)})}t(me),t(E);var ve=i(E,2);{var Ye=e=>{var o=Sr(),p=i(a(o),4),h=a(p);h.textContent='{"text": "What is in this image?", "image": "/path/to/image.jpg", "response": "A cat sitting on a table"}',t(p),v(2),t(o),d(e,o)};y(ve,e=>{r.model_type==="vision"&&e(Ye)})}var ce=i(ve,2),_e=i(a(ce),2);_(_e),t(ce),t(de),t(O);var K=i(O,2),be=i(a(K),2);{var Xe=e=>{var o=qr();d(e,o)};y(be,e=>{r.model_type==="vision"&&e(Xe)})}var ge=i(be,2),U=a(ge),fe=i(a(U),2);_(fe),t(U);var W=i(U,2),ye=i(a(W),2);_(ye),t(W);var Y=i(W,2),xe=i(a(Y),2);_(xe),t(Y);var X=i(Y,2),he=i(a(X),2);_(he),v(2),t(X);var we=i(X,2),ke=i(a(we),2);_(ke),v(2),t(we),t(ge),t(K);var Z=i(K,2),$e=i(a(Z),2),ee=a($e),Le=i(a(ee),2);_(Le),t(ee);var ze=i(ee,2),je=i(a(ze),2);_(je),t(ze),t($e),t(Z);var Me=i(Z,2),Se=a(Me);ae(Se,{type:"submit",variant:"primary",get loading(){return c(u)},get disabled(){return c(u)},children:(e,o)=>{v();var p=S();q(()=>J(p,c(u)?"Creating...":"Start Training")),d(e,p)},$$slots:{default:!0}});var Ze=i(Se,2);ae(Ze,{href:"/training",variant:"secondary",children:(e,o)=>{v();var p=S("Cancel");d(e,p)},$$slots:{default:!0}}),t(Me),t(L),q(()=>{P(j,1,`p-4 border-2 rounded-lg text-left transition-all ${r.model_type==="text"?"border-primary-500 bg-primary-50":"border-gray-300 hover:border-gray-400"}`),P(N,1,`w-4 h-4 rounded-full border-2 ${r.model_type==="text"?"border-primary-500 bg-primary-500":"border-gray-400"}`),P(C,1,`p-4 border-2 rounded-lg text-left transition-all ${r.model_type==="vision"?"border-primary-500 bg-primary-50":"border-gray-300 hover:border-gray-400"}`),P(R,1,`w-4 h-4 rounded-full border-2 ${r.model_type==="vision"?"border-primary-500 bg-primary-500":"border-gray-400"}`),br(I,"placeholder",r.model_type==="vision"?"./data/vision_dataset.jsonl":"./data/my-dataset.jsonl")}),vr("submit",L,V),b(G,()=>r.name,e=>r.name=e),yr(T,()=>r.base_model,e=>r.base_model=e),b(I,()=>r.dataset_path,e=>r.dataset_path=e),b(_e,()=>r.output_dir,e=>r.output_dir=e),b(fe,()=>r.hyperparameters.learning_rate,e=>r.hyperparameters.learning_rate=e),b(ye,()=>r.hyperparameters.num_epochs,e=>r.hyperparameters.num_epochs=e),b(xe,()=>r.hyperparameters.batch_size,e=>r.hyperparameters.batch_size=e),b(he,()=>r.hyperparameters.gradient_accumulation_steps,e=>r.hyperparameters.gradient_accumulation_steps=e),b(ke,()=>r.hyperparameters.max_steps,e=>r.hyperparameters.max_steps=e),b(Le,()=>r.lora_config.r,e=>r.lora_config.r=e),b(je,()=>r.lora_config.lora_alpha,e=>r.lora_config.lora_alpha=e),d($,L)},$$slots:{default:!0}}),t(oe),t(Q),d(s,Q),ur()}mr(["click","input"]);export{Fr as component};
