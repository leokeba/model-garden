import"../chunks/DsnmJJEf.js";import{q as aa,p as ra,a as ta,D as ia,f as m,h as sa,b as l,c as oa,$ as la,t as L,e as r,g as i,l as B,i as c,E as da,n as p,r as t,s as Fe,m as P,j as ae,k as C,d as $}from"../chunks/ZXdHezLC.js";import{i as g}from"../chunks/CnQbhIlQ.js";import{r as b,a as Q,s as na,e as Pe,i as Qe}from"../chunks/C2YVWTdS.js";import{b as f,c as pa}from"../chunks/Bbu82ZTT.js";import{b as ma}from"../chunks/RUopsz-q.js";import{g as ua}from"../chunks/D3b1njJk.js";import{B as re,C as va,a as ca}from"../chunks/ByOzdI1s.js";function ba(V,_){_.name&&!_.output_dir&&(_.output_dir=`./models/${_.name.toLowerCase().replace(/[^a-z0-9]/g,"-")}`)}var _a=m('<div class="p-4 bg-red-50 border border-red-200 rounded-lg"><p class="text-red-700"> </p></div>'),ga=(V,_)=>_.model_type="text",fa=m('<div class="w-full h-full rounded-full bg-white scale-50"></div>'),ya=(V,_)=>_.model_type="vision",xa=m('<div class="w-full h-full rounded-full bg-white scale-50"></div>'),ha=m("<option> </option>"),wa=m("<option> </option>"),ka=m('<p class="text-xs text-gray-500 mt-1">üé® Vision-language models can analyze images and text together</p>'),La=m('<p class="text-sm text-blue-800 mb-2">HuggingFace datasets should use OpenAI messages format with base64 images:</p> <pre class="text-xs bg-blue-100 p-2 rounded overflow-x-auto"><code></code></pre> <p class="text-xs text-blue-700 mt-2"><strong>Example:</strong> <code>Barth371/train_pop_valet_no_wrong_doc</code></p>',1),$a=m('<p class="text-sm text-blue-800 mb-2">Your dataset should be in JSONL format with:</p> <pre class="text-xs bg-blue-100 p-2 rounded overflow-x-auto"><code></code></pre> <p class="text-xs text-blue-700 mt-2"><strong>Tip:</strong> Use <code>model-garden create-vision-dataset</code> CLI to generate sample data</p>',1),ja=m('<div class="p-4 bg-blue-50 border border-blue-200 rounded-lg"><h4 class="text-sm font-semibold text-blue-900 mb-2">üìã Vision Dataset Format</h4> <!></div>'),za=m('<div class="mb-4 p-3 bg-yellow-50 border border-yellow-200 rounded-lg"><p class="text-sm text-yellow-800">‚ö†Ô∏è <strong>Vision models require:</strong> Lower batch size (1-2), higher gradient accumulation (8+), and lower learning rate (2e-5)</p></div>'),Ba=m('<form class="space-y-6"><!> <div><h3 class="text-lg font-semibold text-gray-900 mb-4">Basic Configuration</h3> <div class="grid grid-cols-1 gap-4"><div><div class="block text-sm font-medium text-gray-700 mb-2">Model Type *</div> <div class="grid grid-cols-2 gap-3"><button type="button"><div class="flex items-center gap-2"><div><!></div> <span class="font-medium">Text-Only (LLM)</span></div> <p class="text-sm text-gray-600 mt-2 ml-6">Fine-tune language models for text generation tasks</p></button> <button type="button"><div class="flex items-center gap-2"><div><!></div> <span class="font-medium">Vision-Language (VLM)</span></div> <p class="text-sm text-gray-600 mt-2 ml-6">Fine-tune multimodal models for image + text tasks</p></button></div></div> <div><label for="name" class="block text-sm font-medium text-gray-700 mb-1">Model Name *</label> <input type="text" id="name" placeholder="my-finance-model" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500" required/></div> <div><label for="base_model" class="block text-sm font-medium text-gray-700 mb-1">Base Model *</label> <select id="base_model" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500" required><!></select> <!></div> <div><label for="dataset_path" class="block text-sm font-medium text-gray-700 mb-1">Dataset Path *</label> <input type="text" id="dataset_path" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500" required/> <div class="mt-2 flex items-center"><input type="checkbox" id="from_hub" class="h-4 w-4 text-primary-600 focus:ring-primary-500 border-gray-300 rounded"/> <label for="from_hub" class="ml-2 block text-sm text-gray-700">Load from HuggingFace Hub</label></div> <p class="text-xs text-gray-500 mt-1"><!></p></div> <!> <div><label for="output_dir" class="block text-sm font-medium text-gray-700 mb-1">Output Directory</label> <input type="text" id="output_dir" placeholder="./models/my-model" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/></div></div></div> <div><h3 class="text-lg font-semibold text-gray-900 mb-4">Training Hyperparameters</h3> <!> <div class="grid grid-cols-2 gap-4"><div><label for="learning_rate" class="block text-sm font-medium text-gray-700 mb-1">Learning Rate</label> <input type="number" id="learning_rate" step="0.00001" min="0" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/></div> <div><label for="num_epochs" class="block text-sm font-medium text-gray-700 mb-1">Epochs</label> <input type="number" id="num_epochs" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/></div> <div><label for="batch_size" class="block text-sm font-medium text-gray-700 mb-1">Batch Size</label> <input type="number" id="batch_size" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/></div> <div><label for="gradient_accumulation" class="block text-sm font-medium text-gray-700 mb-1">Gradient Accumulation</label> <input type="number" id="gradient_accumulation" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Higher for vision models (8+)</p></div> <div><label for="max_steps" class="block text-sm font-medium text-gray-700 mb-1">Max Steps</label> <input type="number" id="max_steps" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">-1 for full epochs</p></div></div></div> <div><h3 class="text-lg font-semibold text-gray-900 mb-4">LoRA Configuration</h3> <div class="grid grid-cols-2 gap-4"><div><label for="lora_r" class="block text-sm font-medium text-gray-700 mb-1">LoRA Rank (r)</label> <input type="number" id="lora_r" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/></div> <div><label for="lora_alpha" class="block text-sm font-medium text-gray-700 mb-1">LoRA Alpha</label> <input type="number" id="lora_alpha" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/></div></div></div> <div class="flex gap-4 pt-4"><!> <!></div></form>'),Ca=m('<div class="min-h-screen bg-gray-50"><div class="bg-white shadow"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex justify-between items-center py-6"><div class="flex items-center"><!> <h1 class="text-3xl font-bold text-gray-900 ml-4">New Training Job</h1></div></div></div></div> <div class="max-w-3xl mx-auto px-4 sm:px-6 lg:px-8 py-8"><!></div></div>');function Ja(V,_){ra(_,!0);let a=ta({name:"",model_type:"text",base_model:"unsloth/tinyllama-bnb-4bit",dataset_path:"./data/sample.jsonl",output_dir:"",hyperparameters:{learning_rate:2e-4,num_epochs:3,batch_size:2,max_steps:-1,gradient_accumulation_steps:4},lora_config:{r:16,lora_alpha:16},from_hub:!1}),j=Fe(!1),h=Fe("");const te=["unsloth/tinyllama-bnb-4bit","unsloth/phi-2-bnb-4bit","unsloth/mistral-7b-bnb-4bit","unsloth/llama-2-7b-bnb-4bit","unsloth/llama-3-8b-bnb-4bit"],ie=["Qwen/Qwen2.5-VL-3B-Instruct","Qwen/Qwen2.5-VL-7B-Instruct","Qwen/Qwen2.5-VL-72B-Instruct","unsloth/Qwen2.5-VL-3B-Instruct-bnb-4bit","unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit"];ia(()=>{a.model_type==="vision"?(a.base_model=ie[0],a.dataset_path="./data/vision_dataset.jsonl",a.hyperparameters.batch_size=1,a.hyperparameters.gradient_accumulation_steps=8,a.hyperparameters.learning_rate=2e-5):(a.base_model=te[0],a.dataset_path="./data/sample.jsonl",a.hyperparameters.batch_size=2,a.hyperparameters.gradient_accumulation_steps=4,a.hyperparameters.learning_rate=2e-4)});async function Ae(w){if(w.preventDefault(),!a.name||!a.base_model||!a.dataset_path){$(h,"Please fill in all required fields");return}$(j,!0),$(h,"");try{const x=await ca.createTrainingJob({...a,is_vision:a.model_type==="vision"});x.success?ua(`/training/${x.data.job_id}`):$(h,"Failed to create training job")}catch(x){$(h,x instanceof Error?x.message:"Failed to create training job",!0)}finally{$(j,!1)}}var A=Ca();sa(w=>{la.title="New Training Job - Model Garden"});var I=r(A),se=r(I),oe=r(se),le=r(oe),Ie=r(le);re(Ie,{href:"/training",variant:"ghost",size:"sm",children:(w,x)=>{p();var k=L("‚Üê Training Jobs");l(w,k)},$$slots:{default:!0}}),p(2),t(le),t(oe),t(se),t(I);var de=i(I,2),Je=r(de);va(Je,{children:(w,x)=>{var k=Ba(),ne=r(k);{var Oe=e=>{var s=_a(),d=r(s),y=r(d,!0);t(d),t(s),B(()=>P(y,c(h))),l(e,s)};g(ne,e=>{c(h)&&e(Oe)})}var J=i(ne,2),pe=i(r(J),2),O=r(pe),me=i(r(O),2),z=r(me);z.__click=[ga,a];var ue=r(z),S=r(ue),Se=r(S);{var qe=e=>{var s=fa();l(e,s)};g(Se,e=>{a.model_type==="text"&&e(qe)})}t(S),p(2),t(ue),p(2),t(z);var M=i(z,2);M.__click=[ya,a];var ve=r(M),q=r(ve),He=r(q);{var Ne=e=>{var s=xa();l(e,s)};g(He,e=>{a.model_type==="vision"&&e(Ne)})}t(q),p(2),t(ve),p(2),t(M),t(me),t(O);var H=i(O,2),N=i(r(H),2);b(N),N.__input=[ba,a],t(H);var E=i(H,2),T=i(r(E),2),Ee=r(T);{var Re=e=>{var s=ae(),d=C(s);Pe(d,17,()=>te,Qe,(y,u)=>{var o=ha(),n=r(o,!0);t(o);var v={};B(()=>{P(n,c(u)),v!==(v=c(u))&&(o.value=(o.__value=c(u))??"")}),l(y,o)}),l(e,s)},De=e=>{var s=ae(),d=C(s);Pe(d,17,()=>ie,Qe,(y,u)=>{var o=wa(),n=r(o,!0);t(o);var v={};B(()=>{P(n,c(u)),v!==(v=c(u))&&(o.value=(o.__value=c(u))??"")}),l(y,o)}),l(e,s)};g(Ee,e=>{a.model_type==="text"?e(Re):e(De,!1)})}t(T);var Ge=i(T,2);{var We=e=>{var s=ka();l(e,s)};g(Ge,e=>{a.model_type==="vision"&&e(We)})}t(E);var R=i(E,2),F=i(r(R),2);b(F);var D=i(F,2),ce=r(D);b(ce),p(2),t(D);var be=i(D,2),Ue=r(be);{var Ye=e=>{var s=L('Enter a HuggingFace dataset identifier (e.g., "username/dataset-name")');l(e,s)},Ke=e=>{var s=ae(),d=C(s);{var y=o=>{var n=L("Path to your JSONL dataset with image paths/base64 or local file");l(o,n)},u=o=>{var n=L("Path to your JSONL dataset file");l(o,n)};g(d,o=>{a.model_type==="vision"?o(y):o(u,!1)},!0)}l(e,s)};g(Ue,e=>{a.from_hub?e(Ye):e(Ke,!1)})}t(be),t(R);var _e=i(R,2);{var Xe=e=>{var s=ja(),d=i(r(s),2);{var y=o=>{var n=La(),v=i(C(n),2),ee=r(v);ee.textContent='{"messages": [{"role": "user", "content": [{"type": "image", "image": "data:image/jpeg;base64,..."}, {"type": "text", "text": "What is shown?"}]}]}',t(v),p(2),l(o,n)},u=o=>{var n=$a(),v=i(C(n),2),ee=r(v);ee.textContent='{"text": "What is in this image?", "image": "/path/to/image.jpg", "response": "A cat sitting on a table"}',t(v),p(2),l(o,n)};g(d,o=>{a.from_hub?o(y):o(u,!1)})}t(s),l(e,s)};g(_e,e=>{a.model_type==="vision"&&e(Xe)})}var ge=i(_e,2),fe=i(r(ge),2);b(fe),t(ge),t(pe),t(J);var G=i(J,2),ye=i(r(G),2);{var Ze=e=>{var s=za();l(e,s)};g(ye,e=>{a.model_type==="vision"&&e(Ze)})}var xe=i(ye,2),W=r(xe),he=i(r(W),2);b(he),t(W);var U=i(W,2),we=i(r(U),2);b(we),t(U);var Y=i(U,2),ke=i(r(Y),2);b(ke),t(Y);var K=i(Y,2),Le=i(r(K),2);b(Le),p(2),t(K);var $e=i(K,2),je=i(r($e),2);b(je),p(2),t($e),t(xe),t(G);var X=i(G,2),ze=i(r(X),2),Z=r(ze),Be=i(r(Z),2);b(Be),t(Z);var Ce=i(Z,2),Ve=i(r(Ce),2);b(Ve),t(Ce),t(ze),t(X);var Me=i(X,2),Te=r(Me);re(Te,{type:"submit",variant:"primary",get loading(){return c(j)},get disabled(){return c(j)},children:(e,s)=>{p();var d=L();B(()=>P(d,c(j)?"Creating...":"Start Training")),l(e,d)},$$slots:{default:!0}});var ea=i(Te,2);re(ea,{href:"/training",variant:"secondary",children:(e,s)=>{p();var d=L("Cancel");l(e,d)},$$slots:{default:!0}}),t(Me),t(k),B(()=>{Q(z,1,`p-4 border-2 rounded-lg text-left transition-all ${a.model_type==="text"?"border-primary-500 bg-primary-50":"border-gray-300 hover:border-gray-400"}`),Q(S,1,`w-4 h-4 rounded-full border-2 ${a.model_type==="text"?"border-primary-500 bg-primary-500":"border-gray-400"}`),Q(M,1,`p-4 border-2 rounded-lg text-left transition-all ${a.model_type==="vision"?"border-primary-500 bg-primary-50":"border-gray-300 hover:border-gray-400"}`),Q(q,1,`w-4 h-4 rounded-full border-2 ${a.model_type==="vision"?"border-primary-500 bg-primary-500":"border-gray-400"}`),na(F,"placeholder",a.from_hub?"username/dataset-name":a.model_type==="vision"?"./data/vision_dataset.jsonl":"./data/my-dataset.jsonl")}),da("submit",k,Ae),f(N,()=>a.name,e=>a.name=e),ma(T,()=>a.base_model,e=>a.base_model=e),f(F,()=>a.dataset_path,e=>a.dataset_path=e),pa(ce,()=>a.from_hub,e=>a.from_hub=e),f(fe,()=>a.output_dir,e=>a.output_dir=e),f(he,()=>a.hyperparameters.learning_rate,e=>a.hyperparameters.learning_rate=e),f(we,()=>a.hyperparameters.num_epochs,e=>a.hyperparameters.num_epochs=e),f(ke,()=>a.hyperparameters.batch_size,e=>a.hyperparameters.batch_size=e),f(Le,()=>a.hyperparameters.gradient_accumulation_steps,e=>a.hyperparameters.gradient_accumulation_steps=e),f(je,()=>a.hyperparameters.max_steps,e=>a.hyperparameters.max_steps=e),f(Be,()=>a.lora_config.r,e=>a.lora_config.r=e),f(Ve,()=>a.lora_config.lora_alpha,e=>a.lora_config.lora_alpha=e),l(w,k)},$$slots:{default:!0}}),t(de),t(A),l(V,A),oa()}aa(["click","input"]);export{Ja as component};
