import"../chunks/DsnmJJEf.js";import{q as ha,p as wa,a as ka,D as La,f as m,h as Ma,b as l,c as Sa,$ as Aa,t as L,e as t,g as i,l as $,i as f,E as za,n as d,r,s as Ge,m as V,j as P,k as M,d as S}from"../chunks/Dt_rlGj7.js";import{i as _}from"../chunks/BpMPWboC.js";import{r as v,s as T,a as Ue,e as We,i as Ye}from"../chunks/C6mVS30y.js";import{b as c,c as Ke}from"../chunks/CZFHF5H2.js";import{b as Xe}from"../chunks/NmsYRWce.js";import{g as $a}from"../chunks/CqZYzXRB.js";import{B as ne,C as ja,a as Oa}from"../chunks/DvgRy-IG.js";function Ra(j,y){y.name&&!y.output_dir&&(y.output_dir=`./models/${y.name.toLowerCase().replace(/[^a-z0-9]/g,"-")}`)}var Ba=m('<div class="p-4 bg-red-50 border border-red-200 rounded-lg"><p class="text-red-700"> </p></div>'),Ca=(j,y)=>y.model_type="text",Fa=m('<div class="w-full h-full rounded-full bg-white scale-50"></div>'),Va=(j,y)=>y.model_type="vision",Pa=m('<div class="w-full h-full rounded-full bg-white scale-50"></div>'),Ta=m("<option> </option>"),qa=m("<option> </option>"),Ha=m('<p class="text-xs text-gray-500 mt-1">üé® Vision-language models can analyze images and text together</p>'),Qa=m('<p class="text-sm text-blue-800 mb-2">HuggingFace datasets should use OpenAI messages format with base64 images:</p> <pre class="text-xs bg-blue-100 p-2 rounded overflow-x-auto"><code></code></pre> <p class="text-xs text-blue-700 mt-2"><strong>Example:</strong> <code>Barth371/train_pop_valet_no_wrong_doc</code></p>',1),Ea=m('<p class="text-sm text-blue-800 mb-2">Your dataset should be in JSONL format with:</p> <pre class="text-xs bg-blue-100 p-2 rounded overflow-x-auto"><code></code></pre> <p class="text-xs text-blue-700 mt-2"><strong>Tip:</strong> Use <code>model-garden create-vision-dataset</code> CLI to generate sample data</p>',1),Ia=m('<div class="p-4 bg-blue-50 border border-blue-200 rounded-lg"><h4 class="text-sm font-semibold text-blue-900 mb-2">üìã Vision Dataset Format</h4> <!></div>'),Ja=m('<div class="mb-4 p-3 bg-yellow-50 border border-yellow-200 rounded-lg"><p class="text-sm text-yellow-800">‚ö†Ô∏è <strong>Vision models require:</strong> Lower batch size (1-2), higher gradient accumulation (8+), and lower learning rate (2e-5)</p></div>'),Na=m("<strong>‚úÖ Merged 16-bit (Recommended):</strong> Full model with LoRA weights merged using Unsloth. Creates split files for vLLM compatibility.",1),Da=m("<strong>üì¶ Merged 4-bit:</strong> Full model with LoRA weights merged in 4-bit quantized format. Smaller file size.",1),Ga=m("<strong>üîß LoRA Adapters Only (Advanced):</strong> Saves only the adapter weights. Requires the base model to load.",1),Ua=m('<form class="space-y-6"><!> <div><h3 class="text-lg font-semibold text-gray-900 mb-4">Basic Configuration</h3> <div class="grid grid-cols-1 gap-4"><div><div class="block text-sm font-medium text-gray-700 mb-2">Model Type *</div> <div class="grid grid-cols-2 gap-3"><button type="button"><div class="flex items-center gap-2"><div><!></div> <span class="font-medium">Text-Only (LLM)</span></div> <p class="text-sm text-gray-600 mt-2 ml-6">Fine-tune language models for text generation tasks</p></button> <button type="button"><div class="flex items-center gap-2"><div><!></div> <span class="font-medium">Vision-Language (VLM)</span></div> <p class="text-sm text-gray-600 mt-2 ml-6">Fine-tune multimodal models for image + text tasks</p></button></div></div> <div><label for="name" class="block text-sm font-medium text-gray-700 mb-1">Model Name *</label> <input type="text" id="name" placeholder="my-finance-model" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500" required/></div> <div><label for="base_model" class="block text-sm font-medium text-gray-700 mb-1">Base Model *</label> <select id="base_model" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500" required><!></select> <!></div> <div><label for="dataset_path" class="block text-sm font-medium text-gray-700 mb-1">Dataset Path *</label> <input type="text" id="dataset_path" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500" required/> <div class="mt-2 flex items-center"><input type="checkbox" id="from_hub" class="h-4 w-4 text-primary-600 focus:ring-primary-500 border-gray-300 rounded"/> <label for="from_hub" class="ml-2 block text-sm text-gray-700">Load from HuggingFace Hub</label></div> <p class="text-xs text-gray-500 mt-1"><!></p></div> <div><label for="validation_dataset_path" class="block text-sm font-medium text-gray-700 mb-1">Validation Dataset Path (Optional)</label> <input type="text" id="validation_dataset_path" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <div class="mt-2 flex items-center"><input type="checkbox" id="validation_from_hub" class="h-4 w-4 text-primary-600 focus:ring-primary-500 border-gray-300 rounded"/> <label for="validation_from_hub" class="ml-2 block text-sm text-gray-700">Load validation dataset from HuggingFace Hub</label></div> <p class="text-xs text-gray-500 mt-1">üìä Optional: Provide a validation dataset to track validation loss during training</p></div> <!> <div><label for="output_dir" class="block text-sm font-medium text-gray-700 mb-1">Output Directory</label> <input type="text" id="output_dir" placeholder="./models/my-model" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/></div></div></div> <div><h3 class="text-lg font-semibold text-gray-900 mb-4">Training Hyperparameters</h3> <!> <div class="grid grid-cols-2 gap-4"><div><label for="learning_rate" class="block text-sm font-medium text-gray-700 mb-1">Learning Rate</label> <input type="number" id="learning_rate" step="0.00001" min="0" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/></div> <div><label for="num_epochs" class="block text-sm font-medium text-gray-700 mb-1">Epochs</label> <input type="number" id="num_epochs" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/></div> <div><label for="batch_size" class="block text-sm font-medium text-gray-700 mb-1">Batch Size</label> <input type="number" id="batch_size" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/></div> <div><label for="gradient_accumulation" class="block text-sm font-medium text-gray-700 mb-1">Gradient Accumulation</label> <input type="number" id="gradient_accumulation" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Higher for vision models (8+)</p></div> <div><label for="max_steps" class="block text-sm font-medium text-gray-700 mb-1">Max Steps</label> <input type="number" id="max_steps" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">-1 for full epochs</p></div> <div><label for="eval_steps" class="block text-sm font-medium text-gray-700 mb-1">Evaluation Steps</label> <input type="number" id="eval_steps" placeholder="Auto (same as save_steps)" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Evaluate every N steps (only used if validation dataset provided)</p></div></div></div> <div><h3 class="text-lg font-semibold text-gray-900 mb-4">LoRA Configuration</h3> <div class="grid grid-cols-2 gap-4"><div><label for="lora_r" class="block text-sm font-medium text-gray-700 mb-1">LoRA Rank (r)</label> <input type="number" id="lora_r" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/></div> <div><label for="lora_alpha" class="block text-sm font-medium text-gray-700 mb-1">LoRA Alpha</label> <input type="number" id="lora_alpha" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/></div></div></div> <div><h3 class="text-lg font-semibold text-gray-900 mb-4">Model Save Options</h3> <div><label for="save_method" class="block text-sm font-medium text-gray-700 mb-2">Save Method</label> <select id="save_method" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"><option>Save Merged Model (16-bit) - Recommended</option><option>Save Merged Model (4-bit) - Smaller Size</option><option>Save LoRA Adapters Only - Advanced</option></select> <div class="mt-3 p-3 bg-blue-50 border border-blue-200 rounded-lg"><p class="text-sm text-blue-800"><!></p></div></div></div> <div class="flex gap-4 pt-4"><!> <!></div></form>'),Wa=m('<div class="min-h-screen bg-gray-50"><div class="bg-white shadow"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex justify-between items-center py-6"><div class="flex items-center"><!> <h1 class="text-3xl font-bold text-gray-900 ml-4">New Training Job</h1></div></div></div></div> <div class="max-w-3xl mx-auto px-4 sm:px-6 lg:px-8 py-8"><!></div></div>');function it(j,y){wa(y,!0);let a=ka({name:"",model_type:"text",base_model:"unsloth/tinyllama-bnb-4bit",dataset_path:"./data/sample.jsonl",validation_dataset_path:"",output_dir:"",hyperparameters:{learning_rate:2e-4,num_epochs:3,batch_size:2,max_steps:-1,gradient_accumulation_steps:4,eval_steps:null},lora_config:{r:16,lora_alpha:16},from_hub:!1,validation_from_hub:!1,save_method:"merged_16bit"}),A=Ge(!1),h=Ge("");const pe=["unsloth/tinyllama-bnb-4bit","unsloth/phi-2-bnb-4bit","unsloth/mistral-7b-bnb-4bit","unsloth/llama-2-7b-bnb-4bit","unsloth/llama-3-8b-bnb-4bit"],me=["Qwen/Qwen2.5-VL-3B-Instruct","Qwen/Qwen2.5-VL-7B-Instruct","Qwen/Qwen2.5-VL-72B-Instruct","unsloth/Qwen2.5-VL-3B-Instruct-bnb-4bit","unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit"];La(()=>{a.model_type==="vision"?(a.base_model=me[0],a.dataset_path="./data/vision_dataset.jsonl",a.hyperparameters.batch_size=1,a.hyperparameters.gradient_accumulation_steps=8,a.hyperparameters.learning_rate=2e-5):(a.base_model=pe[0],a.dataset_path="./data/sample.jsonl",a.hyperparameters.batch_size=2,a.hyperparameters.gradient_accumulation_steps=4,a.hyperparameters.learning_rate=2e-4)});async function Ze(w){if(w.preventDefault(),!a.name||!a.base_model||!a.dataset_path){S(h,"Please fill in all required fields");return}S(A,!0),S(h,"");try{const x=await Oa.createTrainingJob({...a,is_vision:a.model_type==="vision"});x.success?$a(`/training/${x.data.job_id}`):S(h,"Failed to create training job")}catch(x){S(h,x instanceof Error?x.message:"Failed to create training job",!0)}finally{S(A,!1)}}var q=Wa();Ma(w=>{Aa.title="New Training Job - Model Garden"});var H=t(q),ve=t(H),ue=t(ve),_e=t(ue),ea=t(_e);ne(ea,{href:"/training",variant:"ghost",size:"sm",children:(w,x)=>{d();var k=L("‚Üê Training Jobs");l(w,k)},$$slots:{default:!0}}),d(2),r(_e),r(ue),r(ve),r(H);var ce=i(H,2),aa=t(ce);ja(aa,{children:(w,x)=>{var k=Ua(),be=t(k);{var ta=e=>{var o=Ba(),n=t(o),b=t(n,!0);r(n),r(o),$(()=>V(b,f(h))),l(e,o)};_(be,e=>{f(h)&&e(ta)})}var Q=i(be,2),ge=i(t(Q),2),E=t(ge),fe=i(t(E),2),z=t(fe);z.__click=[Ca,a];var ye=t(z),I=t(ye),ra=t(I);{var ia=e=>{var o=Fa();l(e,o)};_(ra,e=>{a.model_type==="text"&&e(ia)})}r(I),d(2),r(ye),d(2),r(z);var O=i(z,2);O.__click=[Va,a];var xe=t(O),J=t(xe),oa=t(J);{var sa=e=>{var o=Pa();l(e,o)};_(oa,e=>{a.model_type==="vision"&&e(sa)})}r(J),d(2),r(xe),d(2),r(O),r(fe),r(E);var N=i(E,2),D=i(t(N),2);v(D),D.__input=[Ra,a],r(N);var G=i(N,2),R=i(t(G),2),la=t(R);{var da=e=>{var o=P(),n=M(o);We(n,17,()=>pe,Ye,(b,u)=>{var s=Ta(),p=t(s,!0);r(s);var g={};$(()=>{V(p,f(u)),g!==(g=f(u))&&(s.value=(s.__value=f(u))??"")}),l(b,s)}),l(e,o)},na=e=>{var o=P(),n=M(o);We(n,17,()=>me,Ye,(b,u)=>{var s=qa(),p=t(s,!0);r(s);var g={};$(()=>{V(p,f(u)),g!==(g=f(u))&&(s.value=(s.__value=f(u))??"")}),l(b,s)}),l(e,o)};_(la,e=>{a.model_type==="text"?e(da):e(na,!1)})}r(R);var pa=i(R,2);{var ma=e=>{var o=Ha();l(e,o)};_(pa,e=>{a.model_type==="vision"&&e(ma)})}r(G);var U=i(G,2),B=i(t(U),2);v(B);var W=i(B,2),he=t(W);v(he),d(2),r(W);var we=i(W,2),va=t(we);{var ua=e=>{var o=L('Enter a HuggingFace dataset identifier (e.g., "username/dataset-name")');l(e,o)},_a=e=>{var o=P(),n=M(o);{var b=s=>{var p=L("Path to your JSONL dataset with image paths/base64 or local file");l(s,p)},u=s=>{var p=L("Path to your JSONL dataset file");l(s,p)};_(n,s=>{a.model_type==="vision"?s(b):s(u,!1)},!0)}l(e,o)};_(va,e=>{a.from_hub?e(ua):e(_a,!1)})}r(we),r(U);var Y=i(U,2),C=i(t(Y),2);v(C);var ke=i(C,2),Le=t(ke);v(Le),d(2),r(ke),d(2),r(Y);var Me=i(Y,2);{var ca=e=>{var o=Ia(),n=i(t(o),2);{var b=s=>{var p=Qa(),g=i(M(p),2),de=t(g);de.textContent='{"messages": [{"role": "user", "content": [{"type": "image", "image": "data:image/jpeg;base64,..."}, {"type": "text", "text": "What is shown?"}]}]}',r(g),d(2),l(s,p)},u=s=>{var p=Ea(),g=i(M(p),2),de=t(g);de.textContent='{"text": "What is in this image?", "image": "/path/to/image.jpg", "response": "A cat sitting on a table"}',r(g),d(2),l(s,p)};_(n,s=>{a.from_hub?s(b):s(u,!1)})}r(o),l(e,o)};_(Me,e=>{a.model_type==="vision"&&e(ca)})}var Se=i(Me,2),Ae=i(t(Se),2);v(Ae),r(Se),r(ge),r(Q);var K=i(Q,2),ze=i(t(K),2);{var ba=e=>{var o=Ja();l(e,o)};_(ze,e=>{a.model_type==="vision"&&e(ba)})}var $e=i(ze,2),X=t($e),je=i(t(X),2);v(je),r(X);var Z=i(X,2),Oe=i(t(Z),2);v(Oe),r(Z);var ee=i(Z,2),Re=i(t(ee),2);v(Re),r(ee);var ae=i(ee,2),Be=i(t(ae),2);v(Be),d(2),r(ae);var te=i(ae,2),Ce=i(t(te),2);v(Ce),d(2),r(te);var Fe=i(te,2),Ve=i(t(Fe),2);v(Ve),d(2),r(Fe),r($e),r(K);var re=i(K,2),Pe=i(t(re),2),ie=t(Pe),Te=i(t(ie),2);v(Te),r(ie);var qe=i(ie,2),He=i(t(qe),2);v(He),r(qe),r(Pe),r(re);var oe=i(re,2),Qe=i(t(oe),2),F=i(t(Qe),2),se=t(F);se.value=se.__value="merged_16bit";var le=i(se);le.value=le.__value="merged_4bit";var Ee=i(le);Ee.value=Ee.__value="lora",r(F);var Ie=i(F,2),Je=t(Ie),ga=t(Je);{var fa=e=>{var o=Na();d(),l(e,o)},ya=e=>{var o=P(),n=M(o);{var b=s=>{var p=Da();d(),l(s,p)},u=s=>{var p=Ga();d(),l(s,p)};_(n,s=>{a.save_method==="merged_4bit"?s(b):s(u,!1)},!0)}l(e,o)};_(ga,e=>{a.save_method==="merged_16bit"?e(fa):e(ya,!1)})}r(Je),r(Ie),r(Qe),r(oe);var Ne=i(oe,2),De=t(Ne);ne(De,{type:"submit",variant:"primary",get loading(){return f(A)},get disabled(){return f(A)},children:(e,o)=>{d();var n=L();$(()=>V(n,f(A)?"Creating...":"Start Training")),l(e,n)},$$slots:{default:!0}});var xa=i(De,2);ne(xa,{href:"/training",variant:"secondary",children:(e,o)=>{d();var n=L("Cancel");l(e,n)},$$slots:{default:!0}}),r(Ne),r(k),$(()=>{T(z,1,`p-4 border-2 rounded-lg text-left transition-all ${a.model_type==="text"?"border-primary-500 bg-primary-50":"border-gray-300 hover:border-gray-400"}`),T(I,1,`w-4 h-4 rounded-full border-2 ${a.model_type==="text"?"border-primary-500 bg-primary-500":"border-gray-400"}`),T(O,1,`p-4 border-2 rounded-lg text-left transition-all ${a.model_type==="vision"?"border-primary-500 bg-primary-50":"border-gray-300 hover:border-gray-400"}`),T(J,1,`w-4 h-4 rounded-full border-2 ${a.model_type==="vision"?"border-primary-500 bg-primary-500":"border-gray-400"}`),Ue(B,"placeholder",a.from_hub?"username/dataset-name":a.model_type==="vision"?"./data/vision_dataset.jsonl":"./data/my-dataset.jsonl"),Ue(C,"placeholder",a.validation_from_hub?"username/val-dataset-name":a.model_type==="vision"?"./data/vision_val_dataset.jsonl":"./data/my-val-dataset.jsonl")}),za("submit",k,Ze),c(D,()=>a.name,e=>a.name=e),Xe(R,()=>a.base_model,e=>a.base_model=e),c(B,()=>a.dataset_path,e=>a.dataset_path=e),Ke(he,()=>a.from_hub,e=>a.from_hub=e),c(C,()=>a.validation_dataset_path,e=>a.validation_dataset_path=e),Ke(Le,()=>a.validation_from_hub,e=>a.validation_from_hub=e),c(Ae,()=>a.output_dir,e=>a.output_dir=e),c(je,()=>a.hyperparameters.learning_rate,e=>a.hyperparameters.learning_rate=e),c(Oe,()=>a.hyperparameters.num_epochs,e=>a.hyperparameters.num_epochs=e),c(Re,()=>a.hyperparameters.batch_size,e=>a.hyperparameters.batch_size=e),c(Be,()=>a.hyperparameters.gradient_accumulation_steps,e=>a.hyperparameters.gradient_accumulation_steps=e),c(Ce,()=>a.hyperparameters.max_steps,e=>a.hyperparameters.max_steps=e),c(Ve,()=>a.hyperparameters.eval_steps,e=>a.hyperparameters.eval_steps=e),c(Te,()=>a.lora_config.r,e=>a.lora_config.r=e),c(He,()=>a.lora_config.lora_alpha,e=>a.lora_config.lora_alpha=e),Xe(F,()=>a.save_method,e=>a.save_method=e),l(w,k)},$$slots:{default:!0}}),r(ce),r(q),l(j,q),Sa()}ha(["click","input"]);export{it as component};
