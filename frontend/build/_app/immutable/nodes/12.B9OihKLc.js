import"../chunks/DsnmJJEf.js";import{v as Ur,p as Hr,a as Tt,s as re,o as Ir,F as Ft,i as n,j as q,f as p,h as Gr,c as l,d as Jr,$ as Wr,t as se,e as a,g as t,m as D,x as $r,n as o,r,q as P,l as U,k as _e}from"../chunks/DLqH200v.js";import{i as c}from"../chunks/uQGPdJUk.js";import{r as d,s as fe,a as Qt,b as Kr,e as Ga,i as Ja}from"../chunks/BCgPi0gr.js";import{b as x,c as H}from"../chunks/DIKPUBCA.js";import{b as oe}from"../chunks/CmB2ust7.js";import{g as Yr}from"../chunks/IDmvfdnG.js";import{a as Ot}from"../chunks/Btudmqni.js";import{B as Vt,C as Xr}from"../chunks/3RODIbcq.js";var Zr=p('<div class="p-4 bg-red-50 border border-red-200 rounded-lg"><p class="text-red-700"> </p></div>'),es=(ie,O)=>O.model_type="text",ts=p('<div class="w-full h-full rounded-full bg-white scale-50"></div>'),as=(ie,O)=>O.model_type="vision",rs=p('<div class="w-full h-full rounded-full bg-white scale-50"></div>'),ss=(ie,O)=>q(O,!1),os=(ie,O)=>q(O,!0),is=p("<option>Loading models...</option>"),ls=p("<option> </option>"),ns=p("<option> </option>"),ds=p('<p class="text-xs text-yellow-600 mt-1"> </p>'),ps=p('<select id="base_model" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500" required><!></select> <!>',1),ms=p('<li>‚Ä¢ Format: <code class="bg-blue-100 px-1 rounded">organization/model-name</code></li> <li>‚Ä¢ Examples: <code class="bg-blue-100 px-1 rounded">Qwen/Qwen2-VL-7B-Instruct</code>, <code class="bg-blue-100 px-1 rounded">llava-hf/llava-1.5-7b-hf</code></li> <li>‚Ä¢ ‚ö†Ô∏è Model must support vision-language tasks</li>',1),vs=p('<li>‚Ä¢ Format: <code class="bg-blue-100 px-1 rounded">organization/model-name</code></li> <li>‚Ä¢ Examples: <code class="bg-blue-100 px-1 rounded">meta-llama/Llama-2-7b-hf</code>, <code class="bg-blue-100 px-1 rounded">mistralai/Mistral-7B-v0.1</code></li> <li>‚Ä¢ ‚ö†Ô∏è Private models require HF_TOKEN environment variable</li>',1),_s=p('<input type="text" id="base_model_custom" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500 font-mono text-sm" required/> <div class="mt-2 p-3 bg-blue-50 border border-blue-200 rounded-lg"><p class="text-xs text-blue-800 mb-2"><strong>üí° Enter any HuggingFace model ID:</strong></p> <ul class="text-xs text-blue-700 space-y-1"><!></ul></div>',1),cs=p('<p class="text-xs text-gray-500 mt-1">üé® Vision-language models can analyze images and text together</p>'),us=p('<p class="text-xs text-blue-700"><strong>Context:</strong> </p>'),gs=p('<div class="space-y-1"><p class="text-xs text-blue-700"><strong>VRAM:</strong> </p> <!></div>'),bs=p('<p class="text-xs text-blue-700 mt-2"><strong>Best for:</strong> </p>'),ys=p('<div class="mt-3 p-3 bg-blue-50 border border-blue-200 rounded-lg"><div class="flex items-start justify-between"><div class="flex-1"><h4 class="text-sm font-semibold text-blue-900 mb-1"> </h4> <p class="text-xs text-blue-800 mb-2"> </p> <!> <!></div></div></div>'),fs=p(`<div class="mt-3 p-3 bg-yellow-50 border border-yellow-200 rounded-lg"><p class="text-xs text-yellow-800"><strong>‚ö†Ô∏è Custom Model:</strong> Default hyperparameters may not
                    be optimal for this model. You may need to adjust learning rate,
                    batch size, and LoRA settings based on the model architecture.</p></div>`),xs=p(`Enter a HuggingFace dataset identifier (e.g.,
                  "username/dataset-name")<br/> For specific files, use: "username/repo::train.jsonl"`,1),hs=p(`<p class="text-sm text-blue-800 mb-2">HuggingFace datasets should use OpenAI messages format with
                    base64 images:</p> <pre class="text-xs bg-blue-100 p-2 rounded overflow-x-auto"><code></code></pre> <p class="text-xs text-blue-700 mt-2"><strong>Example:</strong> <code>Barth371/train_pop_valet_no_wrong_doc</code></p>`,1),ks=p(`<p class="text-sm text-blue-800 mb-2">Your dataset should be in JSONL format with:</p> <pre class="text-xs bg-blue-100 p-2 rounded overflow-x-auto"><code></code></pre> <p class="text-xs text-blue-700 mt-2"><strong>Tip:</strong> Use <code>model-garden create-vision-dataset</code> CLI to generate
                    sample data</p>`,1),ws=p('<div class="p-4 bg-blue-50 border border-blue-200 rounded-lg"><h4 class="text-sm font-semibold text-blue-900 mb-2">üìã Vision Dataset Format</h4> <!></div>'),Ls=p('<span class="text-xs text-green-600 bg-green-50 px-2 py-1 rounded">‚úì Using registry defaults</span>'),Ss=p(`<div class="mb-4 p-3 bg-yellow-50 border border-yellow-200 rounded-lg"><p class="text-sm text-yellow-800">‚ö†Ô∏è <strong>Vision models require:</strong> Lower batch size (1-2),
                higher gradient accumulation (8+), and lower learning rate (2e-5)</p></div>`),Ms=p(`<div class="ml-6 space-y-4 p-4 bg-white border border-gray-200 rounded-lg"><div><label for="early_stopping_patience" class="block text-sm font-medium text-gray-700 mb-1">Patience (evaluations)</label> <input type="number" id="early_stopping_patience" min="1" max="20" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Number of evaluations with no improvement before
                        stopping (3-5 typical)</p></div> <div><label for="early_stopping_threshold" class="block text-sm font-medium text-gray-700 mb-1">Improvement Threshold</label> <input type="number" id="early_stopping_threshold" min="0" step="0.0001" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Minimum change to qualify as improvement (0.0001 =
                        0.01%, smaller = more sensitive)</p></div> <div class="p-3 bg-green-50 border border-green-200 rounded-lg"><p class="text-xs text-green-800"><strong>üí° Example:</strong> With patience=3 and threshold=0.0001,
                        training stops if validation loss doesn't improve by at least
                        0.01% for 3 consecutive evaluations.</p></div></div>`),As=p(`<div class="mb-6"><h4 class="text-md font-medium text-gray-800 mb-3 flex items-center gap-2">üìä Evaluation Settings</h4> <div class="grid grid-cols-2 gap-4"><div><label for="eval_strategy" class="block text-sm font-medium text-gray-700 mb-1">Evaluation Strategy</label> <select id="eval_strategy" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"><option>Every N steps</option><option>Every epoch</option><option>No evaluation</option></select></div> <div><label for="eval_steps" class="block text-sm font-medium text-gray-700 mb-1">Evaluation Steps</label> <input type="number" id="eval_steps" placeholder="Auto (same as save_steps)" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Evaluate every N steps (leave empty for auto)</p></div> <div><label for="metric_for_best_model" class="block text-sm font-medium text-gray-700 mb-1">Best Model Metric</label> <select id="metric_for_best_model" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"><option>Validation Loss (lower is better)</option><option>Accuracy (higher is better)</option><option>F1 Score (higher is better)</option></select></div> <div><div class="flex items-center mt-6"><input type="checkbox" id="load_best_model_at_end" class="h-4 w-4 text-primary-600 focus:ring-primary-500 border-gray-300 rounded"/> <label for="load_best_model_at_end" class="ml-2 block text-sm text-gray-700">Load best model at end</label></div> <p class="text-xs text-gray-500 mt-1">Automatically load checkpoint with best validation metric</p></div></div></div> <div class="mb-6"><h4 class="text-md font-medium text-gray-800 mb-3 flex items-center gap-2">‚è∏Ô∏è Automatic Early Stopping</h4> <div class="p-4 bg-blue-50 border border-blue-200 rounded-lg mb-4"><p class="text-sm text-blue-800 mb-2"><strong>Automatic Early Stopping:</strong> Stops training when
                  validation loss stops improving, preventing overfitting and saving
                  compute time.</p> <p class="text-xs text-blue-700">This is different from the manual "Stop Early" button on the
                  training page. This monitors validation metrics and stops
                  automatically.</p></div> <div class="space-y-4"><div><div class="flex items-center"><input type="checkbox" id="early_stopping_enabled" class="h-4 w-4 text-primary-600 focus:ring-primary-500 border-gray-300 rounded"/> <label for="early_stopping_enabled" class="ml-2 block text-sm font-medium text-gray-700">Enable Automatic Early Stopping</label></div> <p class="text-xs text-gray-500 mt-1 ml-6">Monitor validation loss and stop when it stops improving</p></div> <!></div></div>`,1),Rs=(ie,O)=>q(O,!n(O)),Bs=p('<div class="mb-6 p-4 bg-gray-50 border border-gray-200 rounded-lg"><h4 class="text-md font-medium text-gray-800 mb-3 flex items-center gap-2">‚öôÔ∏è Optimizer Settings</h4> <div class="grid grid-cols-2 gap-4 mb-6"><div><label for="weight_decay" class="block text-sm font-medium text-gray-700 mb-1">Weight Decay</label> <input type="number" id="weight_decay" step="0.001" min="0" max="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">L2 regularization strength (0.01 typical)</p></div> <div><label for="lr_scheduler_type" class="block text-sm font-medium text-gray-700 mb-1">LR Scheduler</label> <select id="lr_scheduler_type" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"><option>Linear (default)</option><option>Cosine (good for vision)</option><option>Constant</option><option>Constant with Warmup</option><option>Polynomial</option></select> <p class="text-xs text-gray-500 mt-1">Learning rate schedule type</p></div> <div><label for="warmup_steps" class="block text-sm font-medium text-gray-700 mb-1">Warmup Steps</label> <input type="number" id="warmup_steps" min="0" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Steps to warmup learning rate from 0</p></div> <div><label for="max_grad_norm" class="block text-sm font-medium text-gray-700 mb-1">Max Gradient Norm</label> <input type="number" id="max_grad_norm" step="0.1" min="0" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Gradient clipping threshold (1.0 standard)</p></div></div> <h4 class="text-md font-medium text-gray-800 mb-3 flex items-center gap-2">üéõÔ∏è Adam Optimizer Parameters</h4> <div class="grid grid-cols-3 gap-4 mb-6"><div><label for="adam_beta1" class="block text-sm font-medium text-gray-700 mb-1">Adam Beta1</label> <input type="number" id="adam_beta1" step="0.01" min="0" max="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Exponential decay rate for 1st moment (0.9 default)</p></div> <div><label for="adam_beta2" class="block text-sm font-medium text-gray-700 mb-1">Adam Beta2</label> <input type="number" id="adam_beta2" step="0.001" min="0" max="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Exponential decay rate for 2nd moment (0.999 default)</p></div> <div><label for="adam_epsilon" class="block text-sm font-medium text-gray-700 mb-1">Adam Epsilon</label> <input type="number" id="adam_epsilon" step="1e-9" min="0" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Small constant for numerical stability (1e-8 default)</p></div></div> <h4 class="text-md font-medium text-gray-800 mb-3 flex items-center gap-2">üîÑ Data Loading Settings</h4> <div class="grid grid-cols-2 gap-4"><div><label for="dataloader_num_workers" class="block text-sm font-medium text-gray-700 mb-1">Dataloader Workers</label> <input type="number" id="dataloader_num_workers" min="0" max="16" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Number of worker processes (0 = main process only)</p></div> <div><div class="flex items-center mt-6"><input type="checkbox" id="dataloader_pin_memory" class="h-4 w-4 text-primary-600 focus:ring-primary-500 border-gray-300 rounded"/> <label for="dataloader_pin_memory" class="ml-2 block text-sm text-gray-700">Pin memory to GPU</label></div> <p class="text-xs text-gray-500 mt-1">Faster data transfer to GPU (recommended)</p></div></div></div>'),Cs=p('<div class="text-xs text-gray-600 bg-blue-50 border border-blue-100 rounded px-3 py-2">‚ÑπÔ∏è Using default 4-bit quantization (most memory efficient)</div>'),qs=p('<span class="text-xs text-green-600 bg-green-50 px-2 py-1 rounded">‚úì Using registry defaults</span>'),Es=(ie,O)=>q(O,!n(O)),zs=(ie,O)=>{const e=ie.currentTarget.value.trim();e?O.lora_config.target_modules=e.split(",").map(le=>le.trim()).filter(le=>le.length>0):O.lora_config.target_modules=null},js=p(`<div class="mb-4 p-4 bg-purple-50 border border-purple-200 rounded-lg"><h4 class="text-md font-medium text-purple-900 mb-3 flex items-center gap-2">üé® Selective Layer Fine-tuning (Vision Models)</h4> <p class="text-sm text-purple-800 mb-4">Control which parts of the vision-language model to train.
                    Disable layers you don't want to modify:</p> <div class="space-y-3"><div class="flex items-start gap-3"><input type="checkbox" id="finetune_vision_layers" class="h-4 w-4 mt-0.5 text-purple-600 focus:ring-purple-500 border-gray-300 rounded"/> <div class="flex-1"><label for="finetune_vision_layers" class="text-sm font-medium text-gray-700">Fine-tune Vision Encoder Layers</label> <p class="text-xs text-gray-500 mt-0.5">Train the image processing layers. Disable to freeze
                          vision encoder and only adapt language model.</p></div></div> <div class="flex items-start gap-3"><input type="checkbox" id="finetune_language_layers" class="h-4 w-4 mt-0.5 text-purple-600 focus:ring-purple-500 border-gray-300 rounded"/> <div class="flex-1"><label for="finetune_language_layers" class="text-sm font-medium text-gray-700">Fine-tune Language Model Layers</label> <p class="text-xs text-gray-500 mt-0.5">Train the text generation layers. Disable to freeze
                          language model and only adapt vision encoder.</p></div></div> <div class="flex items-start gap-3"><input type="checkbox" id="finetune_attention_modules" class="h-4 w-4 mt-0.5 text-purple-600 focus:ring-purple-500 border-gray-300 rounded"/> <div class="flex-1"><label for="finetune_attention_modules" class="text-sm font-medium text-gray-700">Fine-tune Attention Modules</label> <p class="text-xs text-gray-500 mt-0.5">Train attention layers (Q, K, V, O projections).
                          Disable for faster training with slightly lower
                          quality.</p></div></div> <div class="flex items-start gap-3"><input type="checkbox" id="finetune_mlp_modules" class="h-4 w-4 mt-0.5 text-purple-600 focus:ring-purple-500 border-gray-300 rounded"/> <div class="flex-1"><label for="finetune_mlp_modules" class="text-sm font-medium text-gray-700">Fine-tune MLP Modules</label> <p class="text-xs text-gray-500 mt-0.5">Train feed-forward layers (gate, up, down
                          projections). Disable for faster training with
                          slightly lower quality.</p></div></div></div> <div class="mt-4 p-3 bg-purple-100 border border-purple-300 rounded-lg"><p class="text-xs text-purple-900 font-medium mb-2">üí° Common Configurations:</p> <ul class="text-xs text-purple-800 space-y-1"><li><strong>All enabled (default):</strong> Full model fine-tuning
                        - best quality, slowest</li> <li><strong>Language only:</strong> Disable vision layers - adapt
                        text generation while keeping vision frozen</li> <li><strong>Vision only:</strong> Disable language layers - adapt
                        image understanding while keeping language frozen</li> <li><strong>Attention only:</strong> Disable MLPs - focus on
                        cross-modal attention mechanisms</li></ul></div></div>`),Ts=p(`<div class="p-4 bg-gray-50 border border-gray-200 rounded-lg"><div class="grid grid-cols-2 gap-4 mb-4"><div><label for="lora_bias" class="block text-sm font-medium text-gray-700 mb-1">LoRA Bias</label> <select id="lora_bias" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"><option>None (default)</option><option>All bias terms</option><option>LoRA layers only</option></select> <p class="text-xs text-gray-500 mt-1">How to handle bias parameters in LoRA layers</p></div> <div><label for="use_gradient_checkpointing" class="block text-sm font-medium text-gray-700 mb-1">Gradient Checkpointing</label> <select id="use_gradient_checkpointing" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"><option>Unsloth (30% less VRAM, minor quality loss)</option><option>Standard (better quality)</option><option>Disabled (best quality, most VRAM)</option></select> <p class="text-xs text-gray-500 mt-1">Tradeoff between memory usage and training quality</p></div></div> <div class="grid grid-cols-2 gap-4 mb-4"><div><div class="flex items-center mt-2"><input type="checkbox" id="use_rslora" class="h-4 w-4 text-primary-600 focus:ring-primary-500 border-gray-300 rounded"/> <label for="use_rslora" class="ml-2 block text-sm text-gray-700">Use RSLoRA (Rank-Stabilized LoRA)</label></div> <p class="text-xs text-gray-500 mt-1">Better stability for high ranks (r > 16)</p></div> <div><label for="random_state" class="block text-sm font-medium text-gray-700 mb-1">Random Seed</label> <input type="number" id="random_state" min="0" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Seed for reproducible results (42 is popular)</p></div></div> <div class="grid grid-cols-2 gap-4 mb-4"><div><label for="task_type" class="block text-sm font-medium text-gray-700 mb-1">Task Type</label> <select id="task_type" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"><option>Causal LM (Text Generation)</option><option>Sequence-to-Sequence</option><option>Token Classification</option><option>Sequence Classification</option><option>Question Answering</option></select> <p class="text-xs text-gray-500 mt-1">Type of task for PEFT optimization</p></div> <div><label for="target_modules_input" class="block text-sm font-medium text-gray-700 mb-1">Target Modules (Advanced)</label> <input type="text" id="target_modules_input" placeholder="q_proj, k_proj, v_proj (leave empty for auto)" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Comma-separated list of layers to apply LoRA (auto-detected
                    if empty)</p></div></div> <!> <div class="p-3 bg-blue-50 border border-blue-200 rounded-lg"><h4 class="text-sm font-semibold text-blue-900 mb-2">üí° LoRA Tips</h4> <ul class="text-xs text-blue-800 space-y-1"><li><strong>Rank (r):</strong> Start with 16, increase to 64+ for
                    complex tasks or large datasets</li> <li><strong>Alpha:</strong> Usually equal to rank. Higher alpha =
                    stronger adaptation</li> <li><strong>Dropout:</strong> Add 0.1-0.3 if overfitting, keep 0
                    for small datasets</li> <li><strong>RSLoRA:</strong> Enable for ranks > 16 to improve training
                    stability</li> <li><strong>Target Modules:</strong> Leave empty for auto-detection.
                    Common: "q_proj,k_proj,v_proj,o_proj" for attention layers</li> <li><strong>Task Type:</strong> Use "CAUSAL_LM" for text generation,
                    "SEQ_2_SEQ_LM" for translation/summarization</li></ul></div></div>`),Fs=p(`<strong>‚úÖ Merged 16-bit (Recommended):</strong> Full model with
                  LoRA weights merged using Unsloth. Creates split files for vLLM
                  compatibility.`,1),Qs=p(`<strong>üì¶ Merged 4-bit:</strong> Full model with LoRA weights
                  merged in 4-bit quantized format. Smaller file size.`,1),Os=p(`<strong>üîß LoRA Adapters Only (Advanced):</strong> Saves only the
                  adapter weights. Requires the base model to load.`,1),Vs=p(`<strong>Conservative:</strong> Masks JSON structural
                          characters: <code></code> and
                          whitespace. Masks ~31% of tokens. <em>Recommended for most cases.</em>`,1),Ns=p("<strong>Moderate:</strong> Conservative + masks <code>null</code> keyword. Good when null values are predictable.",1),Ps=p(`<strong>Aggressive:</strong> Moderate + masks schema field
                          names. Maximum focus on semantic content. Requires specifying
                          schema keys below.`,1),Ds=p(`<div><label for="selective_loss_schema_keys" class="block text-sm font-medium text-gray-700 mb-1">Schema Keys to Mask</label> <input type="text" id="selective_loss_schema_keys" placeholder="Marque,Modele,contents,confidence_score" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Comma-separated list of JSON field names to mask (e.g.,
                        "name,address,phone")</p> <div class="mt-2 p-2 bg-yellow-50 border border-yellow-200 rounded"><p class="text-xs text-yellow-800">‚ö†Ô∏è Only mask keys that are predictable and don't carry
                          semantic meaning. The model should still learn what
                          values go with each key.</p></div></div>`),Us=p(`<br/><em>Currently: Masking starts immediately (traditional
                            approach)</em>`,1),Hs=p("<br/><em> </em>",1),Is=p("<br/><em>Currently: Masking starts immediately</em>",1),Gs=p("<br/><em> </em>",1),Js=p(`<br/><strong class="text-amber-600">Note:</strong> Both
                          epoch and step delays are set. Epoch-based will take precedence.`,1),Ws=p(`<div class="ml-6 space-y-4 p-4 bg-white border border-gray-200 rounded-lg"><div><label for="selective_loss_level" class="block text-sm font-medium text-gray-700 mb-2">Masking Level</label> <select id="selective_loss_level" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"><option>Conservative (Structure Only)</option><option>Moderate (Structure + null)</option><option>Aggressive (Structure + null + Schema Keys)</option></select> <div class="mt-2 p-3 bg-gray-50 rounded-lg"><p class="text-xs text-gray-700"><!></p></div></div> <!> <div><label for="selective_loss_masking_start_step" class="block text-sm font-medium text-gray-700 mb-2"> </label> <input type="range" id="selective_loss_masking_start_step" min="0" max="500" step="10" class="w-full h-2 bg-gray-200 rounded-lg appearance-none cursor-pointer accent-primary-500"/> <div class="flex justify-between text-xs text-gray-500 mt-1"><span>0 (Immediate)</span> <span>100</span> <span>200</span> <span>500 steps</span></div> <div class="mt-2 p-3 bg-blue-50 rounded-lg"><p class="text-xs text-blue-700"><strong>üí° Legacy Method:</strong> Step-based delay
                        depends on batch configuration. For more predictable
                        results, use epoch-based delay above. Setting this to
                        50-200 lets the model learn JSON structure first before
                        applying selective masking. This can prevent
                        degeneration issues with aggressive masking. <!></p></div></div> <div><label for="selective_loss_masking_start_epoch" class="block text-sm font-medium text-gray-700 mb-2"> </label> <input type="range" id="selective_loss_masking_start_epoch" min="0" max="2" step="0.1" class="w-full h-2 bg-gray-200 rounded-lg appearance-none cursor-pointer accent-green-500"/> <div class="flex justify-between text-xs text-gray-500 mt-1"><span>0.0 (Immediate)</span> <span>0.5</span> <span>1.0</span> <span>2.0 epochs</span></div> <div class="mt-2 p-3 bg-green-50 rounded-lg"><p class="text-xs text-green-700"><strong>üéØ Recommended:</strong> Epoch-based masking is
                        more robust than step-based as it's not affected by
                        batch size or gradient accumulation changes. <!> <!></p></div></div> <div><div class="flex items-center"><input type="checkbox" id="selective_loss_verbose" class="h-4 w-4 text-primary-600 focus:ring-primary-500 border-gray-300 rounded"/> <label for="selective_loss_verbose" class="ml-2 block text-sm text-gray-700">Verbose mode (print masking statistics)</label></div> <p class="text-xs text-gray-500 mt-1">Display detailed token masking stats during training</p></div> <div class="p-3 bg-blue-50 border border-blue-200 rounded-lg"><h4 class="text-sm font-semibold text-blue-900 mb-2">üìä What Gets Masked?</h4> <ul class="text-xs text-blue-800 space-y-1"><li>‚úì Structural: <code></code> and whitespace
                        (spaces, newlines, tabs)</li> <li>‚úì Quotes: <code>"</code> (string delimiters - purely structural)</li> <li>‚úì Null keyword: <code>null</code> (moderate/aggressive only)</li> <li>‚úó NOT masked: <code>true</code>, <code>false</code> (can
                        be semantic)</li> <li>‚úì Schema keys: Field names like <code>name</code> (aggressive
                        only)</li></ul> <p class="text-xs text-blue-700 mt-2"><strong>Example:</strong> In <code></code>, conservative
                      mode masks <code></code> and spaces (~31% of tokens),
                      trains on <code>name John age 30</code></p></div></div>`),$s=p(`<div><h3 class="text-lg font-semibold text-gray-900 mb-4">üéØ Selective Loss (Structured Outputs)</h3> <div class="p-4 bg-gradient-to-r from-purple-50 to-blue-50 border border-purple-200 rounded-lg mb-4"><p class="text-sm text-gray-800 mb-2"><strong>üî¨ Experimental Feature:</strong> Optimize training for structured
                outputs (JSON, forms, etc.)</p> <p class="text-xs text-gray-700">Masks structural tokens (braces, colons, whitespace) so the
                model focuses on semantic content. Useful for form extraction,
                structured data generation, and similar tasks.</p></div> <div class="space-y-4"><div><div class="flex items-center"><input type="checkbox" id="selective_loss" class="h-4 w-4 text-primary-600 focus:ring-primary-500 border-gray-300 rounded"/> <label for="selective_loss" class="ml-2 block text-sm font-medium text-gray-700">Enable Selective Loss Masking</label></div> <p class="text-xs text-gray-500 mt-1 ml-6">Automatically mask JSON structural tokens during training</p></div> <!></div></div>`),Ks=p(`<form class="space-y-6"><!> <div><h3 class="text-lg font-semibold text-gray-900 mb-4">Basic Configuration</h3> <div class="grid grid-cols-1 gap-4"><div><div class="block text-sm font-medium text-gray-700 mb-2">Model Type *</div> <div class="grid grid-cols-2 gap-3"><button type="button"><div class="flex items-center gap-2"><div><!></div> <span class="font-medium">Text-Only (LLM)</span></div> <p class="text-sm text-gray-600 mt-2 ml-6">Fine-tune language models for text generation tasks</p></button> <button type="button"><div class="flex items-center gap-2"><div><!></div> <span class="font-medium">Vision-Language (VLM)</span></div> <p class="text-sm text-gray-600 mt-2 ml-6">Fine-tune multimodal models for image + text tasks</p></button></div></div> <div><label for="name" class="block text-sm font-medium text-gray-700 mb-1">Model Name *</label> <input type="text" id="name" placeholder="my-finance-model" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500" required/></div> <div><label for="base_model" class="block text-sm font-medium text-gray-700 mb-1">Base Model *</label> <div class="mb-3 flex items-center gap-4"><button type="button">üìã Registry Models</button> <button type="button">üîß Custom HuggingFace Model</button></div> <!> <!> <!></div> <div><label for="dataset_path" class="block text-sm font-medium text-gray-700 mb-1">Dataset Path *</label> <input type="text" id="dataset_path" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500" required/> <div class="mt-2 flex items-center"><input type="checkbox" id="from_hub" class="h-4 w-4 text-primary-600 focus:ring-primary-500 border-gray-300 rounded"/> <label for="from_hub" class="ml-2 block text-sm text-gray-700">Load from HuggingFace Hub</label></div> <p class="text-xs text-gray-500 mt-1"><!></p></div> <div><label for="validation_dataset_path" class="block text-sm font-medium text-gray-700 mb-1">Validation Dataset Path (Optional)</label> <input type="text" id="validation_dataset_path" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <div class="mt-2 flex items-center"><input type="checkbox" id="validation_from_hub" class="h-4 w-4 text-primary-600 focus:ring-primary-500 border-gray-300 rounded"/> <label for="validation_from_hub" class="ml-2 block text-sm text-gray-700">Load validation dataset from HuggingFace Hub</label></div> <p class="text-xs text-gray-500 mt-1">üìä Optional: Provide a validation dataset to track validation
                loss during training<br/> <!></p></div> <!> <div><label for="output_dir" class="block text-sm font-medium text-gray-700 mb-1">Output Directory</label> <input type="text" id="output_dir" placeholder="my-model" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="mt-1 text-sm text-gray-500"> </p></div></div></div> <div><div class="flex items-center justify-between mb-4"><h3 class="text-lg font-semibold text-gray-900">Training Hyperparameters</h3> <!></div> <!> <div class="mb-6"><h4 class="text-md font-medium text-gray-800 mb-3 flex items-center gap-2">üéØ Essential Parameters</h4> <div class="grid grid-cols-2 gap-4"><div><label for="learning_rate" class="block text-sm font-medium text-gray-700 mb-1">Learning Rate</label> <input type="number" id="learning_rate" step="0.00001" min="0" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1"><!></p></div> <div><label for="num_epochs" class="block text-sm font-medium text-gray-700 mb-1">Number of Epochs</label> <input type="number" id="num_epochs" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Number of complete passes through dataset</p></div> <div><label for="batch_size" class="block text-sm font-medium text-gray-700 mb-1">Batch Size per GPU</label> <input type="number" id="batch_size" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1"><!></p></div> <div><label for="gradient_accumulation" class="block text-sm font-medium text-gray-700 mb-1">Gradient Accumulation Steps</label> <input type="number" id="gradient_accumulation" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Effective batch size = batch_size √ó
                  gradient_accumulation_steps</p></div> <div><label for="max_steps" class="block text-sm font-medium text-gray-700 mb-1">Max Steps (Optional)</label> <input type="number" id="max_steps" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Override epochs with exact step count (-1 for full epochs)</p></div> <div><label for="optim" class="block text-sm font-medium text-gray-700 mb-1">Optimizer</label> <select id="optim" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"><option>AdamW 8-bit (Memory Efficient)</option><option>AdamW (Better Quality)</option><option>AdamW Fused (Best Quality/Speed)</option><option>Adafactor (Most Memory Efficient)</option><option>SGD</option></select> <p class="text-xs text-gray-500 mt-1">8-bit saves memory, standard/fused offers better quality</p></div></div></div> <div class="mb-6"><h4 class="text-md font-medium text-gray-800 mb-3 flex items-center gap-2">üíæ Checkpoints & Logging</h4> <div class="grid grid-cols-3 gap-4"><div><label for="logging_steps" class="block text-sm font-medium text-gray-700 mb-1">Logging Steps</label> <input type="number" id="logging_steps" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Log metrics every N steps</p></div> <div><label for="save_steps" class="block text-sm font-medium text-gray-700 mb-1">Save Steps</label> <input type="number" id="save_steps" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Save checkpoint every N steps</p></div> <div><label for="save_total_limit" class="block text-sm font-medium text-gray-700 mb-1">Max Checkpoints</label> <input type="number" id="save_total_limit" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Keep only N most recent checkpoints</p></div></div></div> <!> <div class="mb-4"><button type="button" class="flex items-center gap-2 px-4 py-2 text-sm font-medium text-primary-700 bg-primary-50 border border-primary-200 rounded-lg hover:bg-primary-100 transition-colors"><span> </span> Advanced Hyperparameters</button></div> <!></div> <div><h3 class="text-lg font-semibold text-gray-900 mb-4 flex items-center gap-2">üéØ Quality Settings</h3> <div class="p-4 bg-blue-50 border border-blue-200 rounded-lg mb-4"><div class="flex items-start gap-3"><div class="flex-shrink-0"><svg class="w-5 h-5 text-blue-600 mt-0.5" fill="currentColor" viewBox="0 0 20 20"><path fill-rule="evenodd" d="M18 10a8 8 0 11-16 0 8 8 0 0116 0zm-7-4a1 1 0 11-2 0 1 1 0 012 0zM9 9a1 1 0 000 2v3a1 1 0 001 1h1a1 1 0 100-2v-3a1 1 0 00-1-1H9z" clip-rule="evenodd"></path></svg></div> <div class="flex-1"><p class="text-sm text-blue-800 font-medium mb-1">Quality vs Memory Tradeoff</p> <p class="text-xs text-blue-700">Default settings prioritize memory efficiency. Enable quality
                  mode or adjust individual settings for better accuracy at the
                  cost of 2-4x more VRAM.</p></div></div></div> <div class="space-y-4"><div class="p-4 bg-gradient-to-r from-purple-50 to-blue-50 border border-purple-200 rounded-lg"><div class="flex items-start justify-between"><div class="flex-1"><div class="flex items-center gap-3 mb-2"><input type="checkbox" id="quality_mode" class="h-5 w-5 text-purple-600 focus:ring-purple-500 border-gray-300 rounded"/> <label for="quality_mode" class="text-base font-semibold text-gray-900">üèÜ Quality Mode (Recommended for Production)</label></div> <p class="text-sm text-gray-700 ml-8">Automatically enables 16-bit precision, better optimizer,
                    and optimized settings for maximum accuracy.</p> <div class="mt-3 ml-8 p-3 bg-white border border-purple-100 rounded-lg"><p class="text-xs font-medium text-purple-900 mb-2">Quality mode includes:</p> <ul class="text-xs text-gray-600 space-y-1"><li>‚úì 16-bit precision (better than 4-bit)</li> <li>‚úì Standard gradient checkpointing (better than
                        "unsloth")</li> <li>‚úì AdamW optimizer (better than 8-bit version)</li> <li>‚úì RSLoRA for ranks ‚â• 32</li> <li>‚ö†Ô∏è Requires ~4x more VRAM</li></ul></div></div></div></div> <div class="p-4 bg-gray-50 border border-gray-200 rounded-lg"><h4 class="text-sm font-semibold text-gray-900 mb-3">Manual Precision Settings</h4> <p class="text-xs text-gray-600 mb-3">Override individual settings (quality mode will take precedence
                if enabled)</p> <div class="space-y-3"><div class="flex items-start gap-3"><input type="checkbox" id="load_in_16bit" class="h-4 w-4 mt-0.5 text-primary-600 focus:ring-primary-500 border-gray-300 rounded disabled:opacity-50"/> <div class="flex-1"><label for="load_in_16bit" class="text-sm font-medium text-gray-700">Load in 16-bit precision</label> <p class="text-xs text-gray-500 mt-0.5">Best quality, uses 4x more VRAM than 4-bit</p></div></div> <div class="flex items-start gap-3"><input type="checkbox" id="load_in_8bit" class="h-4 w-4 mt-0.5 text-primary-600 focus:ring-primary-500 border-gray-300 rounded disabled:opacity-50"/> <div class="flex-1"><label for="load_in_8bit" class="text-sm font-medium text-gray-700">Load in 8-bit precision</label> <p class="text-xs text-gray-500 mt-0.5">Balanced quality/memory, uses 2x more VRAM than 4-bit</p></div></div> <!></div></div></div></div> <div><div class="flex items-center justify-between mb-4"><h3 class="text-lg font-semibold text-gray-900">LoRA Configuration</h3> <!></div> <div class="grid grid-cols-3 gap-4 mb-4"><div><label for="lora_r" class="block text-sm font-medium text-gray-700 mb-1">LoRA Rank (r)</label> <input type="number" id="lora_r" min="1" max="256" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Higher = more parameters (16 typical, 64+ for complex tasks)</p></div> <div><label for="lora_alpha" class="block text-sm font-medium text-gray-700 mb-1">LoRA Alpha</label> <input type="number" id="lora_alpha" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Scaling factor (typically equal to rank)</p></div> <div><label for="lora_dropout" class="block text-sm font-medium text-gray-700 mb-1">LoRA Dropout</label> <input type="number" id="lora_dropout" min="0" max="0.5" step="0.05" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Regularization (0.0-0.3, 0 = no dropout)</p></div></div> <div class="mb-4"><button type="button" class="flex items-center gap-2 px-4 py-2 text-sm font-medium text-primary-700 bg-primary-50 border border-primary-200 rounded-lg hover:bg-primary-100 transition-colors"><span> </span> Advanced LoRA Settings</button></div> <!></div> <div><h3 class="text-lg font-semibold text-gray-900 mb-4">Model Save Options</h3> <div><label for="save_method" class="block text-sm font-medium text-gray-700 mb-2">Save Method</label> <select id="save_method" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"><option>Save Merged Model (16-bit) - Recommended</option><option>Save Merged Model (4-bit) - Smaller Size</option><option>Save LoRA Adapters Only - Advanced</option></select> <div class="mt-3 p-3 bg-blue-50 border border-blue-200 rounded-lg"><p class="text-sm text-blue-800"><!></p></div></div></div> <!> <div class="flex gap-4 pt-4"><!> <!></div></form>`),Ys=p('<div class="min-h-screen bg-gray-50"><div class="bg-white shadow"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex justify-between items-center py-6"><div class="flex items-center"><!> <h1 class="text-3xl font-bold text-gray-900 ml-4">New Training Job</h1></div></div></div></div> <div class="max-w-3xl mx-auto px-4 sm:px-6 lg:px-8 py-8"><!></div></div>');function lo(ie,O){Hr(O,!0);let e=Tt({name:"",model_type:"text",base_model:"unsloth/tinyllama-bnb-4bit",dataset_path:"./data/sample.jsonl",validation_dataset_path:"",output_dir:"",hyperparameters:{learning_rate:2e-5,num_epochs:3,batch_size:2,max_steps:-1,gradient_accumulation_steps:4,warmup_steps:10,logging_steps:10,save_steps:100,eval_steps:null,optim:"adamw_8bit",weight_decay:.01,lr_scheduler_type:"linear",max_grad_norm:1,adam_beta1:.9,adam_beta2:.999,adam_epsilon:1e-8,dataloader_num_workers:0,dataloader_pin_memory:!0,eval_strategy:"steps",load_best_model_at_end:!0,metric_for_best_model:"eval_loss",save_total_limit:3},lora_config:{r:16,lora_alpha:16,lora_dropout:0,lora_bias:"none",use_rslora:!1,use_gradient_checkpointing:"unsloth",random_state:42,target_modules:null,task_type:"CAUSAL_LM",loftq_config:null,finetune_vision_layers:!0,finetune_language_layers:!0,finetune_attention_modules:!0,finetune_mlp_modules:!0},from_hub:!1,validation_from_hub:!1,save_method:"merged_16bit",selective_loss:!1,selective_loss_level:"conservative",selective_loss_schema_keys:"",selective_loss_masking_start_step:0,selective_loss_masking_start_epoch:0,selective_loss_verbose:!1,early_stopping_enabled:!1,early_stopping_patience:3,early_stopping_threshold:1e-4,quality_mode:!1,load_in_16bit:!1,load_in_8bit:!1}),le=re(!1),ce=re(""),ae=re(!1),ne=re(Tt([])),de=re(Tt([])),T=re(null),xe=re(!0),ze=re("");Ir(async()=>{try{q(xe,!0);const[I,V]=await Promise.all([Ot.getRegistryModels("text-llm"),Ot.getRegistryModels("vision-vlm")]);q(ne,I.models,!0),q(de,V.models,!0),e.model_type==="text"&&n(ne).length>0?(q(T,n(ne)[0],!0),e.base_model=n(ne)[0].id):e.model_type==="vision"&&n(de).length>0&&(q(T,n(de)[0],!0),e.base_model=n(de)[0].id)}catch(I){q(ze,I instanceof Error?I.message:"Failed to load models from registry",!0),console.error("Failed to load registry models:",I),q(ne,[{id:"unsloth/tinyllama-bnb-4bit",name:"TinyLlama 1.1B (4-bit)",parameters:"1.1B"},{id:"unsloth/phi-2-bnb-4bit",name:"Phi-2 2.7B (4-bit)",parameters:"2.7B"},{id:"unsloth/mistral-7b-bnb-4bit",name:"Mistral 7B (4-bit)",parameters:"7B"},{id:"unsloth/llama-2-7b-bnb-4bit",name:"Llama 2 7B (4-bit)",parameters:"7B"},{id:"unsloth/llama-3-8b-bnb-4bit",name:"Llama 3 8B (4-bit)",parameters:"8B"}],!0),q(de,[{id:"Qwen/Qwen2.5-VL-3B-Instruct",name:"Qwen2.5-VL 3B",parameters:"3B"},{id:"Qwen/Qwen2.5-VL-7B-Instruct",name:"Qwen2.5-VL 7B",parameters:"7B"},{id:"Qwen/Qwen2.5-VL-72B-Instruct",name:"Qwen2.5-VL 72B",parameters:"72B"},{id:"unsloth/Qwen2.5-VL-3B-Instruct-bnb-4bit",name:"Qwen2.5-VL 3B (4-bit)",parameters:"3B"},{id:"unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit",name:"Qwen2.5-VL 7B (4-bit)",parameters:"7B"}],!0)}finally{q(xe,!1)}}),Ft(()=>{if(n(ae))q(T,null);else{const I=e.model_type==="vision"?n(de):n(ne);if(q(T,I.find(V=>V.id===e.base_model)||null,!0),n(T)?.training_defaults){const V=n(T).training_defaults;V.hyperparameters&&(e.hyperparameters={...e.hyperparameters,...V.hyperparameters}),V.lora_config&&(e.lora_config={...e.lora_config,...V.lora_config}),V.save_method&&(e.save_method=V.save_method)}}}),Ft(()=>{n(ae)?e.model_type==="vision"?e.dataset_path="./data/vision_dataset.jsonl":e.dataset_path="./data/sample.jsonl":e.model_type==="vision"?(n(de).length>0&&(e.base_model=n(de)[0].id),e.dataset_path="./data/vision_dataset.jsonl"):(n(ne).length>0&&(e.base_model=n(ne)[0].id),e.dataset_path="./data/sample.jsonl")});let je=re(!1),Te=re(!1);Ft(()=>{e.name&&(e.output_dir=e.name.toLowerCase().replace(/[^a-z0-9]/g,"-"))});async function Wa(I){if(I.preventDefault(),!e.name||!e.base_model||!e.dataset_path){q(ce,"Please fill in all required fields");return}q(le,!0),q(ce,"");try{let V=null;e.selective_loss_schema_keys&&e.selective_loss_schema_keys.trim()&&(V=e.selective_loss_schema_keys.split(",").map(ue=>ue.trim()).filter(ue=>ue.length>0));const pe=await Ot.createTrainingJob({...e,is_vision:e.model_type==="vision",selective_loss:e.selective_loss,selective_loss_level:e.selective_loss_level,selective_loss_schema_keys:V,selective_loss_masking_start_step:e.selective_loss_masking_start_step,selective_loss_masking_start_epoch:e.selective_loss_masking_start_epoch,selective_loss_verbose:e.selective_loss_verbose,early_stopping_enabled:e.early_stopping_enabled,early_stopping_patience:e.early_stopping_patience,early_stopping_threshold:e.early_stopping_threshold});pe.success?Yr(`/training/${pe.data.job_id}`):q(ce,"Failed to create training job")}catch(V){q(ce,V instanceof Error?V.message:"Failed to create training job",!0)}finally{q(le,!1)}}var Fe=Ys();Gr(I=>{Wr.title="New Training Job - Model Garden"});var Qe=a(Fe),Nt=a(Qe),Pt=a(Nt),Dt=a(Pt),$a=a(Dt);Vt($a,{href:"/training",variant:"ghost",size:"sm",children:(I,V)=>{o();var pe=se("‚Üê Training Jobs");l(I,pe)},$$slots:{default:!0}}),o(2),r(Dt),r(Pt),r(Nt),r(Qe);var Ut=t(Qe,2),Ka=a(Ut);Xr(Ka,{children:(I,V)=>{var pe=Ks(),ue=a(pe);{var Ya=s=>{var i=Zr(),m=a(i),b=a(m,!0);r(m),r(i),D(()=>P(b,n(ce))),l(s,i)};c(ue,s=>{n(ce)&&s(Ya)})}var Oe=t(ue,2),Ht=t(a(Oe),2),Ve=a(Ht),It=t(a(Ve),2),he=a(It);he.__click=[es,e];var Gt=a(he),Ne=a(Gt),Xa=a(Ne);{var Za=s=>{var i=ts();l(s,i)};c(Xa,s=>{e.model_type==="text"&&s(Za)})}r(Ne),o(2),r(Gt),o(2),r(he);var Se=t(he,2);Se.__click=[as,e];var Jt=a(Se),Pe=a(Jt),er=a(Pe);{var tr=s=>{var i=rs();l(s,i)};c(er,s=>{e.model_type==="vision"&&s(tr)})}r(Pe),o(2),r(Jt),o(2),r(Se),r(It),r(Ve);var De=t(Ve,2),Wt=t(a(De),2);d(Wt),r(De);var Ue=t(De,2),He=t(a(Ue),2),Ie=a(He);Ie.__click=[ss,ae];var $t=t(Ie,2);$t.__click=[os,ae],r(He);var Kt=t(He,2);{var ar=s=>{var i=ps(),m=U(i),b=a(m);{var y=v=>{var f=is();f.value=f.__value="",l(v,f)},_=v=>{var f=_e(),E=U(f);{var F=h=>{var w=_e(),Q=U(w);Ga(Q,17,()=>n(ne),Ja,(N,A)=>{var k=ls(),z=a(k);r(k);var j={};D(()=>{P(z,`${n(A).name??""} (${n(A).parameters??""})`),j!==(j=n(A).id)&&(k.value=(k.__value=n(A).id)??"")}),l(N,k)}),l(h,w)},S=h=>{var w=_e(),Q=U(w);Ga(Q,17,()=>n(de),Ja,(N,A)=>{var k=ns(),z=a(k);r(k);var j={};D(()=>{P(z,`${n(A).name??""} (${n(A).parameters??""})`),j!==(j=n(A).id)&&(k.value=(k.__value=n(A).id)??"")}),l(N,k)}),l(h,w)};c(E,h=>{e.model_type==="text"?h(F):h(S,!1)},!0)}l(v,f)};c(b,v=>{n(xe)?v(y):v(_,!1)})}r(m);var g=t(m,2);{var L=v=>{var f=ds(),E=a(f);r(f),D(()=>P(E,`‚ö†Ô∏è Using fallback models: ${n(ze)??""}`)),l(v,f)};c(g,v=>{n(ze)&&v(L)})}D(()=>m.disabled=n(xe)),oe(m,()=>e.base_model,v=>e.base_model=v),l(s,i)},rr=s=>{var i=_s(),m=U(i);d(m);var b=t(m,2),y=t(a(b),2),_=a(y);{var g=v=>{var f=ms();o(4),l(v,f)},L=v=>{var f=vs();o(4),l(v,f)};c(_,v=>{e.model_type==="vision"?v(g):v(L,!1)})}r(y),r(b),D(()=>Qt(m,"placeholder",e.model_type==="vision"?"e.g., Qwen/Qwen2-VL-7B-Instruct":"e.g., meta-llama/Llama-2-7b-hf")),x(m,()=>e.base_model,v=>e.base_model=v),l(s,i)};c(Kt,s=>{n(ae)?s(rr,!1):s(ar)})}var Yt=t(Kt,2);{var sr=s=>{var i=cs();l(s,i)};c(Yt,s=>{e.model_type==="vision"&&!n(ae)&&s(sr)})}var or=t(Yt,2);{var ir=s=>{var i=ys(),m=a(i),b=a(m),y=a(b),_=a(y);r(y);var g=t(y,2),L=a(g,!0);r(g);var v=t(g,2);{var f=S=>{var h=gs(),w=a(h),Q=t(a(w));r(w);var N=t(w,2);{var A=k=>{var z=us(),j=t(a(z));r(z),D(G=>P(j,` ${G??""}
                              tokens`),[()=>n(T).capabilities.context_window.toLocaleString()]),l(k,z)};c(N,k=>{n(T).capabilities?.context_window&&k(A)})}r(h),D(()=>P(Q,` ${n(T).requirements.min_vram_gb??""}GB
                            minimum,
                            ${n(T).requirements.recommended_vram_gb??""}GB recommended`)),l(S,h)};c(v,S=>{n(T).requirements&&S(f)})}var E=t(v,2);{var F=S=>{var h=bs(),w=t(a(h));r(h),D(Q=>P(w,` ${Q??""}`),[()=>n(T).recommended_for.join(", ")]),l(S,h)};c(E,S=>{n(T).recommended_for&&n(T).recommended_for.length>0&&S(F)})}r(b),r(m),r(i),D(()=>{P(_,`üìä ${n(T).name??""}`),P(L,n(T).description)}),l(s,i)},lr=s=>{var i=_e(),m=U(i);{var b=y=>{var _=fs();l(y,_)};c(m,y=>{n(ae)&&y(b)},!0)}l(s,i)};c(or,s=>{n(T)&&!n(xe)&&!n(ae)?s(ir):s(lr,!1)})}r(Ue);var Ge=t(Ue,2),Me=t(a(Ge),2);d(Me);var Je=t(Me,2),Xt=a(Je);d(Xt),o(2),r(Je);var Zt=t(Je,2),nr=a(Zt);{var dr=s=>{var i=xs();o(2),l(s,i)},pr=s=>{var i=_e(),m=U(i);{var b=_=>{var g=se(`Path to your JSONL dataset with image paths/base64 or local
                  file`);l(_,g)},y=_=>{var g=se("Path to your JSONL dataset file");l(_,g)};c(m,_=>{e.model_type==="vision"?_(b):_(y,!1)},!0)}l(s,i)};c(nr,s=>{e.from_hub?s(dr):s(pr,!1)})}r(Zt),r(Ge);var We=t(Ge,2),Ae=t(a(We),2);d(Ae);var $e=t(Ae,2),ea=a($e);d(ea),o(2),r($e);var ta=t($e,2),mr=t(a(ta),3);{var vr=s=>{var i=se(`Use HuggingFace format: "username/repo" or
                  "username/repo::validation.jsonl"`);l(s,i)};c(mr,s=>{e.validation_from_hub&&s(vr)})}r(ta),r(We);var aa=t(We,2);{var _r=s=>{var i=ws(),m=t(a(i),2);{var b=_=>{var g=hs(),L=t(U(g),2),v=a(L);v.textContent='{"messages": [{"role": "user", "content": [{"type": "image", "image": "data:image/jpeg;base64,..."}, {"type": "text", "text": "What is shown?"}]}]}',r(L),o(2),l(_,g)},y=_=>{var g=ks(),L=t(U(g),2),v=a(L);v.textContent='{"text": "What is in this image?", "image": "/path/to/image.jpg", "response": "A cat sitting on a table"}',r(L),o(2),l(_,g)};c(m,_=>{e.from_hub?_(b):_(y,!1)})}r(i),l(s,i)};c(aa,s=>{e.model_type==="vision"&&s(_r)})}var ra=t(aa,2),Ke=t(a(ra),2);d(Ke);var sa=t(Ke,2),cr=a(sa);r(sa),r(ra),r(Ht),r(Oe);var Ye=t(Oe,2),Xe=a(Ye),ur=t(a(Xe),2);{var gr=s=>{var i=Ls();l(s,i)};c(ur,s=>{n(T)?.training_defaults&&s(gr)})}r(Xe);var oa=t(Xe,2);{var br=s=>{var i=Ss();l(s,i)};c(oa,s=>{e.model_type==="vision"&&s(br)})}var Ze=t(oa,2),ia=t(a(Ze),2),et=a(ia),tt=t(a(et),2);d(tt);var la=t(tt,2),yr=a(la);{var fr=s=>{var i=se(`2e-5 recommended for
                    vision models`);l(s,i)},xr=s=>{var i=se("2e-4 typical for text models");l(s,i)};c(yr,s=>{e.model_type==="vision"?s(fr):s(xr,!1)})}r(la),r(et);var at=t(et,2),na=t(a(at),2);d(na),o(2),r(at);var rt=t(at,2),st=t(a(rt),2);d(st);var da=t(st,2),hr=a(da);{var kr=s=>{var i=se("Use 1 for vision models");l(s,i)},wr=s=>{var i=se(`2-4
                    typical for text models`);l(s,i)};c(hr,s=>{e.model_type==="vision"?s(kr):s(wr,!1)})}r(da),r(rt);var ot=t(rt,2),pa=t(a(ot),2);d(pa),o(2),r(ot);var it=t(ot,2),ma=t(a(it),2);d(ma),o(2),r(it);var va=t(it,2),lt=t(a(va),2),nt=a(lt);nt.value=nt.__value="adamw_8bit";var dt=t(nt);dt.value=dt.__value="adamw_torch";var pt=t(dt);pt.value=pt.__value="adamw_torch_fused";var mt=t(pt);mt.value=mt.__value="adafactor";var _a=t(mt);_a.value=_a.__value="sgd",r(lt),o(2),r(va),r(ia),r(Ze);var vt=t(Ze,2),ca=t(a(vt),2),_t=a(ca),ua=t(a(_t),2);d(ua),o(2),r(_t);var ct=t(_t,2),ga=t(a(ct),2);d(ga),o(2),r(ct);var ba=t(ct,2),ya=t(a(ba),2);d(ya),o(2),r(ba),r(ca),r(vt);var fa=t(vt,2);{var Lr=s=>{var i=As(),m=U(i),b=t(a(m),2),y=a(b),_=t(a(y),2),g=a(_);g.value=g.__value="steps";var L=t(g);L.value=L.__value="epoch";var v=t(L);v.value=v.__value="no",r(_),r(y);var f=t(y,2),E=t(a(f),2);d(E),o(2),r(f);var F=t(f,2),S=t(a(F),2),h=a(S);h.value=h.__value="eval_loss";var w=t(h);w.value=w.__value="eval_accuracy";var Q=t(w);Q.value=Q.__value="eval_f1",r(S),r(F);var N=t(F,2),A=a(N),k=a(A);d(k),o(2),r(A),o(2),r(N),r(b),r(m);var z=t(m,2),j=t(a(z),4),G=a(j),J=a(G),W=a(J);d(W),o(2),r(J),o(2),r(G);var Z=t(G,2);{var $=R=>{var K=Ms(),Y=a(K),M=t(a(Y),2);d(M),o(2),r(Y);var ge=t(Y,2),B=t(a(ge),2);d(B),o(2),r(ge),o(2),r(K),x(M,()=>e.early_stopping_patience,ee=>e.early_stopping_patience=ee),x(B,()=>e.early_stopping_threshold,ee=>e.early_stopping_threshold=ee),l(R,K)};c(Z,R=>{e.early_stopping_enabled&&R($)})}r(j),r(z),oe(_,()=>e.hyperparameters.eval_strategy,R=>e.hyperparameters.eval_strategy=R),x(E,()=>e.hyperparameters.eval_steps,R=>e.hyperparameters.eval_steps=R),oe(S,()=>e.hyperparameters.metric_for_best_model,R=>e.hyperparameters.metric_for_best_model=R),H(k,()=>e.hyperparameters.load_best_model_at_end,R=>e.hyperparameters.load_best_model_at_end=R),H(W,()=>e.early_stopping_enabled,R=>e.early_stopping_enabled=R),l(s,i)};c(fa,s=>{e.validation_dataset_path&&s(Lr)})}var ut=t(fa,2),gt=a(ut);gt.__click=[Rs,je];var xa=a(gt),Sr=a(xa,!0);r(xa),o(),r(gt),r(ut);var Mr=t(ut,2);{var Ar=s=>{var i=Bs(),m=t(a(i),2),b=a(m),y=t(a(b),2);d(y),o(2),r(b);var _=t(b,2),g=t(a(_),2),L=a(g);L.value=L.__value="linear";var v=t(L);v.value=v.__value="cosine";var f=t(v);f.value=f.__value="constant";var E=t(f);E.value=E.__value="constant_with_warmup";var F=t(E);F.value=F.__value="polynomial",r(g),o(2),r(_);var S=t(_,2),h=t(a(S),2);d(h),o(2),r(S);var w=t(S,2),Q=t(a(w),2);d(Q),o(2),r(w),r(m);var N=t(m,4),A=a(N),k=t(a(A),2);d(k),o(2),r(A);var z=t(A,2),j=t(a(z),2);d(j),o(2),r(z);var G=t(z,2),J=t(a(G),2);d(J),o(2),r(G),r(N);var W=t(N,4),Z=a(W),$=t(a(Z),2);d($),o(2),r(Z);var R=t(Z,2),K=a(R),Y=a(K);d(Y),o(2),r(K),o(2),r(R),r(W),r(i),x(y,()=>e.hyperparameters.weight_decay,M=>e.hyperparameters.weight_decay=M),oe(g,()=>e.hyperparameters.lr_scheduler_type,M=>e.hyperparameters.lr_scheduler_type=M),x(h,()=>e.hyperparameters.warmup_steps,M=>e.hyperparameters.warmup_steps=M),x(Q,()=>e.hyperparameters.max_grad_norm,M=>e.hyperparameters.max_grad_norm=M),x(k,()=>e.hyperparameters.adam_beta1,M=>e.hyperparameters.adam_beta1=M),x(j,()=>e.hyperparameters.adam_beta2,M=>e.hyperparameters.adam_beta2=M),x(J,()=>e.hyperparameters.adam_epsilon,M=>e.hyperparameters.adam_epsilon=M),x($,()=>e.hyperparameters.dataloader_num_workers,M=>e.hyperparameters.dataloader_num_workers=M),H(Y,()=>e.hyperparameters.dataloader_pin_memory,M=>e.hyperparameters.dataloader_pin_memory=M),l(s,i)};c(Mr,s=>{n(je)&&s(Ar)})}r(Ye);var bt=t(Ye,2),ha=t(a(bt),4),yt=a(ha),ka=a(yt),wa=a(ka),La=a(wa),Sa=a(La);d(Sa),o(2),r(La),o(4),r(wa),r(ka),r(yt);var Ma=t(yt,2),Aa=t(a(Ma),4),ft=a(Aa),xt=a(ft);d(xt),o(2),r(ft);var ht=t(ft,2),kt=a(ht);d(kt),o(2),r(ht);var Rr=t(ht,2);{var Br=s=>{var i=Cs();l(s,i)};c(Rr,s=>{!e.quality_mode&&!e.load_in_16bit&&!e.load_in_8bit&&s(Br)})}r(Aa),r(Ma),r(ha),r(bt);var wt=t(bt,2),Lt=a(wt),Cr=t(a(Lt),2);{var qr=s=>{var i=qs();l(s,i)};c(Cr,s=>{n(T)?.training_defaults?.lora_config&&s(qr)})}r(Lt);var St=t(Lt,2),Mt=a(St),Ra=t(a(Mt),2);d(Ra),o(2),r(Mt);var At=t(Mt,2),Ba=t(a(At),2);d(Ba),o(2),r(At);var Ca=t(At,2),qa=t(a(Ca),2);d(qa),o(2),r(Ca),r(St);var Rt=t(St,2),Bt=a(Rt);Bt.__click=[Es,Te];var Ea=a(Bt),Er=a(Ea,!0);r(Ea),o(),r(Bt),r(Rt);var zr=t(Rt,2);{var jr=s=>{var i=Ts(),m=a(i),b=a(m),y=t(a(b),2),_=a(y);_.value=_.__value="none";var g=t(_);g.value=g.__value="all";var L=t(g);L.value=L.__value="lora_only",r(y),o(2),r(b);var v=t(b,2),f=t(a(v),2),E=a(f);E.value=E.__value="unsloth";var F=t(E);F.value=F.__value="true";var S=t(F);S.value=S.__value="false",r(f),o(2),r(v),r(m);var h=t(m,2),w=a(h),Q=a(w),N=a(Q);d(N),o(2),r(Q),o(2),r(w);var A=t(w,2),k=t(a(A),2);d(k),o(2),r(A),r(h);var z=t(h,2),j=a(z),G=t(a(j),2),J=a(G);J.value=J.__value="CAUSAL_LM";var W=t(J);W.value=W.__value="SEQ_2_SEQ_LM";var Z=t(W);Z.value=Z.__value="TOKEN_CLS";var $=t(Z);$.value=$.__value="SEQ_CLS";var R=t($);R.value=R.__value="QUESTION_ANS",r(G),o(2),r(j);var K=t(j,2),Y=t(a(K),2);d(Y),Y.__input=[zs,e],o(2),r(K),r(z);var M=t(z,2);{var ge=B=>{var ee=js(),Be=t(a(ee),4),me=a(Be),ke=a(me);d(ke),o(2),r(me);var be=t(me,2),we=a(be);d(we),o(2),r(be);var Le=t(be,2),Ce=a(Le);d(Ce),o(2),r(Le);var qe=t(Le,2),Ee=a(qe);d(Ee),o(2),r(qe),r(Be),o(2),r(ee),H(ke,()=>e.lora_config.finetune_vision_layers,te=>e.lora_config.finetune_vision_layers=te),H(we,()=>e.lora_config.finetune_language_layers,te=>e.lora_config.finetune_language_layers=te),H(Ce,()=>e.lora_config.finetune_attention_modules,te=>e.lora_config.finetune_attention_modules=te),H(Ee,()=>e.lora_config.finetune_mlp_modules,te=>e.lora_config.finetune_mlp_modules=te),l(B,ee)};c(M,B=>{e.model_type==="vision"&&B(ge)})}o(2),r(i),D(B=>Kr(Y,B),[()=>e.lora_config.target_modules?.join(", ")||""]),oe(y,()=>e.lora_config.lora_bias,B=>e.lora_config.lora_bias=B),oe(f,()=>e.lora_config.use_gradient_checkpointing,B=>e.lora_config.use_gradient_checkpointing=B),H(N,()=>e.lora_config.use_rslora,B=>e.lora_config.use_rslora=B),x(k,()=>e.lora_config.random_state,B=>e.lora_config.random_state=B),oe(G,()=>e.lora_config.task_type,B=>e.lora_config.task_type=B),l(s,i)};c(zr,s=>{n(Te)&&s(jr)})}r(wt);var Ct=t(wt,2),za=t(a(Ct),2),Re=t(a(za),2),qt=a(Re);qt.value=qt.__value="merged_16bit";var Et=t(qt);Et.value=Et.__value="merged_4bit";var ja=t(Et);ja.value=ja.__value="lora",r(Re);var Ta=t(Re,2),Fa=a(Ta),Tr=a(Fa);{var Fr=s=>{var i=Fs();o(),l(s,i)},Qr=s=>{var i=_e(),m=U(i);{var b=_=>{var g=Qs();o(),l(_,g)},y=_=>{var g=Os();o(),l(_,g)};c(m,_=>{e.save_method==="merged_4bit"?_(b):_(y,!1)},!0)}l(s,i)};c(Tr,s=>{e.save_method==="merged_16bit"?s(Fr):s(Qr,!1)})}r(Fa),r(Ta),r(za),r(Ct);var Qa=t(Ct,2);{var Or=s=>{var i=$s(),m=t(a(i),4),b=a(m),y=a(b),_=a(y);d(_),o(2),r(y),o(2),r(b);var g=t(b,2);{var L=v=>{var f=Ws(),E=a(f),F=t(a(E),2),S=a(F);S.value=S.__value="conservative";var h=t(S);h.value=h.__value="moderate";var w=t(h);w.value=w.__value="aggressive",r(F);var Q=t(F,2),N=a(Q),A=a(N);{var k=u=>{var C=Vs(),X=t(U(C),2);X.textContent='{, }, [, ], :, ,, "',o(2),l(u,C)},z=u=>{var C=_e(),X=U(C);{var ve=ye=>{var jt=Ns();o(3),l(ye,jt)},Dr=ye=>{var jt=Ps();o(),l(ye,jt)};c(X,ye=>{e.selective_loss_level==="moderate"?ye(ve):ye(Dr,!1)},!0)}l(u,C)};c(A,u=>{e.selective_loss_level==="conservative"?u(k):u(z,!1)})}r(N),r(Q),r(E);var j=t(E,2);{var G=u=>{var C=Ds(),X=t(a(C),2);d(X),o(4),r(C),x(X,()=>e.selective_loss_schema_keys,ve=>e.selective_loss_schema_keys=ve),l(u,C)};c(j,u=>{e.selective_loss_level==="aggressive"&&u(G)})}var J=t(j,2),W=a(J),Z=a(W);r(W);var $=t(W,2);d($);var R=t($,4),K=a(R),Y=t(a(K),2);{var M=u=>{var C=Us();o(),l(u,C)},ge=u=>{var C=Hs(),X=t(U(C)),ve=a(X);r(X),D(()=>P(ve,`Currently: Model learns structure for ${e.selective_loss_masking_start_step??""}
                            steps, then masking begins`)),l(u,C)};c(Y,u=>{e.selective_loss_masking_start_step===0?u(M):u(ge,!1)})}r(K),r(R),r(J);var B=t(J,2),ee=a(B),Be=a(ee);r(ee);var me=t(ee,2);d(me);var ke=t(me,4),be=a(ke),we=t(a(be),2);{var Le=u=>{var C=Is();o(),l(u,C)},Ce=u=>{var C=Gs(),X=t(U(C)),ve=a(X);r(X),D(()=>P(ve,`Currently: Model learns structure for ${e.selective_loss_masking_start_epoch??""}
                            epochs, then masking begins`)),l(u,C)};c(we,u=>{e.selective_loss_masking_start_epoch===0?u(Le):u(Ce,!1)})}var qe=t(we,2);{var Ee=u=>{var C=Js();o(2),l(u,C)};c(qe,u=>{e.selective_loss_masking_start_epoch>0&&e.selective_loss_masking_start_step>0&&u(Ee)})}r(be),r(ke),r(B);var te=t(B,2),Na=a(te),Pa=a(Na);d(Pa),o(2),r(Na),o(2),r(te);var Da=t(te,2),zt=t(a(Da),2),Ua=a(zt),Nr=t(a(Ua));Nr.textContent="{ } [ ] : ,",o(),r(Ua),o(8),r(zt);var Ha=t(zt,2),Ia=t(a(Ha),2);Ia.textContent='{"name": "John", "age": 30}';var Pr=t(Ia,2);Pr.textContent='{ } : , "',o(2),r(Ha),r(Da),r(f),D(()=>{P(Z,`Masking Start Step (Legacy): ${e.selective_loss_masking_start_step??""}`),P(Be,`Masking Start Epoch: ${e.selective_loss_masking_start_epoch??""}`)}),oe(F,()=>e.selective_loss_level,u=>e.selective_loss_level=u),x($,()=>e.selective_loss_masking_start_step,u=>e.selective_loss_masking_start_step=u),x(me,()=>e.selective_loss_masking_start_epoch,u=>e.selective_loss_masking_start_epoch=u),H(Pa,()=>e.selective_loss_verbose,u=>e.selective_loss_verbose=u),l(v,f)};c(g,v=>{e.selective_loss&&v(L)})}r(m),r(i),H(_,()=>e.selective_loss,v=>e.selective_loss=v),l(s,i)};c(Qa,s=>{e.model_type==="vision"&&s(Or)})}var Oa=t(Qa,2),Va=a(Oa);Vt(Va,{type:"submit",variant:"primary",get loading(){return n(le)},get disabled(){return n(le)},children:(s,i)=>{o();var m=se();D(()=>P(m,n(le)?"Creating...":"Start Training")),l(s,m)},$$slots:{default:!0}});var Vr=t(Va,2);Vt(Vr,{href:"/training",variant:"secondary",children:(s,i)=>{o();var m=se("Cancel");l(s,m)},$$slots:{default:!0}}),r(Oa),r(pe),D(()=>{fe(he,1,`p-4 border-2 rounded-lg text-left transition-all ${e.model_type==="text"?"border-primary-500 bg-primary-50":"border-gray-300 hover:border-gray-400"}`),fe(Ne,1,`w-4 h-4 rounded-full border-2 ${e.model_type==="text"?"border-primary-500 bg-primary-500":"border-gray-400"}`),fe(Se,1,`p-4 border-2 rounded-lg text-left transition-all ${e.model_type==="vision"?"border-primary-500 bg-primary-50":"border-gray-300 hover:border-gray-400"}`),fe(Pe,1,`w-4 h-4 rounded-full border-2 ${e.model_type==="vision"?"border-primary-500 bg-primary-500":"border-gray-400"}`),fe(Ie,1,`px-3 py-1.5 text-sm rounded-lg border ${n(ae)?"bg-white border-gray-300 text-gray-700 hover:bg-gray-50":"bg-primary-50 border-primary-500 text-primary-700 font-medium"}`),fe($t,1,`px-3 py-1.5 text-sm rounded-lg border ${n(ae)?"bg-primary-50 border-primary-500 text-primary-700 font-medium":"bg-white border-gray-300 text-gray-700 hover:bg-gray-50"}`),Qt(Me,"placeholder",e.from_hub?"username/dataset-name":e.model_type==="vision"?"./data/vision_dataset.jsonl":"./data/my-dataset.jsonl"),Qt(Ae,"placeholder",e.validation_from_hub?"username/val-dataset-name":e.model_type==="vision"?"./data/vision_val_dataset.jsonl":"./data/my-val-dataset.jsonl"),P(cr,`Model will be saved to models/${(e.output_dir||"my-model")??""}`),P(Sr,n(je)?"‚ñº":"‚ñ∂"),xt.disabled=e.quality_mode,kt.disabled=e.quality_mode||e.load_in_16bit,P(Er,n(Te)?"‚ñº":"‚ñ∂")}),$r("submit",pe,Wa),x(Wt,()=>e.name,s=>e.name=s),x(Me,()=>e.dataset_path,s=>e.dataset_path=s),H(Xt,()=>e.from_hub,s=>e.from_hub=s),x(Ae,()=>e.validation_dataset_path,s=>e.validation_dataset_path=s),H(ea,()=>e.validation_from_hub,s=>e.validation_from_hub=s),x(Ke,()=>e.output_dir,s=>e.output_dir=s),x(tt,()=>e.hyperparameters.learning_rate,s=>e.hyperparameters.learning_rate=s),x(na,()=>e.hyperparameters.num_epochs,s=>e.hyperparameters.num_epochs=s),x(st,()=>e.hyperparameters.batch_size,s=>e.hyperparameters.batch_size=s),x(pa,()=>e.hyperparameters.gradient_accumulation_steps,s=>e.hyperparameters.gradient_accumulation_steps=s),x(ma,()=>e.hyperparameters.max_steps,s=>e.hyperparameters.max_steps=s),oe(lt,()=>e.hyperparameters.optim,s=>e.hyperparameters.optim=s),x(ua,()=>e.hyperparameters.logging_steps,s=>e.hyperparameters.logging_steps=s),x(ga,()=>e.hyperparameters.save_steps,s=>e.hyperparameters.save_steps=s),x(ya,()=>e.hyperparameters.save_total_limit,s=>e.hyperparameters.save_total_limit=s),H(Sa,()=>e.quality_mode,s=>e.quality_mode=s),H(xt,()=>e.load_in_16bit,s=>e.load_in_16bit=s),H(kt,()=>e.load_in_8bit,s=>e.load_in_8bit=s),x(Ra,()=>e.lora_config.r,s=>e.lora_config.r=s),x(Ba,()=>e.lora_config.lora_alpha,s=>e.lora_config.lora_alpha=s),x(qa,()=>e.lora_config.lora_dropout,s=>e.lora_config.lora_dropout=s),oe(Re,()=>e.save_method,s=>e.save_method=s),l(I,pe)},$$slots:{default:!0}}),r(Ut),r(Fe),l(ie,Fe),Jr()}Ur(["click","input"]);export{lo as component};
