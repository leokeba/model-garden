import"../chunks/DsnmJJEf.js";import{q as La,p as Ma,a as Sa,D as ja,f as m,h as Aa,b as l,c as za,$ as $a,t as L,e as r,g as t,l as z,i as f,E as Fa,n as d,r as o,s as Ge,m as V,j as P,k as M,d as S}from"../chunks/Dt_rlGj7.js";import{i as _}from"../chunks/BpMPWboC.js";import{r as v,s as H,a as We,e as Ye,i as Ke}from"../chunks/C6mVS30y.js";import{b as c,c as Xe}from"../chunks/CZFHF5H2.js";import{b as Ze}from"../chunks/NmsYRWce.js";import{g as Oa}from"../chunks/Q_hmv9ZM.js";import{B as pe,C as Ra,a as Ba}from"../chunks/DvgRy-IG.js";function Ca($,y){y.name&&!y.output_dir&&(y.output_dir=`./models/${y.name.toLowerCase().replace(/[^a-z0-9]/g,"-")}`)}var Va=m('<div class="p-4 bg-red-50 border border-red-200 rounded-lg"><p class="text-red-700"> </p></div>'),Pa=($,y)=>y.model_type="text",Ha=m('<div class="w-full h-full rounded-full bg-white scale-50"></div>'),Ta=($,y)=>y.model_type="vision",qa=m('<div class="w-full h-full rounded-full bg-white scale-50"></div>'),Qa=m("<option> </option>"),Ea=m("<option> </option>"),Ia=m('<p class="text-xs text-gray-500 mt-1">üé® Vision-language models can analyze images and text together</p>'),Ja=m('Enter a HuggingFace dataset identifier (e.g., "username/dataset-name")<br/> For specific files, use: "username/repo::train.jsonl"',1),Na=m('<p class="text-sm text-blue-800 mb-2">HuggingFace datasets should use OpenAI messages format with base64 images:</p> <pre class="text-xs bg-blue-100 p-2 rounded overflow-x-auto"><code></code></pre> <p class="text-xs text-blue-700 mt-2"><strong>Example:</strong> <code>Barth371/train_pop_valet_no_wrong_doc</code></p>',1),Da=m('<p class="text-sm text-blue-800 mb-2">Your dataset should be in JSONL format with:</p> <pre class="text-xs bg-blue-100 p-2 rounded overflow-x-auto"><code></code></pre> <p class="text-xs text-blue-700 mt-2"><strong>Tip:</strong> Use <code>model-garden create-vision-dataset</code> CLI to generate sample data</p>',1),Ua=m('<div class="p-4 bg-blue-50 border border-blue-200 rounded-lg"><h4 class="text-sm font-semibold text-blue-900 mb-2">üìã Vision Dataset Format</h4> <!></div>'),Ga=m('<div class="mb-4 p-3 bg-yellow-50 border border-yellow-200 rounded-lg"><p class="text-sm text-yellow-800">‚ö†Ô∏è <strong>Vision models require:</strong> Lower batch size (1-2), higher gradient accumulation (8+), and lower learning rate (2e-5)</p></div>'),Wa=m("<strong>‚úÖ Merged 16-bit (Recommended):</strong> Full model with LoRA weights merged using Unsloth. Creates split files for vLLM compatibility.",1),Ya=m("<strong>üì¶ Merged 4-bit:</strong> Full model with LoRA weights merged in 4-bit quantized format. Smaller file size.",1),Ka=m("<strong>üîß LoRA Adapters Only (Advanced):</strong> Saves only the adapter weights. Requires the base model to load.",1),Xa=m('<form class="space-y-6"><!> <div><h3 class="text-lg font-semibold text-gray-900 mb-4">Basic Configuration</h3> <div class="grid grid-cols-1 gap-4"><div><div class="block text-sm font-medium text-gray-700 mb-2">Model Type *</div> <div class="grid grid-cols-2 gap-3"><button type="button"><div class="flex items-center gap-2"><div><!></div> <span class="font-medium">Text-Only (LLM)</span></div> <p class="text-sm text-gray-600 mt-2 ml-6">Fine-tune language models for text generation tasks</p></button> <button type="button"><div class="flex items-center gap-2"><div><!></div> <span class="font-medium">Vision-Language (VLM)</span></div> <p class="text-sm text-gray-600 mt-2 ml-6">Fine-tune multimodal models for image + text tasks</p></button></div></div> <div><label for="name" class="block text-sm font-medium text-gray-700 mb-1">Model Name *</label> <input type="text" id="name" placeholder="my-finance-model" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500" required/></div> <div><label for="base_model" class="block text-sm font-medium text-gray-700 mb-1">Base Model *</label> <select id="base_model" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500" required><!></select> <!></div> <div><label for="dataset_path" class="block text-sm font-medium text-gray-700 mb-1">Dataset Path *</label> <input type="text" id="dataset_path" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500" required/> <div class="mt-2 flex items-center"><input type="checkbox" id="from_hub" class="h-4 w-4 text-primary-600 focus:ring-primary-500 border-gray-300 rounded"/> <label for="from_hub" class="ml-2 block text-sm text-gray-700">Load from HuggingFace Hub</label></div> <p class="text-xs text-gray-500 mt-1"><!></p></div> <div><label for="validation_dataset_path" class="block text-sm font-medium text-gray-700 mb-1">Validation Dataset Path (Optional)</label> <input type="text" id="validation_dataset_path" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <div class="mt-2 flex items-center"><input type="checkbox" id="validation_from_hub" class="h-4 w-4 text-primary-600 focus:ring-primary-500 border-gray-300 rounded"/> <label for="validation_from_hub" class="ml-2 block text-sm text-gray-700">Load validation dataset from HuggingFace Hub</label></div> <p class="text-xs text-gray-500 mt-1">üìä Optional: Provide a validation dataset to track validation loss during training<br/> <!></p></div> <!> <div><label for="output_dir" class="block text-sm font-medium text-gray-700 mb-1">Output Directory</label> <input type="text" id="output_dir" placeholder="./models/my-model" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/></div></div></div> <div><h3 class="text-lg font-semibold text-gray-900 mb-4">Training Hyperparameters</h3> <!> <div class="grid grid-cols-2 gap-4"><div><label for="learning_rate" class="block text-sm font-medium text-gray-700 mb-1">Learning Rate</label> <input type="number" id="learning_rate" step="0.00001" min="0" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/></div> <div><label for="num_epochs" class="block text-sm font-medium text-gray-700 mb-1">Epochs</label> <input type="number" id="num_epochs" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/></div> <div><label for="batch_size" class="block text-sm font-medium text-gray-700 mb-1">Batch Size</label> <input type="number" id="batch_size" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/></div> <div><label for="gradient_accumulation" class="block text-sm font-medium text-gray-700 mb-1">Gradient Accumulation</label> <input type="number" id="gradient_accumulation" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Higher for vision models (8+)</p></div> <div><label for="max_steps" class="block text-sm font-medium text-gray-700 mb-1">Max Steps</label> <input type="number" id="max_steps" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">-1 for full epochs</p></div> <div><label for="eval_steps" class="block text-sm font-medium text-gray-700 mb-1">Evaluation Steps</label> <input type="number" id="eval_steps" placeholder="Auto (same as save_steps)" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Evaluate every N steps (only used if validation dataset provided)</p></div></div></div> <div><h3 class="text-lg font-semibold text-gray-900 mb-4">LoRA Configuration</h3> <div class="grid grid-cols-2 gap-4"><div><label for="lora_r" class="block text-sm font-medium text-gray-700 mb-1">LoRA Rank (r)</label> <input type="number" id="lora_r" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/></div> <div><label for="lora_alpha" class="block text-sm font-medium text-gray-700 mb-1">LoRA Alpha</label> <input type="number" id="lora_alpha" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/></div></div></div> <div><h3 class="text-lg font-semibold text-gray-900 mb-4">Model Save Options</h3> <div><label for="save_method" class="block text-sm font-medium text-gray-700 mb-2">Save Method</label> <select id="save_method" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"><option>Save Merged Model (16-bit) - Recommended</option><option>Save Merged Model (4-bit) - Smaller Size</option><option>Save LoRA Adapters Only - Advanced</option></select> <div class="mt-3 p-3 bg-blue-50 border border-blue-200 rounded-lg"><p class="text-sm text-blue-800"><!></p></div></div></div> <div class="flex gap-4 pt-4"><!> <!></div></form>'),Za=m('<div class="min-h-screen bg-gray-50"><div class="bg-white shadow"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex justify-between items-center py-6"><div class="flex items-center"><!> <h1 class="text-3xl font-bold text-gray-900 ml-4">New Training Job</h1></div></div></div></div> <div class="max-w-3xl mx-auto px-4 sm:px-6 lg:px-8 py-8"><!></div></div>');function dr($,y){Ma(y,!0);let a=Sa({name:"",model_type:"text",base_model:"unsloth/tinyllama-bnb-4bit",dataset_path:"./data/sample.jsonl",validation_dataset_path:"",output_dir:"",hyperparameters:{learning_rate:2e-4,num_epochs:3,batch_size:2,max_steps:-1,gradient_accumulation_steps:4,eval_steps:null},lora_config:{r:16,lora_alpha:16},from_hub:!1,validation_from_hub:!1,save_method:"merged_16bit"}),j=Ge(!1),h=Ge("");const me=["unsloth/tinyllama-bnb-4bit","unsloth/phi-2-bnb-4bit","unsloth/mistral-7b-bnb-4bit","unsloth/llama-2-7b-bnb-4bit","unsloth/llama-3-8b-bnb-4bit"],ve=["Qwen/Qwen2.5-VL-3B-Instruct","Qwen/Qwen2.5-VL-7B-Instruct","Qwen/Qwen2.5-VL-72B-Instruct","unsloth/Qwen2.5-VL-3B-Instruct-bnb-4bit","unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit"];ja(()=>{a.model_type==="vision"?(a.base_model=ve[0],a.dataset_path="./data/vision_dataset.jsonl",a.hyperparameters.batch_size=1,a.hyperparameters.gradient_accumulation_steps=8,a.hyperparameters.learning_rate=2e-5):(a.base_model=me[0],a.dataset_path="./data/sample.jsonl",a.hyperparameters.batch_size=2,a.hyperparameters.gradient_accumulation_steps=4,a.hyperparameters.learning_rate=2e-4)});async function ea(w){if(w.preventDefault(),!a.name||!a.base_model||!a.dataset_path){S(h,"Please fill in all required fields");return}S(j,!0),S(h,"");try{const x=await Ba.createTrainingJob({...a,is_vision:a.model_type==="vision"});x.success?Oa(`/training/${x.data.job_id}`):S(h,"Failed to create training job")}catch(x){S(h,x instanceof Error?x.message:"Failed to create training job",!0)}finally{S(j,!1)}}var T=Za();Aa(w=>{$a.title="New Training Job - Model Garden"});var q=r(T),ue=r(q),_e=r(ue),ce=r(_e),aa=r(ce);pe(aa,{href:"/training",variant:"ghost",size:"sm",children:(w,x)=>{d();var k=L("‚Üê Training Jobs");l(w,k)},$$slots:{default:!0}}),d(2),o(ce),o(_e),o(ue),o(q);var be=t(q,2),ra=r(be);Ra(ra,{children:(w,x)=>{var k=Xa(),ge=r(k);{var ta=e=>{var i=Va(),n=r(i),b=r(n,!0);o(n),o(i),z(()=>V(b,f(h))),l(e,i)};_(ge,e=>{f(h)&&e(ta)})}var Q=t(ge,2),fe=t(r(Q),2),E=r(fe),ye=t(r(E),2),A=r(ye);A.__click=[Pa,a];var xe=r(A),I=r(xe),oa=r(I);{var ia=e=>{var i=Ha();l(e,i)};_(oa,e=>{a.model_type==="text"&&e(ia)})}o(I),d(2),o(xe),d(2),o(A);var F=t(A,2);F.__click=[Ta,a];var he=r(F),J=r(he),sa=r(J);{var la=e=>{var i=qa();l(e,i)};_(sa,e=>{a.model_type==="vision"&&e(la)})}o(J),d(2),o(he),d(2),o(F),o(ye),o(E);var N=t(E,2),D=t(r(N),2);v(D),D.__input=[Ca,a],o(N);var U=t(N,2),O=t(r(U),2),da=r(O);{var na=e=>{var i=P(),n=M(i);Ye(n,17,()=>me,Ke,(b,u)=>{var s=Qa(),p=r(s,!0);o(s);var g={};z(()=>{V(p,f(u)),g!==(g=f(u))&&(s.value=(s.__value=f(u))??"")}),l(b,s)}),l(e,i)},pa=e=>{var i=P(),n=M(i);Ye(n,17,()=>ve,Ke,(b,u)=>{var s=Ea(),p=r(s,!0);o(s);var g={};z(()=>{V(p,f(u)),g!==(g=f(u))&&(s.value=(s.__value=f(u))??"")}),l(b,s)}),l(e,i)};_(da,e=>{a.model_type==="text"?e(na):e(pa,!1)})}o(O);var ma=t(O,2);{var va=e=>{var i=Ia();l(e,i)};_(ma,e=>{a.model_type==="vision"&&e(va)})}o(U);var G=t(U,2),R=t(r(G),2);v(R);var W=t(R,2),we=r(W);v(we),d(2),o(W);var ke=t(W,2),ua=r(ke);{var _a=e=>{var i=Ja();d(2),l(e,i)},ca=e=>{var i=P(),n=M(i);{var b=s=>{var p=L("Path to your JSONL dataset with image paths/base64 or local file");l(s,p)},u=s=>{var p=L("Path to your JSONL dataset file");l(s,p)};_(n,s=>{a.model_type==="vision"?s(b):s(u,!1)},!0)}l(e,i)};_(ua,e=>{a.from_hub?e(_a):e(ca,!1)})}o(ke),o(G);var Y=t(G,2),B=t(r(Y),2);v(B);var K=t(B,2),Le=r(K);v(Le),d(2),o(K);var Me=t(K,2),ba=t(r(Me),3);{var ga=e=>{var i=L('Use HuggingFace format: "username/repo" or "username/repo::validation.jsonl"');l(e,i)};_(ba,e=>{a.validation_from_hub&&e(ga)})}o(Me),o(Y);var Se=t(Y,2);{var fa=e=>{var i=Ua(),n=t(r(i),2);{var b=s=>{var p=Na(),g=t(M(p),2),ne=r(g);ne.textContent='{"messages": [{"role": "user", "content": [{"type": "image", "image": "data:image/jpeg;base64,..."}, {"type": "text", "text": "What is shown?"}]}]}',o(g),d(2),l(s,p)},u=s=>{var p=Da(),g=t(M(p),2),ne=r(g);ne.textContent='{"text": "What is in this image?", "image": "/path/to/image.jpg", "response": "A cat sitting on a table"}',o(g),d(2),l(s,p)};_(n,s=>{a.from_hub?s(b):s(u,!1)})}o(i),l(e,i)};_(Se,e=>{a.model_type==="vision"&&e(fa)})}var je=t(Se,2),Ae=t(r(je),2);v(Ae),o(je),o(fe),o(Q);var X=t(Q,2),ze=t(r(X),2);{var ya=e=>{var i=Ga();l(e,i)};_(ze,e=>{a.model_type==="vision"&&e(ya)})}var $e=t(ze,2),Z=r($e),Fe=t(r(Z),2);v(Fe),o(Z);var ee=t(Z,2),Oe=t(r(ee),2);v(Oe),o(ee);var ae=t(ee,2),Re=t(r(ae),2);v(Re),o(ae);var re=t(ae,2),Be=t(r(re),2);v(Be),d(2),o(re);var te=t(re,2),Ce=t(r(te),2);v(Ce),d(2),o(te);var Ve=t(te,2),Pe=t(r(Ve),2);v(Pe),d(2),o(Ve),o($e),o(X);var oe=t(X,2),He=t(r(oe),2),ie=r(He),Te=t(r(ie),2);v(Te),o(ie);var qe=t(ie,2),Qe=t(r(qe),2);v(Qe),o(qe),o(He),o(oe);var se=t(oe,2),Ee=t(r(se),2),C=t(r(Ee),2),le=r(C);le.value=le.__value="merged_16bit";var de=t(le);de.value=de.__value="merged_4bit";var Ie=t(de);Ie.value=Ie.__value="lora",o(C);var Je=t(C,2),Ne=r(Je),xa=r(Ne);{var ha=e=>{var i=Wa();d(),l(e,i)},wa=e=>{var i=P(),n=M(i);{var b=s=>{var p=Ya();d(),l(s,p)},u=s=>{var p=Ka();d(),l(s,p)};_(n,s=>{a.save_method==="merged_4bit"?s(b):s(u,!1)},!0)}l(e,i)};_(xa,e=>{a.save_method==="merged_16bit"?e(ha):e(wa,!1)})}o(Ne),o(Je),o(Ee),o(se);var De=t(se,2),Ue=r(De);pe(Ue,{type:"submit",variant:"primary",get loading(){return f(j)},get disabled(){return f(j)},children:(e,i)=>{d();var n=L();z(()=>V(n,f(j)?"Creating...":"Start Training")),l(e,n)},$$slots:{default:!0}});var ka=t(Ue,2);pe(ka,{href:"/training",variant:"secondary",children:(e,i)=>{d();var n=L("Cancel");l(e,n)},$$slots:{default:!0}}),o(De),o(k),z(()=>{H(A,1,`p-4 border-2 rounded-lg text-left transition-all ${a.model_type==="text"?"border-primary-500 bg-primary-50":"border-gray-300 hover:border-gray-400"}`),H(I,1,`w-4 h-4 rounded-full border-2 ${a.model_type==="text"?"border-primary-500 bg-primary-500":"border-gray-400"}`),H(F,1,`p-4 border-2 rounded-lg text-left transition-all ${a.model_type==="vision"?"border-primary-500 bg-primary-50":"border-gray-300 hover:border-gray-400"}`),H(J,1,`w-4 h-4 rounded-full border-2 ${a.model_type==="vision"?"border-primary-500 bg-primary-500":"border-gray-400"}`),We(R,"placeholder",a.from_hub?"username/dataset-name":a.model_type==="vision"?"./data/vision_dataset.jsonl":"./data/my-dataset.jsonl"),We(B,"placeholder",a.validation_from_hub?"username/val-dataset-name":a.model_type==="vision"?"./data/vision_val_dataset.jsonl":"./data/my-val-dataset.jsonl")}),Fa("submit",k,ea),c(D,()=>a.name,e=>a.name=e),Ze(O,()=>a.base_model,e=>a.base_model=e),c(R,()=>a.dataset_path,e=>a.dataset_path=e),Xe(we,()=>a.from_hub,e=>a.from_hub=e),c(B,()=>a.validation_dataset_path,e=>a.validation_dataset_path=e),Xe(Le,()=>a.validation_from_hub,e=>a.validation_from_hub=e),c(Ae,()=>a.output_dir,e=>a.output_dir=e),c(Fe,()=>a.hyperparameters.learning_rate,e=>a.hyperparameters.learning_rate=e),c(Oe,()=>a.hyperparameters.num_epochs,e=>a.hyperparameters.num_epochs=e),c(Re,()=>a.hyperparameters.batch_size,e=>a.hyperparameters.batch_size=e),c(Be,()=>a.hyperparameters.gradient_accumulation_steps,e=>a.hyperparameters.gradient_accumulation_steps=e),c(Ce,()=>a.hyperparameters.max_steps,e=>a.hyperparameters.max_steps=e),c(Pe,()=>a.hyperparameters.eval_steps,e=>a.hyperparameters.eval_steps=e),c(Te,()=>a.lora_config.r,e=>a.lora_config.r=e),c(Qe,()=>a.lora_config.lora_alpha,e=>a.lora_config.lora_alpha=e),Ze(C,()=>a.save_method,e=>a.save_method=e),l(w,k)},$$slots:{default:!0}}),o(be),o(T),l($,T),za()}La(["click","input"]);export{dr as component};
