import"../chunks/DsnmJJEf.js";import{q as ur,p as gr,a as ba,s as se,o as br,E as ya,i as l,d as R,f as v,h as yr,b as n,c as fr,$ as xr,t as Y,e as t,g as a,l as J,w as hr,n as i,r,m as N,j as ve,k as K}from"../chunks/D_fozEd9.js";import{i as u}from"../chunks/C2ngLJft.js";import{r as m,s as ke,a as yt,b as kr,e as ft,i as xt}from"../chunks/489nzvkF.js";import{b as y,c as oe}from"../chunks/aZwOC-1j.js";import{b as X}from"../chunks/DV7skBy0.js";import{g as wr}from"../chunks/2k1G2s8Z.js";import{a as fa}from"../chunks/GrWgsx-T.js";import{B as xa,C as Sr}from"../chunks/DWJusdRQ.js";var Lr=v('<div class="p-4 bg-red-50 border border-red-200 rounded-lg"><p class="text-red-700"> </p></div>'),Mr=(ie,O)=>O.model_type="text",Ar=v('<div class="w-full h-full rounded-full bg-white scale-50"></div>'),Br=(ie,O)=>O.model_type="vision",Cr=v('<div class="w-full h-full rounded-full bg-white scale-50"></div>'),Er=v("<option>Loading models...</option>"),Rr=v("<option> </option>"),Tr=v("<option> </option>"),jr=v('<p class="text-xs text-yellow-600 mt-1"> </p>'),Nr=v('<p class="text-xs text-gray-500 mt-1">üé® Vision-language models can analyze images and text together</p>'),Or=v('<p class="text-xs text-blue-700"><strong>Context:</strong> </p>'),zr=v('<div class="space-y-1"><p class="text-xs text-blue-700"><strong>VRAM:</strong> </p> <!></div>'),Pr=v('<p class="text-xs text-blue-700 mt-2"><strong>Best for:</strong> </p>'),qr=v('<div class="mt-3 p-3 bg-blue-50 border border-blue-200 rounded-lg"><div class="flex items-start justify-between"><div class="flex-1"><h4 class="text-sm font-semibold text-blue-900 mb-1"> </h4> <p class="text-xs text-blue-800 mb-2"> </p> <!> <!></div></div></div>'),Fr=v(`Enter a HuggingFace dataset identifier (e.g.,
                  "username/dataset-name")<br/> For specific files, use: "username/repo::train.jsonl"`,1),Qr=v(`<p class="text-sm text-blue-800 mb-2">HuggingFace datasets should use OpenAI messages format with
                    base64 images:</p> <pre class="text-xs bg-blue-100 p-2 rounded overflow-x-auto"><code></code></pre> <p class="text-xs text-blue-700 mt-2"><strong>Example:</strong> <code>Barth371/train_pop_valet_no_wrong_doc</code></p>`,1),Vr=v(`<p class="text-sm text-blue-800 mb-2">Your dataset should be in JSONL format with:</p> <pre class="text-xs bg-blue-100 p-2 rounded overflow-x-auto"><code></code></pre> <p class="text-xs text-blue-700 mt-2"><strong>Tip:</strong> Use <code>model-garden create-vision-dataset</code> CLI to generate
                    sample data</p>`,1),Ur=v('<div class="p-4 bg-blue-50 border border-blue-200 rounded-lg"><h4 class="text-sm font-semibold text-blue-900 mb-2">üìã Vision Dataset Format</h4> <!></div>'),$r=v('<span class="text-xs text-green-600 bg-green-50 px-2 py-1 rounded">‚úì Using registry defaults</span>'),Gr=v(`<div class="mb-4 p-3 bg-yellow-50 border border-yellow-200 rounded-lg"><p class="text-sm text-yellow-800">‚ö†Ô∏è <strong>Vision models require:</strong> Lower batch size (1-2),
                higher gradient accumulation (8+), and lower learning rate (2e-5)</p></div>`),Jr=v(`<div class="ml-6 space-y-4 p-4 bg-white border border-gray-200 rounded-lg"><div><label for="early_stopping_patience" class="block text-sm font-medium text-gray-700 mb-1">Patience (evaluations)</label> <input type="number" id="early_stopping_patience" min="1" max="20" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Number of evaluations with no improvement before
                        stopping (3-5 typical)</p></div> <div><label for="early_stopping_threshold" class="block text-sm font-medium text-gray-700 mb-1">Improvement Threshold</label> <input type="number" id="early_stopping_threshold" min="0" step="0.0001" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Minimum change to qualify as improvement (0.0001 =
                        0.01%, smaller = more sensitive)</p></div> <div class="p-3 bg-green-50 border border-green-200 rounded-lg"><p class="text-xs text-green-800"><strong>üí° Example:</strong> With patience=3 and threshold=0.0001,
                        training stops if validation loss doesn't improve by at least
                        0.01% for 3 consecutive evaluations.</p></div></div>`),Dr=v(`<div class="mb-6"><h4 class="text-md font-medium text-gray-800 mb-3 flex items-center gap-2">üìä Evaluation Settings</h4> <div class="grid grid-cols-2 gap-4"><div><label for="eval_strategy" class="block text-sm font-medium text-gray-700 mb-1">Evaluation Strategy</label> <select id="eval_strategy" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"><option>Every N steps</option><option>Every epoch</option><option>No evaluation</option></select></div> <div><label for="eval_steps" class="block text-sm font-medium text-gray-700 mb-1">Evaluation Steps</label> <input type="number" id="eval_steps" placeholder="Auto (same as save_steps)" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Evaluate every N steps (leave empty for auto)</p></div> <div><label for="metric_for_best_model" class="block text-sm font-medium text-gray-700 mb-1">Best Model Metric</label> <select id="metric_for_best_model" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"><option>Validation Loss (lower is better)</option><option>Accuracy (higher is better)</option><option>F1 Score (higher is better)</option></select></div> <div><div class="flex items-center mt-6"><input type="checkbox" id="load_best_model_at_end" class="h-4 w-4 text-primary-600 focus:ring-primary-500 border-gray-300 rounded"/> <label for="load_best_model_at_end" class="ml-2 block text-sm text-gray-700">Load best model at end</label></div> <p class="text-xs text-gray-500 mt-1">Automatically load checkpoint with best validation metric</p></div></div></div> <div class="mb-6"><h4 class="text-md font-medium text-gray-800 mb-3 flex items-center gap-2">‚è∏Ô∏è Automatic Early Stopping</h4> <div class="p-4 bg-blue-50 border border-blue-200 rounded-lg mb-4"><p class="text-sm text-blue-800 mb-2"><strong>Automatic Early Stopping:</strong> Stops training when
                  validation loss stops improving, preventing overfitting and saving
                  compute time.</p> <p class="text-xs text-blue-700">This is different from the manual "Stop Early" button on the
                  training page. This monitors validation metrics and stops
                  automatically.</p></div> <div class="space-y-4"><div><div class="flex items-center"><input type="checkbox" id="early_stopping_enabled" class="h-4 w-4 text-primary-600 focus:ring-primary-500 border-gray-300 rounded"/> <label for="early_stopping_enabled" class="ml-2 block text-sm font-medium text-gray-700">Enable Automatic Early Stopping</label></div> <p class="text-xs text-gray-500 mt-1 ml-6">Monitor validation loss and stop when it stops improving</p></div> <!></div></div>`,1),Hr=(ie,O)=>R(O,!l(O)),Ir=v('<div class="mb-6 p-4 bg-gray-50 border border-gray-200 rounded-lg"><h4 class="text-md font-medium text-gray-800 mb-3 flex items-center gap-2">‚öôÔ∏è Optimizer Settings</h4> <div class="grid grid-cols-2 gap-4 mb-6"><div><label for="weight_decay" class="block text-sm font-medium text-gray-700 mb-1">Weight Decay</label> <input type="number" id="weight_decay" step="0.001" min="0" max="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">L2 regularization strength (0.01 typical)</p></div> <div><label for="lr_scheduler_type" class="block text-sm font-medium text-gray-700 mb-1">LR Scheduler</label> <select id="lr_scheduler_type" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"><option>Linear (default)</option><option>Cosine (good for vision)</option><option>Constant</option><option>Constant with Warmup</option><option>Polynomial</option></select> <p class="text-xs text-gray-500 mt-1">Learning rate schedule type</p></div> <div><label for="warmup_steps" class="block text-sm font-medium text-gray-700 mb-1">Warmup Steps</label> <input type="number" id="warmup_steps" min="0" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Steps to warmup learning rate from 0</p></div> <div><label for="max_grad_norm" class="block text-sm font-medium text-gray-700 mb-1">Max Gradient Norm</label> <input type="number" id="max_grad_norm" step="0.1" min="0" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Gradient clipping threshold (1.0 standard)</p></div></div> <h4 class="text-md font-medium text-gray-800 mb-3 flex items-center gap-2">üéõÔ∏è Adam Optimizer Parameters</h4> <div class="grid grid-cols-3 gap-4 mb-6"><div><label for="adam_beta1" class="block text-sm font-medium text-gray-700 mb-1">Adam Beta1</label> <input type="number" id="adam_beta1" step="0.01" min="0" max="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Exponential decay rate for 1st moment (0.9 default)</p></div> <div><label for="adam_beta2" class="block text-sm font-medium text-gray-700 mb-1">Adam Beta2</label> <input type="number" id="adam_beta2" step="0.001" min="0" max="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Exponential decay rate for 2nd moment (0.999 default)</p></div> <div><label for="adam_epsilon" class="block text-sm font-medium text-gray-700 mb-1">Adam Epsilon</label> <input type="number" id="adam_epsilon" step="1e-9" min="0" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Small constant for numerical stability (1e-8 default)</p></div></div> <h4 class="text-md font-medium text-gray-800 mb-3 flex items-center gap-2">üîÑ Data Loading Settings</h4> <div class="grid grid-cols-2 gap-4"><div><label for="dataloader_num_workers" class="block text-sm font-medium text-gray-700 mb-1">Dataloader Workers</label> <input type="number" id="dataloader_num_workers" min="0" max="16" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Number of worker processes (0 = main process only)</p></div> <div><div class="flex items-center mt-6"><input type="checkbox" id="dataloader_pin_memory" class="h-4 w-4 text-primary-600 focus:ring-primary-500 border-gray-300 rounded"/> <label for="dataloader_pin_memory" class="ml-2 block text-sm text-gray-700">Pin memory to GPU</label></div> <p class="text-xs text-gray-500 mt-1">Faster data transfer to GPU (recommended)</p></div></div></div>'),Wr=v('<span class="text-xs text-green-600 bg-green-50 px-2 py-1 rounded">‚úì Using registry defaults</span>'),Kr=(ie,O)=>R(O,!l(O)),Yr=(ie,O)=>{const e=ie.currentTarget.value.trim();e?O.lora_config.target_modules=e.split(",").map(Z=>Z.trim()).filter(Z=>Z.length>0):O.lora_config.target_modules=null},Xr=v(`<div class="p-4 bg-gray-50 border border-gray-200 rounded-lg"><div class="grid grid-cols-2 gap-4 mb-4"><div><label for="lora_bias" class="block text-sm font-medium text-gray-700 mb-1">LoRA Bias</label> <select id="lora_bias" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"><option>None (default)</option><option>All bias terms</option><option>LoRA layers only</option></select> <p class="text-xs text-gray-500 mt-1">How to handle bias parameters in LoRA layers</p></div> <div><label for="use_gradient_checkpointing" class="block text-sm font-medium text-gray-700 mb-1">Gradient Checkpointing</label> <select id="use_gradient_checkpointing" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"><option>Unsloth (recommended)</option><option>Standard PyTorch</option><option>Disabled</option></select> <p class="text-xs text-gray-500 mt-1">Reduces memory at cost of compute time</p></div></div> <div class="grid grid-cols-2 gap-4 mb-4"><div><div class="flex items-center mt-2"><input type="checkbox" id="use_rslora" class="h-4 w-4 text-primary-600 focus:ring-primary-500 border-gray-300 rounded"/> <label for="use_rslora" class="ml-2 block text-sm text-gray-700">Use RSLoRA (Rank-Stabilized LoRA)</label></div> <p class="text-xs text-gray-500 mt-1">Better stability for high ranks (r > 16)</p></div> <div><label for="random_state" class="block text-sm font-medium text-gray-700 mb-1">Random Seed</label> <input type="number" id="random_state" min="0" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Seed for reproducible results (42 is popular)</p></div></div> <div class="grid grid-cols-2 gap-4 mb-4"><div><label for="task_type" class="block text-sm font-medium text-gray-700 mb-1">Task Type</label> <select id="task_type" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"><option>Causal LM (Text Generation)</option><option>Sequence-to-Sequence</option><option>Token Classification</option><option>Sequence Classification</option><option>Question Answering</option></select> <p class="text-xs text-gray-500 mt-1">Type of task for PEFT optimization</p></div> <div><label for="target_modules_input" class="block text-sm font-medium text-gray-700 mb-1">Target Modules (Advanced)</label> <input type="text" id="target_modules_input" placeholder="q_proj, k_proj, v_proj (leave empty for auto)" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Comma-separated list of layers to apply LoRA (auto-detected
                    if empty)</p></div></div> <div class="p-3 bg-blue-50 border border-blue-200 rounded-lg"><h4 class="text-sm font-semibold text-blue-900 mb-2">üí° LoRA Tips</h4> <ul class="text-xs text-blue-800 space-y-1"><li><strong>Rank (r):</strong> Start with 16, increase to 64+ for
                    complex tasks or large datasets</li> <li><strong>Alpha:</strong> Usually equal to rank. Higher alpha =
                    stronger adaptation</li> <li><strong>Dropout:</strong> Add 0.1-0.3 if overfitting, keep 0
                    for small datasets</li> <li><strong>RSLoRA:</strong> Enable for ranks > 16 to improve training
                    stability</li> <li><strong>Target Modules:</strong> Leave empty for auto-detection.
                    Common: "q_proj,k_proj,v_proj,o_proj" for attention layers</li> <li><strong>Task Type:</strong> Use "CAUSAL_LM" for text generation,
                    "SEQ_2_SEQ_LM" for translation/summarization</li></ul></div></div>`),Zr=v(`<strong>‚úÖ Merged 16-bit (Recommended):</strong> Full model with
                  LoRA weights merged using Unsloth. Creates split files for vLLM
                  compatibility.`,1),es=v(`<strong>üì¶ Merged 4-bit:</strong> Full model with LoRA weights
                  merged in 4-bit quantized format. Smaller file size.`,1),as=v(`<strong>üîß LoRA Adapters Only (Advanced):</strong> Saves only the
                  adapter weights. Requires the base model to load.`,1),ts=v(`<strong>Conservative:</strong> Masks JSON structural
                          characters: <code></code> and
                          whitespace. Masks ~31% of tokens. <em>Recommended for most cases.</em>`,1),rs=v("<strong>Moderate:</strong> Conservative + masks <code>null</code> keyword. Good when null values are predictable.",1),ss=v(`<strong>Aggressive:</strong> Moderate + masks schema field
                          names. Maximum focus on semantic content. Requires specifying
                          schema keys below.`,1),os=v(`<div><label for="selective_loss_schema_keys" class="block text-sm font-medium text-gray-700 mb-1">Schema Keys to Mask</label> <input type="text" id="selective_loss_schema_keys" placeholder="Marque,Modele,contents,confidence_score" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Comma-separated list of JSON field names to mask (e.g.,
                        "name,address,phone")</p> <div class="mt-2 p-2 bg-yellow-50 border border-yellow-200 rounded"><p class="text-xs text-yellow-800">‚ö†Ô∏è Only mask keys that are predictable and don't carry
                          semantic meaning. The model should still learn what
                          values go with each key.</p></div></div>`),is=v(`<br/><em>Currently: Masking starts immediately (traditional
                            approach)</em>`,1),ls=v("<br/><em> </em>",1),ns=v(`<div class="ml-6 space-y-4 p-4 bg-white border border-gray-200 rounded-lg"><div><label for="selective_loss_level" class="block text-sm font-medium text-gray-700 mb-2">Masking Level</label> <select id="selective_loss_level" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"><option>Conservative (Structure Only)</option><option>Moderate (Structure + null)</option><option>Aggressive (Structure + null + Schema Keys)</option></select> <div class="mt-2 p-3 bg-gray-50 rounded-lg"><p class="text-xs text-gray-700"><!></p></div></div> <!> <div><label for="selective_loss_masking_start_step" class="block text-sm font-medium text-gray-700 mb-2"> </label> <input type="range" id="selective_loss_masking_start_step" min="0" max="500" step="10" class="w-full h-2 bg-gray-200 rounded-lg appearance-none cursor-pointer accent-primary-500"/> <div class="flex justify-between text-xs text-gray-500 mt-1"><span>0 (Immediate)</span> <span>100</span> <span>200</span> <span>500 steps</span></div> <div class="mt-2 p-3 bg-blue-50 rounded-lg"><p class="text-xs text-blue-700"><strong>üí° Tip:</strong> Setting this to 50-200 lets the
                        model learn JSON structure first before applying
                        selective masking. This can prevent degeneration issues
                        with aggressive masking. <!></p></div></div> <div><div class="flex items-center"><input type="checkbox" id="selective_loss_verbose" class="h-4 w-4 text-primary-600 focus:ring-primary-500 border-gray-300 rounded"/> <label for="selective_loss_verbose" class="ml-2 block text-sm text-gray-700">Verbose mode (print masking statistics)</label></div> <p class="text-xs text-gray-500 mt-1">Display detailed token masking stats during training</p></div> <div class="p-3 bg-blue-50 border border-blue-200 rounded-lg"><h4 class="text-sm font-semibold text-blue-900 mb-2">üìä What Gets Masked?</h4> <ul class="text-xs text-blue-800 space-y-1"><li>‚úì Structural: <code></code> and whitespace
                        (spaces, newlines, tabs)</li> <li>‚úì Quotes: <code>"</code> (string delimiters - purely structural)</li> <li>‚úì Null keyword: <code>null</code> (moderate/aggressive only)</li> <li>‚úó NOT masked: <code>true</code>, <code>false</code> (can
                        be semantic)</li> <li>‚úì Schema keys: Field names like <code>name</code> (aggressive
                        only)</li></ul> <p class="text-xs text-blue-700 mt-2"><strong>Example:</strong> In <code></code>, conservative
                      mode masks <code></code> and spaces (~31% of tokens),
                      trains on <code>name John age 30</code></p></div></div>`),ds=v(`<div><h3 class="text-lg font-semibold text-gray-900 mb-4">üéØ Selective Loss (Structured Outputs)</h3> <div class="p-4 bg-gradient-to-r from-purple-50 to-blue-50 border border-purple-200 rounded-lg mb-4"><p class="text-sm text-gray-800 mb-2"><strong>üî¨ Experimental Feature:</strong> Optimize training for structured
                outputs (JSON, forms, etc.)</p> <p class="text-xs text-gray-700">Masks structural tokens (braces, colons, whitespace) so the
                model focuses on semantic content. Useful for form extraction,
                structured data generation, and similar tasks.</p></div> <div class="space-y-4"><div><div class="flex items-center"><input type="checkbox" id="selective_loss" class="h-4 w-4 text-primary-600 focus:ring-primary-500 border-gray-300 rounded"/> <label for="selective_loss" class="ml-2 block text-sm font-medium text-gray-700">Enable Selective Loss Masking</label></div> <p class="text-xs text-gray-500 mt-1 ml-6">Automatically mask JSON structural tokens during training</p></div> <!></div></div>`),ps=v(`<form class="space-y-6"><!> <div><h3 class="text-lg font-semibold text-gray-900 mb-4">Basic Configuration</h3> <div class="grid grid-cols-1 gap-4"><div><div class="block text-sm font-medium text-gray-700 mb-2">Model Type *</div> <div class="grid grid-cols-2 gap-3"><button type="button"><div class="flex items-center gap-2"><div><!></div> <span class="font-medium">Text-Only (LLM)</span></div> <p class="text-sm text-gray-600 mt-2 ml-6">Fine-tune language models for text generation tasks</p></button> <button type="button"><div class="flex items-center gap-2"><div><!></div> <span class="font-medium">Vision-Language (VLM)</span></div> <p class="text-sm text-gray-600 mt-2 ml-6">Fine-tune multimodal models for image + text tasks</p></button></div></div> <div><label for="name" class="block text-sm font-medium text-gray-700 mb-1">Model Name *</label> <input type="text" id="name" placeholder="my-finance-model" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500" required/></div> <div><label for="base_model" class="block text-sm font-medium text-gray-700 mb-1">Base Model *</label> <select id="base_model" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500" required><!></select> <!> <!> <!></div> <div><label for="dataset_path" class="block text-sm font-medium text-gray-700 mb-1">Dataset Path *</label> <input type="text" id="dataset_path" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500" required/> <div class="mt-2 flex items-center"><input type="checkbox" id="from_hub" class="h-4 w-4 text-primary-600 focus:ring-primary-500 border-gray-300 rounded"/> <label for="from_hub" class="ml-2 block text-sm text-gray-700">Load from HuggingFace Hub</label></div> <p class="text-xs text-gray-500 mt-1"><!></p></div> <div><label for="validation_dataset_path" class="block text-sm font-medium text-gray-700 mb-1">Validation Dataset Path (Optional)</label> <input type="text" id="validation_dataset_path" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <div class="mt-2 flex items-center"><input type="checkbox" id="validation_from_hub" class="h-4 w-4 text-primary-600 focus:ring-primary-500 border-gray-300 rounded"/> <label for="validation_from_hub" class="ml-2 block text-sm text-gray-700">Load validation dataset from HuggingFace Hub</label></div> <p class="text-xs text-gray-500 mt-1">üìä Optional: Provide a validation dataset to track validation
                loss during training<br/> <!></p></div> <!> <div><label for="output_dir" class="block text-sm font-medium text-gray-700 mb-1">Output Directory</label> <input type="text" id="output_dir" placeholder="my-model" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="mt-1 text-sm text-gray-500"> </p></div></div></div> <div><div class="flex items-center justify-between mb-4"><h3 class="text-lg font-semibold text-gray-900">Training Hyperparameters</h3> <!></div> <!> <div class="mb-6"><h4 class="text-md font-medium text-gray-800 mb-3 flex items-center gap-2">üéØ Essential Parameters</h4> <div class="grid grid-cols-2 gap-4"><div><label for="learning_rate" class="block text-sm font-medium text-gray-700 mb-1">Learning Rate</label> <input type="number" id="learning_rate" step="0.00001" min="0" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1"><!></p></div> <div><label for="num_epochs" class="block text-sm font-medium text-gray-700 mb-1">Number of Epochs</label> <input type="number" id="num_epochs" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Number of complete passes through dataset</p></div> <div><label for="batch_size" class="block text-sm font-medium text-gray-700 mb-1">Batch Size per GPU</label> <input type="number" id="batch_size" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1"><!></p></div> <div><label for="gradient_accumulation" class="block text-sm font-medium text-gray-700 mb-1">Gradient Accumulation Steps</label> <input type="number" id="gradient_accumulation" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Effective batch size = batch_size √ó
                  gradient_accumulation_steps</p></div> <div><label for="max_steps" class="block text-sm font-medium text-gray-700 mb-1">Max Steps (Optional)</label> <input type="number" id="max_steps" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Override epochs with exact step count (-1 for full epochs)</p></div> <div><label for="optim" class="block text-sm font-medium text-gray-700 mb-1">Optimizer</label> <select id="optim" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"><option>AdamW 8-bit (Recommended - Memory Efficient)</option><option>AdamW (PyTorch)</option><option>AdamW Fused (Faster)</option><option>Adafactor (Very Memory Efficient)</option><option>SGD</option></select> <p class="text-xs text-gray-500 mt-1">8-bit AdamW reduces memory usage significantly</p></div></div></div> <div class="mb-6"><h4 class="text-md font-medium text-gray-800 mb-3 flex items-center gap-2">üíæ Checkpoints & Logging</h4> <div class="grid grid-cols-3 gap-4"><div><label for="logging_steps" class="block text-sm font-medium text-gray-700 mb-1">Logging Steps</label> <input type="number" id="logging_steps" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Log metrics every N steps</p></div> <div><label for="save_steps" class="block text-sm font-medium text-gray-700 mb-1">Save Steps</label> <input type="number" id="save_steps" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Save checkpoint every N steps</p></div> <div><label for="save_total_limit" class="block text-sm font-medium text-gray-700 mb-1">Max Checkpoints</label> <input type="number" id="save_total_limit" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Keep only N most recent checkpoints</p></div></div></div> <!> <div class="mb-4"><button type="button" class="flex items-center gap-2 px-4 py-2 text-sm font-medium text-primary-700 bg-primary-50 border border-primary-200 rounded-lg hover:bg-primary-100 transition-colors"><span> </span> Advanced Hyperparameters</button></div> <!></div> <div><div class="flex items-center justify-between mb-4"><h3 class="text-lg font-semibold text-gray-900">LoRA Configuration</h3> <!></div> <div class="grid grid-cols-3 gap-4 mb-4"><div><label for="lora_r" class="block text-sm font-medium text-gray-700 mb-1">LoRA Rank (r)</label> <input type="number" id="lora_r" min="1" max="256" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Higher = more parameters (16 typical, 64+ for complex tasks)</p></div> <div><label for="lora_alpha" class="block text-sm font-medium text-gray-700 mb-1">LoRA Alpha</label> <input type="number" id="lora_alpha" min="1" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Scaling factor (typically equal to rank)</p></div> <div><label for="lora_dropout" class="block text-sm font-medium text-gray-700 mb-1">LoRA Dropout</label> <input type="number" id="lora_dropout" min="0" max="0.5" step="0.05" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"/> <p class="text-xs text-gray-500 mt-1">Regularization (0.0-0.3, 0 = no dropout)</p></div></div> <div class="mb-4"><button type="button" class="flex items-center gap-2 px-4 py-2 text-sm font-medium text-primary-700 bg-primary-50 border border-primary-200 rounded-lg hover:bg-primary-100 transition-colors"><span> </span> Advanced LoRA Settings</button></div> <!></div> <div><h3 class="text-lg font-semibold text-gray-900 mb-4">Model Save Options</h3> <div><label for="save_method" class="block text-sm font-medium text-gray-700 mb-2">Save Method</label> <select id="save_method" class="w-full px-3 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-primary-500 focus:border-primary-500"><option>Save Merged Model (16-bit) - Recommended</option><option>Save Merged Model (4-bit) - Smaller Size</option><option>Save LoRA Adapters Only - Advanced</option></select> <div class="mt-3 p-3 bg-blue-50 border border-blue-200 rounded-lg"><p class="text-sm text-blue-800"><!></p></div></div></div> <!> <div class="flex gap-4 pt-4"><!> <!></div></form>`),ms=v('<div class="min-h-screen bg-gray-50"><div class="bg-white shadow"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex justify-between items-center py-6"><div class="flex items-center"><!> <h1 class="text-3xl font-bold text-gray-900 ml-4">New Training Job</h1></div></div></div></div> <div class="max-w-3xl mx-auto px-4 sm:px-6 lg:px-8 py-8"><!></div></div>');function hs(ie,O){gr(O,!0);let e=ba({name:"",model_type:"text",base_model:"unsloth/tinyllama-bnb-4bit",dataset_path:"./data/sample.jsonl",validation_dataset_path:"",output_dir:"",hyperparameters:{learning_rate:2e-4,num_epochs:3,batch_size:2,max_steps:-1,gradient_accumulation_steps:4,warmup_steps:10,logging_steps:10,save_steps:100,eval_steps:null,optim:"adamw_8bit",weight_decay:.01,lr_scheduler_type:"linear",max_grad_norm:1,adam_beta1:.9,adam_beta2:.999,adam_epsilon:1e-8,dataloader_num_workers:0,dataloader_pin_memory:!0,eval_strategy:"steps",load_best_model_at_end:!0,metric_for_best_model:"eval_loss",save_total_limit:3},lora_config:{r:16,lora_alpha:16,lora_dropout:0,lora_bias:"none",use_rslora:!1,use_gradient_checkpointing:"unsloth",random_state:42,target_modules:null,task_type:"CAUSAL_LM",loftq_config:null},from_hub:!1,validation_from_hub:!1,save_method:"merged_16bit",selective_loss:!1,selective_loss_level:"conservative",selective_loss_schema_keys:"",selective_loss_masking_start_step:0,selective_loss_verbose:!1,early_stopping_enabled:!1,early_stopping_patience:3,early_stopping_threshold:1e-4}),Z=se(!1),le=se(""),ee=se(ba([])),ae=se(ba([])),C=se(null),_e=se(!0),we=se("");br(async()=>{try{R(_e,!0);const[z,E]=await Promise.all([fa.getRegistryModels("text-llm"),fa.getRegistryModels("vision-vlm")]);R(ee,z.models,!0),R(ae,E.models,!0),e.model_type==="text"&&l(ee).length>0?(R(C,l(ee)[0],!0),e.base_model=l(ee)[0].id):e.model_type==="vision"&&l(ae).length>0&&(R(C,l(ae)[0],!0),e.base_model=l(ae)[0].id)}catch(z){R(we,z instanceof Error?z.message:"Failed to load models from registry",!0),console.error("Failed to load registry models:",z),R(ee,[{id:"unsloth/tinyllama-bnb-4bit",name:"TinyLlama 1.1B (4-bit)",parameters:"1.1B"},{id:"unsloth/phi-2-bnb-4bit",name:"Phi-2 2.7B (4-bit)",parameters:"2.7B"},{id:"unsloth/mistral-7b-bnb-4bit",name:"Mistral 7B (4-bit)",parameters:"7B"},{id:"unsloth/llama-2-7b-bnb-4bit",name:"Llama 2 7B (4-bit)",parameters:"7B"},{id:"unsloth/llama-3-8b-bnb-4bit",name:"Llama 3 8B (4-bit)",parameters:"8B"}],!0),R(ae,[{id:"Qwen/Qwen2.5-VL-3B-Instruct",name:"Qwen2.5-VL 3B",parameters:"3B"},{id:"Qwen/Qwen2.5-VL-7B-Instruct",name:"Qwen2.5-VL 7B",parameters:"7B"},{id:"Qwen/Qwen2.5-VL-72B-Instruct",name:"Qwen2.5-VL 72B",parameters:"72B"},{id:"unsloth/Qwen2.5-VL-3B-Instruct-bnb-4bit",name:"Qwen2.5-VL 3B (4-bit)",parameters:"3B"},{id:"unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit",name:"Qwen2.5-VL 7B (4-bit)",parameters:"7B"}],!0)}finally{R(_e,!1)}}),ya(()=>{const z=e.model_type==="vision"?l(ae):l(ee);if(R(C,z.find(E=>E.id===e.base_model)||null,!0),l(C)?.training_defaults){const E=l(C).training_defaults;E.hyperparameters&&(e.hyperparameters={...e.hyperparameters,...E.hyperparameters}),E.lora_config&&(e.lora_config={...e.lora_config,...E.lora_config}),E.save_method&&(e.save_method=E.save_method)}}),ya(()=>{e.model_type==="vision"?(l(ae).length>0&&(e.base_model=l(ae)[0].id),e.dataset_path="./data/vision_dataset.jsonl"):(l(ee).length>0&&(e.base_model=l(ee)[0].id),e.dataset_path="./data/sample.jsonl")});let Se=se(!1),Le=se(!1);ya(()=>{e.name&&(e.output_dir=e.name.toLowerCase().replace(/[^a-z0-9]/g,"-"))});async function ht(z){if(z.preventDefault(),!e.name||!e.base_model||!e.dataset_path){R(le,"Please fill in all required fields");return}R(Z,!0),R(le,"");try{let E=null;e.selective_loss_schema_keys&&e.selective_loss_schema_keys.trim()&&(E=e.selective_loss_schema_keys.split(",").map(ne=>ne.trim()).filter(ne=>ne.length>0));const te=await fa.createTrainingJob({...e,is_vision:e.model_type==="vision",selective_loss:e.selective_loss,selective_loss_level:e.selective_loss_level,selective_loss_schema_keys:E,selective_loss_masking_start_step:e.selective_loss_masking_start_step,selective_loss_verbose:e.selective_loss_verbose,early_stopping_enabled:e.early_stopping_enabled,early_stopping_patience:e.early_stopping_patience,early_stopping_threshold:e.early_stopping_threshold});te.success?wr(`/training/${te.data.job_id}`):R(le,"Failed to create training job")}catch(E){R(le,E instanceof Error?E.message:"Failed to create training job",!0)}finally{R(Z,!1)}}var Me=ms();yr(z=>{xr.title="New Training Job - Model Garden"});var Ae=t(Me),ha=t(Ae),ka=t(ha),wa=t(ka),kt=t(wa);xa(kt,{href:"/training",variant:"ghost",size:"sm",children:(z,E)=>{i();var te=Y("‚Üê Training Jobs");n(z,te)},$$slots:{default:!0}}),i(2),r(wa),r(ka),r(ha),r(Ae);var Sa=a(Ae,2),wt=t(Sa);Sr(wt,{children:(z,E)=>{var te=ps(),ne=t(te);{var St=s=>{var o=Lr(),_=t(o),f=t(_,!0);r(_),r(o),J(()=>N(f,l(le))),n(s,o)};u(ne,s=>{l(le)&&s(St)})}var Be=a(ne,2),La=a(t(Be),2),Ce=t(La),Ma=a(t(Ce),2),ce=t(Ma);ce.__click=[Mr,e];var Aa=t(ce),Ee=t(Aa),Lt=t(Ee);{var Mt=s=>{var o=Ar();n(s,o)};u(Lt,s=>{e.model_type==="text"&&s(Mt)})}r(Ee),i(2),r(Aa),i(2),r(ce);var be=a(ce,2);be.__click=[Br,e];var Ba=t(be),Re=t(Ba),At=t(Re);{var Bt=s=>{var o=Cr();n(s,o)};u(At,s=>{e.model_type==="vision"&&s(Bt)})}r(Re),i(2),r(Ba),i(2),r(be),r(Ma),r(Ce);var Te=a(Ce,2),Ca=a(t(Te),2);m(Ca),r(Te);var je=a(Te,2),ue=a(t(je),2),Ct=t(ue);{var Et=s=>{var o=Er();o.value=o.__value="",n(s,o)},Rt=s=>{var o=ve(),_=K(o);{var f=d=>{var p=ve(),w=K(p);ft(w,17,()=>l(ee),xt,(h,x)=>{var g=Rr(),A=t(g);r(g);var b={};J(()=>{N(A,`${l(x).name??""} (${l(x).parameters??""})`),b!==(b=l(x).id)&&(g.value=(g.__value=l(x).id)??"")}),n(h,g)}),n(d,p)},k=d=>{var p=ve(),w=K(p);ft(w,17,()=>l(ae),xt,(h,x)=>{var g=Tr(),A=t(g);r(g);var b={};J(()=>{N(A,`${l(x).name??""} (${l(x).parameters??""})`),b!==(b=l(x).id)&&(g.value=(g.__value=l(x).id)??"")}),n(h,g)}),n(d,p)};u(_,d=>{e.model_type==="text"?d(f):d(k,!1)},!0)}n(s,o)};u(Ct,s=>{l(_e)?s(Et):s(Rt,!1)})}r(ue);var Ea=a(ue,2);{var Tt=s=>{var o=jr(),_=t(o);r(o),J(()=>N(_,`‚ö†Ô∏è Using fallback models: ${l(we)??""}`)),n(s,o)};u(Ea,s=>{l(we)&&s(Tt)})}var Ra=a(Ea,2);{var jt=s=>{var o=Nr();n(s,o)};u(Ra,s=>{e.model_type==="vision"&&s(jt)})}var Nt=a(Ra,2);{var Ot=s=>{var o=qr(),_=t(o),f=t(_),k=t(f),d=t(k);r(k);var p=a(k,2),w=t(p,!0);r(p);var h=a(p,2);{var x=b=>{var L=zr(),B=t(L),T=a(t(B));r(B);var P=a(B,2);{var U=q=>{var j=Or(),F=a(t(j));r(j),J(Q=>N(F,` ${Q??""}
                              tokens`),[()=>l(C).capabilities.context_window.toLocaleString()]),n(q,j)};u(P,q=>{l(C).capabilities?.context_window&&q(U)})}r(L),J(()=>N(T,` ${l(C).requirements.min_vram_gb??""}GB
                            minimum,
                            ${l(C).requirements.recommended_vram_gb??""}GB recommended`)),n(b,L)};u(h,b=>{l(C).requirements&&b(x)})}var g=a(h,2);{var A=b=>{var L=Pr(),B=a(t(L));r(L),J(T=>N(B,` ${T??""}`),[()=>l(C).recommended_for.join(", ")]),n(b,L)};u(g,b=>{l(C).recommended_for&&l(C).recommended_for.length>0&&b(A)})}r(f),r(_),r(o),J(()=>{N(d,`üìä ${l(C).name??""}`),N(w,l(C).description)}),n(s,o)};u(Nt,s=>{l(C)&&!l(_e)&&s(Ot)})}r(je);var Ne=a(je,2),ye=a(t(Ne),2);m(ye);var Oe=a(ye,2),Ta=t(Oe);m(Ta),i(2),r(Oe);var ja=a(Oe,2),zt=t(ja);{var Pt=s=>{var o=Fr();i(2),n(s,o)},qt=s=>{var o=ve(),_=K(o);{var f=d=>{var p=Y(`Path to your JSONL dataset with image paths/base64 or local
                  file`);n(d,p)},k=d=>{var p=Y("Path to your JSONL dataset file");n(d,p)};u(_,d=>{e.model_type==="vision"?d(f):d(k,!1)},!0)}n(s,o)};u(zt,s=>{e.from_hub?s(Pt):s(qt,!1)})}r(ja),r(Ne);var ze=a(Ne,2),fe=a(t(ze),2);m(fe);var Pe=a(fe,2),Na=t(Pe);m(Na),i(2),r(Pe);var Oa=a(Pe,2),Ft=a(t(Oa),3);{var Qt=s=>{var o=Y(`Use HuggingFace format: "username/repo" or
                  "username/repo::validation.jsonl"`);n(s,o)};u(Ft,s=>{e.validation_from_hub&&s(Qt)})}r(Oa),r(ze);var za=a(ze,2);{var Vt=s=>{var o=Ur(),_=a(t(o),2);{var f=d=>{var p=Qr(),w=a(K(p),2),h=t(w);h.textContent='{"messages": [{"role": "user", "content": [{"type": "image", "image": "data:image/jpeg;base64,..."}, {"type": "text", "text": "What is shown?"}]}]}',r(w),i(2),n(d,p)},k=d=>{var p=Vr(),w=a(K(p),2),h=t(w);h.textContent='{"text": "What is in this image?", "image": "/path/to/image.jpg", "response": "A cat sitting on a table"}',r(w),i(2),n(d,p)};u(_,d=>{e.from_hub?d(f):d(k,!1)})}r(o),n(s,o)};u(za,s=>{e.model_type==="vision"&&s(Vt)})}var Pa=a(za,2),qe=a(t(Pa),2);m(qe);var qa=a(qe,2),Ut=t(qa);r(qa),r(Pa),r(La),r(Be);var Fe=a(Be,2),Qe=t(Fe),$t=a(t(Qe),2);{var Gt=s=>{var o=$r();n(s,o)};u($t,s=>{l(C)?.training_defaults&&s(Gt)})}r(Qe);var Fa=a(Qe,2);{var Jt=s=>{var o=Gr();n(s,o)};u(Fa,s=>{e.model_type==="vision"&&s(Jt)})}var Ve=a(Fa,2),Qa=a(t(Ve),2),Ue=t(Qa),$e=a(t(Ue),2);m($e);var Va=a($e,2),Dt=t(Va);{var Ht=s=>{var o=Y(`2e-5 recommended for
                    vision models`);n(s,o)},It=s=>{var o=Y("2e-4 typical for text models");n(s,o)};u(Dt,s=>{e.model_type==="vision"?s(Ht):s(It,!1)})}r(Va),r(Ue);var Ge=a(Ue,2),Ua=a(t(Ge),2);m(Ua),i(2),r(Ge);var Je=a(Ge,2),De=a(t(Je),2);m(De);var $a=a(De,2),Wt=t($a);{var Kt=s=>{var o=Y("Use 1 for vision models");n(s,o)},Yt=s=>{var o=Y(`2-4
                    typical for text models`);n(s,o)};u(Wt,s=>{e.model_type==="vision"?s(Kt):s(Yt,!1)})}r($a),r(Je);var He=a(Je,2),Ga=a(t(He),2);m(Ga),i(2),r(He);var Ie=a(He,2),Ja=a(t(Ie),2);m(Ja),i(2),r(Ie);var Da=a(Ie,2),We=a(t(Da),2),Ke=t(We);Ke.value=Ke.__value="adamw_8bit";var Ye=a(Ke);Ye.value=Ye.__value="adamw_torch";var Xe=a(Ye);Xe.value=Xe.__value="adamw_torch_fused";var Ze=a(Xe);Ze.value=Ze.__value="adafactor";var Ha=a(Ze);Ha.value=Ha.__value="sgd",r(We),i(2),r(Da),r(Qa),r(Ve);var ea=a(Ve,2),Ia=a(t(ea),2),aa=t(Ia),Wa=a(t(aa),2);m(Wa),i(2),r(aa);var ta=a(aa,2),Ka=a(t(ta),2);m(Ka),i(2),r(ta);var Ya=a(ta,2),Xa=a(t(Ya),2);m(Xa),i(2),r(Ya),r(Ia),r(ea);var Za=a(ea,2);{var Xt=s=>{var o=Dr(),_=K(o),f=a(t(_),2),k=t(f),d=a(t(k),2),p=t(d);p.value=p.__value="steps";var w=a(p);w.value=w.__value="epoch";var h=a(w);h.value=h.__value="no",r(d),r(k);var x=a(k,2),g=a(t(x),2);m(g),i(2),r(x);var A=a(x,2),b=a(t(A),2),L=t(b);L.value=L.__value="eval_loss";var B=a(L);B.value=B.__value="eval_accuracy";var T=a(B);T.value=T.__value="eval_f1",r(b),r(A);var P=a(A,2),U=t(P),q=t(U);m(q),i(2),r(U),i(2),r(P),r(f),r(_);var j=a(_,2),F=a(t(j),4),Q=t(F),$=t(Q),G=t($);m(G),i(2),r($),i(2),r(Q);var W=a(Q,2);{var D=M=>{var H=Jr(),I=t(H),c=a(t(I),2);m(c),i(2),r(I);var he=a(I,2),de=a(t(he),2);m(de),i(2),r(he),i(2),r(H),y(c,()=>e.early_stopping_patience,pe=>e.early_stopping_patience=pe),y(de,()=>e.early_stopping_threshold,pe=>e.early_stopping_threshold=pe),n(M,H)};u(W,M=>{e.early_stopping_enabled&&M(D)})}r(F),r(j),X(d,()=>e.hyperparameters.eval_strategy,M=>e.hyperparameters.eval_strategy=M),y(g,()=>e.hyperparameters.eval_steps,M=>e.hyperparameters.eval_steps=M),X(b,()=>e.hyperparameters.metric_for_best_model,M=>e.hyperparameters.metric_for_best_model=M),oe(q,()=>e.hyperparameters.load_best_model_at_end,M=>e.hyperparameters.load_best_model_at_end=M),oe(G,()=>e.early_stopping_enabled,M=>e.early_stopping_enabled=M),n(s,o)};u(Za,s=>{e.validation_dataset_path&&s(Xt)})}var ra=a(Za,2),sa=t(ra);sa.__click=[Hr,Se];var et=t(sa),Zt=t(et,!0);r(et),i(),r(sa),r(ra);var er=a(ra,2);{var ar=s=>{var o=Ir(),_=a(t(o),2),f=t(_),k=a(t(f),2);m(k),i(2),r(f);var d=a(f,2),p=a(t(d),2),w=t(p);w.value=w.__value="linear";var h=a(w);h.value=h.__value="cosine";var x=a(h);x.value=x.__value="constant";var g=a(x);g.value=g.__value="constant_with_warmup";var A=a(g);A.value=A.__value="polynomial",r(p),i(2),r(d);var b=a(d,2),L=a(t(b),2);m(L),i(2),r(b);var B=a(b,2),T=a(t(B),2);m(T),i(2),r(B),r(_);var P=a(_,4),U=t(P),q=a(t(U),2);m(q),i(2),r(U);var j=a(U,2),F=a(t(j),2);m(F),i(2),r(j);var Q=a(j,2),$=a(t(Q),2);m($),i(2),r(Q),r(P);var G=a(P,4),W=t(G),D=a(t(W),2);m(D),i(2),r(W);var M=a(W,2),H=t(M),I=t(H);m(I),i(2),r(H),i(2),r(M),r(G),r(o),y(k,()=>e.hyperparameters.weight_decay,c=>e.hyperparameters.weight_decay=c),X(p,()=>e.hyperparameters.lr_scheduler_type,c=>e.hyperparameters.lr_scheduler_type=c),y(L,()=>e.hyperparameters.warmup_steps,c=>e.hyperparameters.warmup_steps=c),y(T,()=>e.hyperparameters.max_grad_norm,c=>e.hyperparameters.max_grad_norm=c),y(q,()=>e.hyperparameters.adam_beta1,c=>e.hyperparameters.adam_beta1=c),y(F,()=>e.hyperparameters.adam_beta2,c=>e.hyperparameters.adam_beta2=c),y($,()=>e.hyperparameters.adam_epsilon,c=>e.hyperparameters.adam_epsilon=c),y(D,()=>e.hyperparameters.dataloader_num_workers,c=>e.hyperparameters.dataloader_num_workers=c),oe(I,()=>e.hyperparameters.dataloader_pin_memory,c=>e.hyperparameters.dataloader_pin_memory=c),n(s,o)};u(er,s=>{l(Se)&&s(ar)})}r(Fe);var oa=a(Fe,2),ia=t(oa),tr=a(t(ia),2);{var rr=s=>{var o=Wr();n(s,o)};u(tr,s=>{l(C)?.training_defaults?.lora_config&&s(rr)})}r(ia);var la=a(ia,2),na=t(la),at=a(t(na),2);m(at),i(2),r(na);var da=a(na,2),tt=a(t(da),2);m(tt),i(2),r(da);var rt=a(da,2),st=a(t(rt),2);m(st),i(2),r(rt),r(la);var pa=a(la,2),ma=t(pa);ma.__click=[Kr,Le];var ot=t(ma),sr=t(ot,!0);r(ot),i(),r(ma),r(pa);var or=a(pa,2);{var ir=s=>{var o=Xr(),_=t(o),f=t(_),k=a(t(f),2),d=t(k);d.value=d.__value="none";var p=a(d);p.value=p.__value="all";var w=a(p);w.value=w.__value="lora_only",r(k),i(2),r(f);var h=a(f,2),x=a(t(h),2),g=t(x);g.value=g.__value="unsloth";var A=a(g);A.value=A.__value="true";var b=a(A);b.value=b.__value="false",r(x),i(2),r(h),r(_);var L=a(_,2),B=t(L),T=t(B),P=t(T);m(P),i(2),r(T),i(2),r(B);var U=a(B,2),q=a(t(U),2);m(q),i(2),r(U),r(L);var j=a(L,2),F=t(j),Q=a(t(F),2),$=t(Q);$.value=$.__value="CAUSAL_LM";var G=a($);G.value=G.__value="SEQ_2_SEQ_LM";var W=a(G);W.value=W.__value="TOKEN_CLS";var D=a(W);D.value=D.__value="SEQ_CLS";var M=a(D);M.value=M.__value="QUESTION_ANS",r(Q),i(2),r(F);var H=a(F,2),I=a(t(H),2);m(I),I.__input=[Yr,e],i(2),r(H),r(j),i(2),r(o),J(c=>kr(I,c),[()=>e.lora_config.target_modules?.join(", ")||""]),X(k,()=>e.lora_config.lora_bias,c=>e.lora_config.lora_bias=c),X(x,()=>e.lora_config.use_gradient_checkpointing,c=>e.lora_config.use_gradient_checkpointing=c),oe(P,()=>e.lora_config.use_rslora,c=>e.lora_config.use_rslora=c),y(q,()=>e.lora_config.random_state,c=>e.lora_config.random_state=c),X(Q,()=>e.lora_config.task_type,c=>e.lora_config.task_type=c),n(s,o)};u(or,s=>{l(Le)&&s(ir)})}r(oa);var va=a(oa,2),it=a(t(va),2),xe=a(t(it),2),_a=t(xe);_a.value=_a.__value="merged_16bit";var ca=a(_a);ca.value=ca.__value="merged_4bit";var lt=a(ca);lt.value=lt.__value="lora",r(xe);var nt=a(xe,2),dt=t(nt),lr=t(dt);{var nr=s=>{var o=Zr();i(),n(s,o)},dr=s=>{var o=ve(),_=K(o);{var f=d=>{var p=es();i(),n(d,p)},k=d=>{var p=as();i(),n(d,p)};u(_,d=>{e.save_method==="merged_4bit"?d(f):d(k,!1)},!0)}n(s,o)};u(lr,s=>{e.save_method==="merged_16bit"?s(nr):s(dr,!1)})}r(dt),r(nt),r(it),r(va);var pt=a(va,2);{var pr=s=>{var o=ds(),_=a(t(o),4),f=t(_),k=t(f),d=t(k);m(d),i(2),r(k),i(2),r(f);var p=a(f,2);{var w=h=>{var x=ns(),g=t(x),A=a(t(g),2),b=t(A);b.value=b.__value="conservative";var L=a(b);L.value=L.__value="moderate";var B=a(L);B.value=B.__value="aggressive",r(A);var T=a(A,2),P=t(T),U=t(P);{var q=S=>{var V=ts(),re=a(K(V),2);re.textContent='{, }, [, ], :, ,, "',i(2),n(S,V)},j=S=>{var V=ve(),re=K(V);{var ge=me=>{var ga=rs();i(3),n(me,ga)},cr=me=>{var ga=ss();i(),n(me,ga)};u(re,me=>{e.selective_loss_level==="moderate"?me(ge):me(cr,!1)},!0)}n(S,V)};u(U,S=>{e.selective_loss_level==="conservative"?S(q):S(j,!1)})}r(P),r(T),r(g);var F=a(g,2);{var Q=S=>{var V=os(),re=a(t(V),2);m(re),i(4),r(V),y(re,()=>e.selective_loss_schema_keys,ge=>e.selective_loss_schema_keys=ge),n(S,V)};u(F,S=>{e.selective_loss_level==="aggressive"&&S(Q)})}var $=a(F,2),G=t($),W=t(G);r(G);var D=a(G,2);m(D);var M=a(D,4),H=t(M),I=a(t(H),2);{var c=S=>{var V=is();i(),n(S,V)},he=S=>{var V=ls(),re=a(K(V)),ge=t(re);r(re),J(()=>N(ge,`Currently: Model learns structure for ${e.selective_loss_masking_start_step??""}
                            steps, then masking begins`)),n(S,V)};u(I,S=>{e.selective_loss_masking_start_step===0?S(c):S(he,!1)})}r(H),r(M),r($);var de=a($,2),pe=t(de),_t=t(pe);m(_t),i(2),r(pe),i(2),r(de);var ct=a(de,2),ua=a(t(ct),2),ut=t(ua),vr=a(t(ut));vr.textContent="{ } [ ] : ,",i(),r(ut),i(8),r(ua);var gt=a(ua,2),bt=a(t(gt),2);bt.textContent='{"name": "John", "age": 30}';var _r=a(bt,2);_r.textContent='{ } : , "',i(2),r(gt),r(ct),r(x),J(()=>N(W,`Masking Start Step: ${e.selective_loss_masking_start_step??""}`)),X(A,()=>e.selective_loss_level,S=>e.selective_loss_level=S),y(D,()=>e.selective_loss_masking_start_step,S=>e.selective_loss_masking_start_step=S),oe(_t,()=>e.selective_loss_verbose,S=>e.selective_loss_verbose=S),n(h,x)};u(p,h=>{e.selective_loss&&h(w)})}r(_),r(o),oe(d,()=>e.selective_loss,h=>e.selective_loss=h),n(s,o)};u(pt,s=>{e.model_type==="vision"&&s(pr)})}var mt=a(pt,2),vt=t(mt);xa(vt,{type:"submit",variant:"primary",get loading(){return l(Z)},get disabled(){return l(Z)},children:(s,o)=>{i();var _=Y();J(()=>N(_,l(Z)?"Creating...":"Start Training")),n(s,_)},$$slots:{default:!0}});var mr=a(vt,2);xa(mr,{href:"/training",variant:"secondary",children:(s,o)=>{i();var _=Y("Cancel");n(s,_)},$$slots:{default:!0}}),r(mt),r(te),J(()=>{ke(ce,1,`p-4 border-2 rounded-lg text-left transition-all ${e.model_type==="text"?"border-primary-500 bg-primary-50":"border-gray-300 hover:border-gray-400"}`),ke(Ee,1,`w-4 h-4 rounded-full border-2 ${e.model_type==="text"?"border-primary-500 bg-primary-500":"border-gray-400"}`),ke(be,1,`p-4 border-2 rounded-lg text-left transition-all ${e.model_type==="vision"?"border-primary-500 bg-primary-50":"border-gray-300 hover:border-gray-400"}`),ke(Re,1,`w-4 h-4 rounded-full border-2 ${e.model_type==="vision"?"border-primary-500 bg-primary-500":"border-gray-400"}`),ue.disabled=l(_e),yt(ye,"placeholder",e.from_hub?"username/dataset-name":e.model_type==="vision"?"./data/vision_dataset.jsonl":"./data/my-dataset.jsonl"),yt(fe,"placeholder",e.validation_from_hub?"username/val-dataset-name":e.model_type==="vision"?"./data/vision_val_dataset.jsonl":"./data/my-val-dataset.jsonl"),N(Ut,`Model will be saved to models/${(e.output_dir||"my-model")??""}`),N(Zt,l(Se)?"‚ñº":"‚ñ∂"),N(sr,l(Le)?"‚ñº":"‚ñ∂")}),hr("submit",te,ht),y(Ca,()=>e.name,s=>e.name=s),X(ue,()=>e.base_model,s=>e.base_model=s),y(ye,()=>e.dataset_path,s=>e.dataset_path=s),oe(Ta,()=>e.from_hub,s=>e.from_hub=s),y(fe,()=>e.validation_dataset_path,s=>e.validation_dataset_path=s),oe(Na,()=>e.validation_from_hub,s=>e.validation_from_hub=s),y(qe,()=>e.output_dir,s=>e.output_dir=s),y($e,()=>e.hyperparameters.learning_rate,s=>e.hyperparameters.learning_rate=s),y(Ua,()=>e.hyperparameters.num_epochs,s=>e.hyperparameters.num_epochs=s),y(De,()=>e.hyperparameters.batch_size,s=>e.hyperparameters.batch_size=s),y(Ga,()=>e.hyperparameters.gradient_accumulation_steps,s=>e.hyperparameters.gradient_accumulation_steps=s),y(Ja,()=>e.hyperparameters.max_steps,s=>e.hyperparameters.max_steps=s),X(We,()=>e.hyperparameters.optim,s=>e.hyperparameters.optim=s),y(Wa,()=>e.hyperparameters.logging_steps,s=>e.hyperparameters.logging_steps=s),y(Ka,()=>e.hyperparameters.save_steps,s=>e.hyperparameters.save_steps=s),y(Xa,()=>e.hyperparameters.save_total_limit,s=>e.hyperparameters.save_total_limit=s),y(at,()=>e.lora_config.r,s=>e.lora_config.r=s),y(tt,()=>e.lora_config.lora_alpha,s=>e.lora_config.lora_alpha=s),y(st,()=>e.lora_config.lora_dropout,s=>e.lora_config.lora_dropout=s),X(xe,()=>e.save_method,s=>e.save_method=s),n(z,te)},$$slots:{default:!0}}),r(Sa),r(Me),n(ie,Me),fr()}ur(["click","input"]);export{hs as component};
